{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SYNC]\u001b[38;5;2m[i 1226 10:20:13.500076 52 cuda_flags.cc:25] CUDA enabled.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "import numpy as np\n",
    "jt.flags.use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(jt.Module):\n",
    "    def __init__(self):\n",
    "        self.layer1 = jt.nn.Linear(1, 10)\n",
    "        self.relu = jt.nn.Relu() \n",
    "        self.layer2 = jt.nn.Linear(10, 1)\n",
    "    def execute (self,x) :\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "def get_data(n): # generate random data for training test.\n",
    "    for i in range(n):\n",
    "        x = np.random.rand(batch_size, 1)\n",
    "        y = x*x\n",
    "        yield jt.float32(x), jt.float32(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 0.12343169003725052\n",
      "step 1, loss = 0.09728299081325531\n",
      "step 2, loss = 0.07804940640926361\n",
      "step 3, loss = 0.07337296009063721\n",
      "step 4, loss = 0.0814935639500618\n",
      "step 5, loss = 0.07479465752840042\n",
      "step 6, loss = 0.08906663209199905\n",
      "step 7, loss = 0.08787931501865387\n",
      "step 8, loss = 0.06506746262311935\n",
      "step 9, loss = 0.07722386717796326\n",
      "step 10, loss = 0.06800637394189835\n",
      "step 11, loss = 0.0701238214969635\n",
      "step 12, loss = 0.0349930115044117\n",
      "step 13, loss = 0.05284535884857178\n",
      "step 14, loss = 0.04084266722202301\n",
      "step 15, loss = 0.0435093492269516\n",
      "step 16, loss = 0.04032568633556366\n",
      "step 17, loss = 0.04160361737012863\n",
      "step 18, loss = 0.03119817189872265\n",
      "step 19, loss = 0.028158793225884438\n",
      "step 20, loss = 0.025464387610554695\n",
      "step 21, loss = 0.03968729451298714\n",
      "step 22, loss = 0.026950113475322723\n",
      "step 23, loss = 0.02541627362370491\n",
      "step 24, loss = 0.020417753607034683\n",
      "step 25, loss = 0.0146144088357687\n",
      "step 26, loss = 0.020323947072029114\n",
      "step 27, loss = 0.016361216083168983\n",
      "step 28, loss = 0.0132859842851758\n",
      "step 29, loss = 0.01339231338351965\n",
      "step 30, loss = 0.009398074820637703\n",
      "step 31, loss = 0.010931376367807388\n",
      "step 32, loss = 0.012104908004403114\n",
      "step 33, loss = 0.007152866572141647\n",
      "step 34, loss = 0.006256237160414457\n",
      "step 35, loss = 0.010692758485674858\n",
      "step 36, loss = 0.006627858150750399\n",
      "step 37, loss = 0.0044960747472941875\n",
      "step 38, loss = 0.006406595464795828\n",
      "step 39, loss = 0.004598703701049089\n",
      "step 40, loss = 0.006036543287336826\n",
      "step 41, loss = 0.004754618741571903\n",
      "step 42, loss = 0.00474140141159296\n",
      "step 43, loss = 0.0072374180890619755\n",
      "step 44, loss = 0.0043026152998209\n",
      "step 45, loss = 0.0038801846094429493\n",
      "step 46, loss = 0.0031205634586513042\n",
      "step 47, loss = 0.004093443509191275\n",
      "step 48, loss = 0.00443101255223155\n",
      "step 49, loss = 0.004464634694159031\n",
      "step 50, loss = 0.003997883293777704\n",
      "step 51, loss = 0.003764744382351637\n",
      "step 52, loss = 0.0031966271344572306\n",
      "step 53, loss = 0.0031505280639976263\n",
      "step 54, loss = 0.0028607009444385767\n",
      "step 55, loss = 0.0032305961940437555\n",
      "step 56, loss = 0.0025517563335597515\n",
      "step 57, loss = 0.002351833274587989\n",
      "step 58, loss = 0.002902164589613676\n",
      "step 59, loss = 0.003769493196159601\n",
      "step 60, loss = 0.0024380898103117943\n",
      "step 61, loss = 0.0029099101666361094\n",
      "step 62, loss = 0.0029396608006209135\n",
      "step 63, loss = 0.0029790804255753756\n",
      "step 64, loss = 0.0023693162947893143\n",
      "step 65, loss = 0.002461865544319153\n",
      "step 66, loss = 0.002427338156849146\n",
      "step 67, loss = 0.0019455559086054564\n",
      "step 68, loss = 0.00213419646024704\n",
      "step 69, loss = 0.0014314355794340372\n",
      "step 70, loss = 0.0014564704615622759\n",
      "step 71, loss = 0.001809691428206861\n",
      "step 72, loss = 0.0033792105969041586\n",
      "step 73, loss = 0.0014034966006875038\n",
      "step 74, loss = 0.002328272210434079\n",
      "step 75, loss = 0.0011459934758022428\n",
      "step 76, loss = 0.0018421384738758206\n",
      "step 77, loss = 0.0021864096634089947\n",
      "step 78, loss = 0.0018369673052802682\n",
      "step 79, loss = 0.001596734393388033\n",
      "step 80, loss = 0.0016129212453961372\n",
      "step 81, loss = 0.0017047582659870386\n",
      "step 82, loss = 0.0012245553079992533\n",
      "step 83, loss = 0.0013830582611262798\n",
      "step 84, loss = 0.0009322171681560576\n",
      "step 85, loss = 0.0017242581816390157\n",
      "step 86, loss = 0.001610378036275506\n",
      "step 87, loss = 0.0010239970870316029\n",
      "step 88, loss = 0.0010317697888240218\n",
      "step 89, loss = 0.001248580520041287\n",
      "step 90, loss = 0.001188941765576601\n",
      "step 91, loss = 0.0010895737214013934\n",
      "step 92, loss = 0.000727370847016573\n",
      "step 93, loss = 0.0010201397817581892\n",
      "step 94, loss = 0.0011814797762781382\n",
      "step 95, loss = 0.0006584368529729545\n",
      "step 96, loss = 0.0005678649758920074\n",
      "step 97, loss = 0.0004928482230752707\n",
      "step 98, loss = 0.0009407356847077608\n",
      "step 99, loss = 0.001114442478865385\n",
      "step 100, loss = 0.0008520617848262191\n",
      "step 101, loss = 0.0006943157059140503\n",
      "step 102, loss = 0.0006370874471031129\n",
      "step 103, loss = 0.0006127485539764166\n",
      "step 104, loss = 0.0007027225801721215\n",
      "step 105, loss = 0.0005232194671407342\n",
      "step 106, loss = 0.0011208013165742159\n",
      "step 107, loss = 0.00031406519701704383\n",
      "step 108, loss = 0.000663703540340066\n",
      "step 109, loss = 0.0006417480180971324\n",
      "step 110, loss = 0.0005401709931902587\n",
      "step 111, loss = 0.0005503767752088606\n",
      "step 112, loss = 0.0003883244935423136\n",
      "step 113, loss = 0.0007393719861283898\n",
      "step 114, loss = 0.0005888131563551724\n",
      "step 115, loss = 0.0008500217227265239\n",
      "step 116, loss = 0.00047854651347734034\n",
      "step 117, loss = 0.0006104601197876036\n",
      "step 118, loss = 0.00046499937889166176\n",
      "step 119, loss = 0.00034137023612856865\n",
      "step 120, loss = 0.0004970974987372756\n",
      "step 121, loss = 0.0006314691272564232\n",
      "step 122, loss = 0.0003072085964959115\n",
      "step 123, loss = 0.00041280416189692914\n",
      "step 124, loss = 0.0007952786982059479\n",
      "step 125, loss = 0.00024977573775686324\n",
      "step 126, loss = 0.0005772755248472095\n",
      "step 127, loss = 0.0003125639050267637\n",
      "step 128, loss = 0.0003690608427859843\n",
      "step 129, loss = 0.00042474610381759703\n",
      "step 130, loss = 0.0005242211045697331\n",
      "step 131, loss = 0.0005234823329374194\n",
      "step 132, loss = 0.00044100472587160766\n",
      "step 133, loss = 0.0002489506150595844\n",
      "step 134, loss = 0.00029384545632638037\n",
      "step 135, loss = 0.00022457781597040594\n",
      "step 136, loss = 0.000317921832902357\n",
      "step 137, loss = 0.0004230267950333655\n",
      "step 138, loss = 0.00038325675996020436\n",
      "step 139, loss = 0.0004153573827352375\n",
      "step 140, loss = 0.0004219308902975172\n",
      "step 141, loss = 0.0003054494736716151\n",
      "step 142, loss = 0.000176165733137168\n",
      "step 143, loss = 0.00028727372409775853\n",
      "step 144, loss = 0.00029027776326984167\n",
      "step 145, loss = 0.00029688430367968976\n",
      "step 146, loss = 0.0004017889150418341\n",
      "step 147, loss = 0.00027626537485048175\n",
      "step 148, loss = 0.00032326558721251786\n",
      "step 149, loss = 0.00017241125169675797\n",
      "step 150, loss = 0.0002573008241597563\n",
      "step 151, loss = 0.00019634496129583567\n",
      "step 152, loss = 0.00039398865192197263\n",
      "step 153, loss = 0.0002471934712957591\n",
      "step 154, loss = 0.00033695902675390244\n",
      "step 155, loss = 0.0002558950800448656\n",
      "step 156, loss = 0.00021670403657481074\n",
      "step 157, loss = 0.0003126476949546486\n",
      "step 158, loss = 0.00023305979266297072\n",
      "step 159, loss = 0.00027076873811893165\n",
      "step 160, loss = 0.0004188570019323379\n",
      "step 161, loss = 0.0003265897976234555\n",
      "step 162, loss = 0.00017703768389765173\n",
      "step 163, loss = 0.00024689186830073595\n",
      "step 164, loss = 0.00016696745296940207\n",
      "step 165, loss = 0.0003126988303847611\n",
      "step 166, loss = 0.0002760551287792623\n",
      "step 167, loss = 0.0001646373129915446\n",
      "step 168, loss = 0.00018545017519500107\n",
      "step 169, loss = 0.0002856386417988688\n",
      "step 170, loss = 0.00023647001944482327\n",
      "step 171, loss = 0.00019903875363525003\n",
      "step 172, loss = 0.0002578469575382769\n",
      "step 173, loss = 0.00013008929090574384\n",
      "step 174, loss = 0.00019493675790727139\n",
      "step 175, loss = 0.00022795733821112663\n",
      "step 176, loss = 0.0001933531166287139\n",
      "step 177, loss = 0.00021849386394023895\n",
      "step 178, loss = 0.0002897692902479321\n",
      "step 179, loss = 0.000259067805018276\n",
      "step 180, loss = 0.00015951856039464474\n",
      "step 181, loss = 0.00018583952623885125\n",
      "step 182, loss = 0.0001048435369739309\n",
      "step 183, loss = 0.00010457313328515738\n",
      "step 184, loss = 0.00019535180763341486\n",
      "step 185, loss = 5.8557176089379936e-05\n",
      "step 186, loss = 0.00013382530596572906\n",
      "step 187, loss = 0.00023464084370061755\n",
      "step 188, loss = 0.00028028394444845617\n",
      "step 189, loss = 0.00021771831961814314\n",
      "step 190, loss = 0.0001944911782629788\n",
      "step 191, loss = 0.00016867861268110573\n",
      "step 192, loss = 0.0002835557097569108\n",
      "step 193, loss = 0.00019554694881662726\n",
      "step 194, loss = 0.0001856588205555454\n",
      "step 195, loss = 0.0002849014417733997\n",
      "step 196, loss = 0.00022225073189474642\n",
      "step 197, loss = 0.0002044606808340177\n",
      "step 198, loss = 0.0001535666815470904\n",
      "step 199, loss = 0.000261524721281603\n",
      "step 200, loss = 0.00018298093345947564\n",
      "step 201, loss = 0.0002353025774937123\n",
      "step 202, loss = 0.00020844464597757906\n",
      "step 203, loss = 0.00012276486086193472\n",
      "step 204, loss = 0.00010254030348733068\n",
      "step 205, loss = 0.000206576005439274\n",
      "step 206, loss = 0.00033382175024598837\n",
      "step 207, loss = 0.00010596094944048673\n",
      "step 208, loss = 0.00018370390171185136\n",
      "step 209, loss = 0.00012564090138766915\n",
      "step 210, loss = 0.0002363060339121148\n",
      "step 211, loss = 0.00012981668987777084\n",
      "step 212, loss = 0.00012680335203185678\n",
      "step 213, loss = 0.0001307169150095433\n",
      "step 214, loss = 8.149167842930183e-05\n",
      "step 215, loss = 0.00029907331918366253\n",
      "step 216, loss = 0.00012810970656573772\n",
      "step 217, loss = 0.00020101664995308965\n",
      "step 218, loss = 0.00015094879199750721\n",
      "step 219, loss = 0.00011883735714945942\n",
      "step 220, loss = 0.00014288732199929655\n",
      "step 221, loss = 0.00015655290917493403\n",
      "step 222, loss = 0.00012953695841133595\n",
      "step 223, loss = 0.0001904850360006094\n",
      "step 224, loss = 0.00014030975580681115\n",
      "step 225, loss = 0.00011070293112425134\n",
      "step 226, loss = 0.00010029476106865332\n",
      "step 227, loss = 0.00012654058809857816\n",
      "step 228, loss = 0.00012216575851198286\n",
      "step 229, loss = 0.0001490698487032205\n",
      "step 230, loss = 0.00015323075058404356\n",
      "step 231, loss = 0.00019239353423472494\n",
      "step 232, loss = 0.000115542410640046\n",
      "step 233, loss = 0.00012859745766036212\n",
      "step 234, loss = 0.00015272310702130198\n",
      "step 235, loss = 0.0001660737907513976\n",
      "step 236, loss = 0.00017983758880291134\n",
      "step 237, loss = 0.00014447594003286213\n",
      "step 238, loss = 0.00015682977391406894\n",
      "step 239, loss = 0.00011035568604711443\n",
      "step 240, loss = 0.00011632035602815449\n",
      "step 241, loss = 9.03377149370499e-05\n",
      "step 242, loss = 0.00016272414359264076\n",
      "step 243, loss = 0.0001216665405081585\n",
      "step 244, loss = 0.00011446687130955979\n",
      "step 245, loss = 0.0001456089230487123\n",
      "step 246, loss = 0.00010881124762818217\n",
      "step 247, loss = 0.0001167988739325665\n",
      "step 248, loss = 0.00013959489297121763\n",
      "step 249, loss = 0.00013582220708485693\n",
      "step 250, loss = 0.0001047099576680921\n",
      "step 251, loss = 9.386355668539181e-05\n",
      "step 252, loss = 0.00012687560229096562\n",
      "step 253, loss = 0.00013175564527045935\n",
      "step 254, loss = 0.00012913077080156654\n",
      "step 255, loss = 8.659402374178171e-05\n",
      "step 256, loss = 0.000135148991830647\n",
      "step 257, loss = 0.00013203198614064604\n",
      "step 258, loss = 0.00012491516827140003\n",
      "step 259, loss = 0.00011010386515408754\n",
      "step 260, loss = 9.227052214555442e-05\n",
      "step 261, loss = 8.269202953670174e-05\n",
      "step 262, loss = 7.927235856186599e-05\n",
      "step 263, loss = 7.937006012070924e-05\n",
      "step 264, loss = 0.00012551178224384785\n",
      "step 265, loss = 0.00010116286284755915\n",
      "step 266, loss = 0.00011173560778843239\n",
      "step 267, loss = 9.696692723082379e-05\n",
      "step 268, loss = 0.00013487994146998972\n",
      "step 269, loss = 0.00011910060129594058\n",
      "step 270, loss = 0.0001425628288416192\n",
      "step 271, loss = 7.650898623978719e-05\n",
      "step 272, loss = 5.472782504511997e-05\n",
      "step 273, loss = 0.0001326051860814914\n",
      "step 274, loss = 0.00024506927002221346\n",
      "step 275, loss = 0.00014475778152700514\n",
      "step 276, loss = 0.00010337630374124274\n",
      "step 277, loss = 0.00014125781308393925\n",
      "step 278, loss = 0.00016221016994677484\n",
      "step 279, loss = 0.00011772370635299012\n",
      "step 280, loss = 0.00011799642379628494\n",
      "step 281, loss = 8.552336657885462e-05\n",
      "step 282, loss = 0.00013194145867601037\n",
      "step 283, loss = 0.00016576868074480444\n",
      "step 284, loss = 0.00010135432239621878\n",
      "step 285, loss = 5.594746471615508e-05\n",
      "step 286, loss = 8.99370206752792e-05\n",
      "step 287, loss = 0.00011417290807003155\n",
      "step 288, loss = 0.00011974839435424656\n",
      "step 289, loss = 8.888890442904085e-05\n",
      "step 290, loss = 8.623303438071162e-05\n",
      "step 291, loss = 6.818924157414585e-05\n",
      "step 292, loss = 9.039565338753164e-05\n",
      "step 293, loss = 0.00011116115638287738\n",
      "step 294, loss = 9.746550495037809e-05\n",
      "step 295, loss = 9.844230226008222e-05\n",
      "step 296, loss = 0.00010150306479772553\n",
      "step 297, loss = 8.341774810105562e-05\n",
      "step 298, loss = 8.250871906057e-05\n",
      "step 299, loss = 9.98321411316283e-05\n",
      "step 300, loss = 8.799219358479604e-05\n",
      "step 301, loss = 0.00010421481420053169\n",
      "step 302, loss = 0.00011734511645045131\n",
      "step 303, loss = 0.00011431736493250355\n",
      "step 304, loss = 9.99771073111333e-05\n",
      "step 305, loss = 0.00012085091293556616\n",
      "step 306, loss = 9.756138024386019e-05\n",
      "step 307, loss = 0.0001384937495458871\n",
      "step 308, loss = 0.00011831297888420522\n",
      "step 309, loss = 6.392461364157498e-05\n",
      "step 310, loss = 6.954831769689918e-05\n",
      "step 311, loss = 9.154823783319443e-05\n",
      "step 312, loss = 0.00011508895840961486\n",
      "step 313, loss = 9.327569568995386e-05\n",
      "step 314, loss = 6.610839773202315e-05\n",
      "step 315, loss = 8.058220555540174e-05\n",
      "step 316, loss = 8.921918197302148e-05\n",
      "step 317, loss = 0.00010066078539239243\n",
      "step 318, loss = 0.0001089932193281129\n",
      "step 319, loss = 9.888330532703549e-05\n",
      "step 320, loss = 9.956093708751723e-05\n",
      "step 321, loss = 8.864909614203498e-05\n",
      "step 322, loss = 9.451155347051099e-05\n",
      "step 323, loss = 8.942786371335387e-05\n",
      "step 324, loss = 9.48655724641867e-05\n",
      "step 325, loss = 9.024277096614242e-05\n",
      "step 326, loss = 7.566213753307238e-05\n",
      "step 327, loss = 9.879325807560235e-05\n",
      "step 328, loss = 8.816519402898848e-05\n",
      "step 329, loss = 6.487231439677998e-05\n",
      "step 330, loss = 6.499373557744548e-05\n",
      "step 331, loss = 6.200346979312599e-05\n",
      "step 332, loss = 7.868544344091788e-05\n",
      "step 333, loss = 8.764730591792613e-05\n",
      "step 334, loss = 8.271393744507805e-05\n",
      "step 335, loss = 0.00010717294208006933\n",
      "step 336, loss = 6.436524563468993e-05\n",
      "step 337, loss = 5.6258486438309774e-05\n",
      "step 338, loss = 7.272006041603163e-05\n",
      "step 339, loss = 4.535948028205894e-05\n",
      "step 340, loss = 5.645376950269565e-05\n",
      "step 341, loss = 9.270424925489351e-05\n",
      "step 342, loss = 0.00010139104415429756\n",
      "step 343, loss = 5.5666532716713846e-05\n",
      "step 344, loss = 9.389939077664167e-05\n",
      "step 345, loss = 0.00012175689334981143\n",
      "step 346, loss = 0.00011543033906491473\n",
      "step 347, loss = 9.016977128339931e-05\n",
      "step 348, loss = 8.668145892443135e-05\n",
      "step 349, loss = 7.611804903717712e-05\n",
      "step 350, loss = 0.00011184977483935654\n",
      "step 351, loss = 7.994238694664091e-05\n",
      "step 352, loss = 4.7919591452227905e-05\n",
      "step 353, loss = 0.00010732275404734537\n",
      "step 354, loss = 0.00010283950541634113\n",
      "step 355, loss = 5.840306039317511e-05\n",
      "step 356, loss = 7.746085611870512e-05\n",
      "step 357, loss = 7.726725016254932e-05\n",
      "step 358, loss = 0.00011537613318068907\n",
      "step 359, loss = 5.806975605082698e-05\n",
      "step 360, loss = 8.459547825623304e-05\n",
      "step 361, loss = 8.011111640371382e-05\n",
      "step 362, loss = 8.567141048843041e-05\n",
      "step 363, loss = 8.85004410520196e-05\n",
      "step 364, loss = 5.6870027037803084e-05\n",
      "step 365, loss = 7.31670661480166e-05\n",
      "step 366, loss = 0.00010200132237514481\n",
      "step 367, loss = 0.00010521911463001743\n",
      "step 368, loss = 0.00010275943350279704\n",
      "step 369, loss = 8.424391853623092e-05\n",
      "step 370, loss = 9.026915358845145e-05\n",
      "step 371, loss = 9.303067781729624e-05\n",
      "step 372, loss = 9.559320460539311e-05\n",
      "step 373, loss = 5.9519959904719144e-05\n",
      "step 374, loss = 0.00010270976054016501\n",
      "step 375, loss = 7.747257041046396e-05\n",
      "step 376, loss = 7.313619425985962e-05\n",
      "step 377, loss = 9.222937660524622e-05\n",
      "step 378, loss = 7.067213300615549e-05\n",
      "step 379, loss = 0.0001131409517256543\n",
      "step 380, loss = 7.895983435446396e-05\n",
      "step 381, loss = 9.851247887127101e-05\n",
      "step 382, loss = 8.094055374385789e-05\n",
      "step 383, loss = 6.91195746185258e-05\n",
      "step 384, loss = 0.0001086225820472464\n",
      "step 385, loss = 9.149612014880404e-05\n",
      "step 386, loss = 8.653548138681799e-05\n",
      "step 387, loss = 8.779296331340447e-05\n",
      "step 388, loss = 8.9308712631464e-05\n",
      "step 389, loss = 7.266327884281054e-05\n",
      "step 390, loss = 6.111565016908571e-05\n",
      "step 391, loss = 9.407734614796937e-05\n",
      "step 392, loss = 7.009582623140886e-05\n",
      "step 393, loss = 6.956602010177448e-05\n",
      "step 394, loss = 7.917248149169609e-05\n",
      "step 395, loss = 7.996035856194794e-05\n",
      "step 396, loss = 8.515544323017821e-05\n",
      "step 397, loss = 7.572166941827163e-05\n",
      "step 398, loss = 6.885315087856725e-05\n",
      "step 399, loss = 7.226052548503503e-05\n",
      "step 400, loss = 6.214094173628837e-05\n",
      "step 401, loss = 7.51999905332923e-05\n",
      "step 402, loss = 8.819665526971221e-05\n",
      "step 403, loss = 7.034688314888626e-05\n",
      "step 404, loss = 6.702516111545265e-05\n",
      "step 405, loss = 7.180665852501988e-05\n",
      "step 406, loss = 6.894176476635039e-05\n",
      "step 407, loss = 8.860484376782551e-05\n",
      "step 408, loss = 7.022508361842483e-05\n",
      "step 409, loss = 9.762680565472692e-05\n",
      "step 410, loss = 8.956078090704978e-05\n",
      "step 411, loss = 0.00010258133261231706\n",
      "step 412, loss = 8.697227895027027e-05\n",
      "step 413, loss = 8.515231456840411e-05\n",
      "step 414, loss = 9.330477769253775e-05\n",
      "step 415, loss = 6.28873604000546e-05\n",
      "step 416, loss = 6.0149912314955145e-05\n",
      "step 417, loss = 0.00010615646169753745\n",
      "step 418, loss = 0.000149582585436292\n",
      "step 419, loss = 9.612384019419551e-05\n",
      "step 420, loss = 9.335834329249337e-05\n",
      "step 421, loss = 0.00012354462523944676\n",
      "step 422, loss = 0.00011208330397494137\n",
      "step 423, loss = 7.806846406310797e-05\n",
      "step 424, loss = 0.00012427290494088084\n",
      "step 425, loss = 9.937414870364591e-05\n",
      "step 426, loss = 0.00010352346725994721\n",
      "step 427, loss = 6.94604532327503e-05\n",
      "step 428, loss = 8.093882206594571e-05\n",
      "step 429, loss = 0.00010366372589487582\n",
      "step 430, loss = 0.00011330990673741326\n",
      "step 431, loss = 8.266766963060945e-05\n",
      "step 432, loss = 7.549016299890354e-05\n",
      "step 433, loss = 0.00012015353422611952\n",
      "step 434, loss = 8.269211684819311e-05\n",
      "step 435, loss = 8.563455776311457e-05\n",
      "step 436, loss = 7.894407463027164e-05\n",
      "step 437, loss = 8.270351827377453e-05\n",
      "step 438, loss = 0.00010987889254465699\n",
      "step 439, loss = 6.944283086340874e-05\n",
      "step 440, loss = 7.859194738557562e-05\n",
      "step 441, loss = 6.541745096910745e-05\n",
      "step 442, loss = 6.597406172659248e-05\n",
      "step 443, loss = 8.226008503697813e-05\n",
      "step 444, loss = 8.298875036416575e-05\n",
      "step 445, loss = 7.803186599630862e-05\n",
      "step 446, loss = 8.771031571086496e-05\n",
      "step 447, loss = 7.735408144071698e-05\n",
      "step 448, loss = 8.475036884192377e-05\n",
      "step 449, loss = 9.235714969690889e-05\n",
      "step 450, loss = 9.366121230414137e-05\n",
      "step 451, loss = 6.409523484762758e-05\n",
      "step 452, loss = 0.00011876920325448737\n",
      "step 453, loss = 8.684355998411775e-05\n",
      "step 454, loss = 9.04541157069616e-05\n",
      "step 455, loss = 8.829399303067476e-05\n",
      "step 456, loss = 8.628526120446622e-05\n",
      "step 457, loss = 6.348199531203136e-05\n",
      "step 458, loss = 6.664769171038643e-05\n",
      "step 459, loss = 9.567643428454176e-05\n",
      "step 460, loss = 0.00010792716784635559\n",
      "step 461, loss = 4.567382711684331e-05\n",
      "step 462, loss = 6.0259866586420685e-05\n",
      "step 463, loss = 7.401191396638751e-05\n",
      "step 464, loss = 7.700658170506358e-05\n",
      "step 465, loss = 7.93251529103145e-05\n",
      "step 466, loss = 6.246269913390279e-05\n",
      "step 467, loss = 8.512114436598495e-05\n",
      "step 468, loss = 7.683360308874398e-05\n",
      "step 469, loss = 6.117298471508548e-05\n",
      "step 470, loss = 0.00010973297321470454\n",
      "step 471, loss = 7.078378985170275e-05\n",
      "step 472, loss = 8.06109092081897e-05\n",
      "step 473, loss = 6.357202073559165e-05\n",
      "step 474, loss = 8.410923328483477e-05\n",
      "step 475, loss = 7.638644456164911e-05\n",
      "step 476, loss = 6.913454853929579e-05\n",
      "step 477, loss = 0.00012101932225050405\n",
      "step 478, loss = 8.775008609518409e-05\n",
      "step 479, loss = 7.826568617019802e-05\n",
      "step 480, loss = 7.657769310753793e-05\n",
      "step 481, loss = 8.168048225343227e-05\n",
      "step 482, loss = 7.289479981409386e-05\n",
      "step 483, loss = 0.00010906936950050294\n",
      "step 484, loss = 6.657968333456665e-05\n",
      "step 485, loss = 7.82801944296807e-05\n",
      "step 486, loss = 6.836740794824436e-05\n",
      "step 487, loss = 9.341071563540027e-05\n",
      "step 488, loss = 8.949809125624597e-05\n",
      "step 489, loss = 5.9915193560300395e-05\n",
      "step 490, loss = 7.18785886419937e-05\n",
      "step 491, loss = 6.590621342184022e-05\n",
      "step 492, loss = 9.339425741927698e-05\n",
      "step 493, loss = 8.328355761477724e-05\n",
      "step 494, loss = 7.713828381383792e-05\n",
      "step 495, loss = 5.455897553474642e-05\n",
      "step 496, loss = 6.809420301578939e-05\n",
      "step 497, loss = 7.416120206471533e-05\n",
      "step 498, loss = 5.7109096815111116e-05\n",
      "step 499, loss = 5.6228062021546066e-05\n",
      "step 500, loss = 8.431269088760018e-05\n",
      "step 501, loss = 0.00012346166477072984\n",
      "step 502, loss = 8.30746503197588e-05\n",
      "step 503, loss = 6.334990030154586e-05\n",
      "step 504, loss = 9.490704542258754e-05\n",
      "step 505, loss = 6.30939903203398e-05\n",
      "step 506, loss = 8.979139965958893e-05\n",
      "step 507, loss = 7.69228208810091e-05\n",
      "step 508, loss = 6.804179429309443e-05\n",
      "step 509, loss = 5.810907168779522e-05\n",
      "step 510, loss = 7.068071863614023e-05\n",
      "step 511, loss = 6.946303619770333e-05\n",
      "step 512, loss = 6.632384611293674e-05\n",
      "step 513, loss = 6.83149482938461e-05\n",
      "step 514, loss = 6.579719774890691e-05\n",
      "step 515, loss = 6.607737304875627e-05\n",
      "step 516, loss = 9.159934415947646e-05\n",
      "step 517, loss = 6.638083141297102e-05\n",
      "step 518, loss = 8.434334449702874e-05\n",
      "step 519, loss = 9.703933756100014e-05\n",
      "step 520, loss = 8.136635733535513e-05\n",
      "step 521, loss = 5.7668250519782305e-05\n",
      "step 522, loss = 5.993224112899043e-05\n",
      "step 523, loss = 6.360864790622145e-05\n",
      "step 524, loss = 7.708615157753229e-05\n",
      "step 525, loss = 7.345164340222254e-05\n",
      "step 526, loss = 6.180474883876741e-05\n",
      "step 527, loss = 7.996405474841595e-05\n",
      "step 528, loss = 0.00010590102465357631\n",
      "step 529, loss = 9.286376734962687e-05\n",
      "step 530, loss = 8.119345875456929e-05\n",
      "step 531, loss = 8.756718307267874e-05\n",
      "step 532, loss = 7.209816249087453e-05\n",
      "step 533, loss = 6.986528751440346e-05\n",
      "step 534, loss = 9.263752872357145e-05\n",
      "step 535, loss = 8.495091606164351e-05\n",
      "step 536, loss = 7.566763088107109e-05\n",
      "step 537, loss = 5.9429345128592104e-05\n",
      "step 538, loss = 7.56655863369815e-05\n",
      "step 539, loss = 7.706208270974457e-05\n",
      "step 540, loss = 6.900254811625928e-05\n",
      "step 541, loss = 7.96650565462187e-05\n",
      "step 542, loss = 5.7504781580064446e-05\n",
      "step 543, loss = 7.597613148391247e-05\n",
      "step 544, loss = 7.118090434232727e-05\n",
      "step 545, loss = 8.128068293444812e-05\n",
      "step 546, loss = 6.564270734088495e-05\n",
      "step 547, loss = 7.821640610927716e-05\n",
      "step 548, loss = 8.324320515384898e-05\n",
      "step 549, loss = 6.810295599279925e-05\n",
      "step 550, loss = 7.296240073628724e-05\n",
      "step 551, loss = 6.971984839765355e-05\n",
      "step 552, loss = 6.969469541218132e-05\n",
      "step 553, loss = 8.140583668136969e-05\n",
      "step 554, loss = 0.00010281986760674044\n",
      "step 555, loss = 7.805328641552478e-05\n",
      "step 556, loss = 9.532099647913128e-05\n",
      "step 557, loss = 9.498557483311743e-05\n",
      "step 558, loss = 9.143867646344006e-05\n",
      "step 559, loss = 6.71614398015663e-05\n",
      "step 560, loss = 0.00010330380609957501\n",
      "step 561, loss = 8.566113683627918e-05\n",
      "step 562, loss = 8.788142440607771e-05\n",
      "step 563, loss = 6.488463986897841e-05\n",
      "step 564, loss = 6.14362652413547e-05\n",
      "step 565, loss = 0.00011966293823206797\n",
      "step 566, loss = 6.400011625373736e-05\n",
      "step 567, loss = 6.431791553040966e-05\n",
      "step 568, loss = 7.81592316343449e-05\n",
      "step 569, loss = 8.481783152092248e-05\n",
      "step 570, loss = 7.515431207139045e-05\n",
      "step 571, loss = 6.131671398179606e-05\n",
      "step 572, loss = 9.44595449254848e-05\n",
      "step 573, loss = 7.730044308118522e-05\n",
      "step 574, loss = 9.421589493285865e-05\n",
      "step 575, loss = 9.086787031264976e-05\n",
      "step 576, loss = 8.31436991575174e-05\n",
      "step 577, loss = 7.030156848486513e-05\n",
      "step 578, loss = 7.547866698587313e-05\n",
      "step 579, loss = 7.823108171578497e-05\n",
      "step 580, loss = 0.00010276308603351936\n",
      "step 581, loss = 0.00010210011532763019\n",
      "step 582, loss = 6.977003795327619e-05\n",
      "step 583, loss = 8.718539902474731e-05\n",
      "step 584, loss = 8.559058187529445e-05\n",
      "step 585, loss = 8.388736023334786e-05\n",
      "step 586, loss = 6.493224645964801e-05\n",
      "step 587, loss = 4.872301724390127e-05\n",
      "step 588, loss = 8.26301402412355e-05\n",
      "step 589, loss = 7.760498556308448e-05\n",
      "step 590, loss = 7.322423334699124e-05\n",
      "step 591, loss = 6.490850500995293e-05\n",
      "step 592, loss = 6.83801990817301e-05\n",
      "step 593, loss = 8.077020902419463e-05\n",
      "step 594, loss = 0.00010256087989546359\n",
      "step 595, loss = 7.976269262144342e-05\n",
      "step 596, loss = 8.173749665729702e-05\n",
      "step 597, loss = 6.575649604201317e-05\n",
      "step 598, loss = 9.239936480298638e-05\n",
      "step 599, loss = 9.469384531257674e-05\n",
      "step 600, loss = 6.556510197697207e-05\n",
      "step 601, loss = 0.0001000515985651873\n",
      "step 602, loss = 6.757079245289788e-05\n",
      "step 603, loss = 5.4970143537502736e-05\n",
      "step 604, loss = 6.245278927963227e-05\n",
      "step 605, loss = 7.362682663369924e-05\n",
      "step 606, loss = 4.792331674252637e-05\n",
      "step 607, loss = 8.588032505940646e-05\n",
      "step 608, loss = 6.783624849049374e-05\n",
      "step 609, loss = 8.354571764357388e-05\n",
      "step 610, loss = 7.978510984685272e-05\n",
      "step 611, loss = 6.83102261973545e-05\n",
      "step 612, loss = 7.723036105744541e-05\n",
      "step 613, loss = 5.348696140572429e-05\n",
      "step 614, loss = 7.534128963015974e-05\n",
      "step 615, loss = 9.730906458571553e-05\n",
      "step 616, loss = 8.277503366116434e-05\n",
      "step 617, loss = 9.870495705399662e-05\n",
      "step 618, loss = 0.00010971609299303964\n",
      "step 619, loss = 8.410382724832743e-05\n",
      "step 620, loss = 0.00013044972729403526\n",
      "step 621, loss = 7.837349403416738e-05\n",
      "step 622, loss = 6.733522604918107e-05\n",
      "step 623, loss = 6.494939589174464e-05\n",
      "step 624, loss = 7.517441554227844e-05\n",
      "step 625, loss = 7.30301981093362e-05\n",
      "step 626, loss = 6.34741663816385e-05\n",
      "step 627, loss = 8.451606845483184e-05\n",
      "step 628, loss = 8.79213766893372e-05\n",
      "step 629, loss = 8.169091597665101e-05\n",
      "step 630, loss = 9.453606617171317e-05\n",
      "step 631, loss = 7.534294854849577e-05\n",
      "step 632, loss = 8.171956142177805e-05\n",
      "step 633, loss = 7.816794095560908e-05\n",
      "step 634, loss = 7.194829959189519e-05\n",
      "step 635, loss = 0.0001121918176067993\n",
      "step 636, loss = 6.0753012803616e-05\n",
      "step 637, loss = 5.9144251281395555e-05\n",
      "step 638, loss = 9.155101724900305e-05\n",
      "step 639, loss = 5.7138091506203637e-05\n",
      "step 640, loss = 6.35702526778914e-05\n",
      "step 641, loss = 6.87076390022412e-05\n",
      "step 642, loss = 7.910372369224206e-05\n",
      "step 643, loss = 4.936627738061361e-05\n",
      "step 644, loss = 5.371198494685814e-05\n",
      "step 645, loss = 9.912555105984211e-05\n",
      "step 646, loss = 7.286526670213789e-05\n",
      "step 647, loss = 6.148965621832758e-05\n",
      "step 648, loss = 6.70858717057854e-05\n",
      "step 649, loss = 6.835712702013552e-05\n",
      "step 650, loss = 6.646323890890926e-05\n",
      "step 651, loss = 5.0847698730649427e-05\n",
      "step 652, loss = 4.654757140087895e-05\n",
      "step 653, loss = 5.004751437809318e-05\n",
      "step 654, loss = 7.234365330077708e-05\n",
      "step 655, loss = 7.487519178539515e-05\n",
      "step 656, loss = 8.310432167490944e-05\n",
      "step 657, loss = 7.128292054403573e-05\n",
      "step 658, loss = 7.610663305968046e-05\n",
      "step 659, loss = 8.460420212941244e-05\n",
      "step 660, loss = 0.0001058762427419424\n",
      "step 661, loss = 7.284349703695625e-05\n",
      "step 662, loss = 6.7080334702041e-05\n",
      "step 663, loss = 4.675445597968064e-05\n",
      "step 664, loss = 7.483601075364277e-05\n",
      "step 665, loss = 7.596685463795438e-05\n",
      "step 666, loss = 6.0935504734516144e-05\n",
      "step 667, loss = 5.978336412226781e-05\n",
      "step 668, loss = 8.587216143496335e-05\n",
      "step 669, loss = 8.426474232692271e-05\n",
      "step 670, loss = 8.168786007445306e-05\n",
      "step 671, loss = 8.063479617703706e-05\n",
      "step 672, loss = 8.201538730645552e-05\n",
      "step 673, loss = 0.0001150169555330649\n",
      "step 674, loss = 5.678504749084823e-05\n",
      "step 675, loss = 6.910759111633524e-05\n",
      "step 676, loss = 7.921580981928855e-05\n",
      "step 677, loss = 7.67610254115425e-05\n",
      "step 678, loss = 8.59493957250379e-05\n",
      "step 679, loss = 0.0001009205006994307\n",
      "step 680, loss = 7.88830075180158e-05\n",
      "step 681, loss = 8.22641741251573e-05\n",
      "step 682, loss = 6.471393862739205e-05\n",
      "step 683, loss = 7.768945943098515e-05\n",
      "step 684, loss = 8.602146408520639e-05\n",
      "step 685, loss = 5.4345357057172805e-05\n",
      "step 686, loss = 7.083954551490024e-05\n",
      "step 687, loss = 7.547852874267846e-05\n",
      "step 688, loss = 9.037301788339391e-05\n",
      "step 689, loss = 6.725825369358063e-05\n",
      "step 690, loss = 6.114682764746249e-05\n",
      "step 691, loss = 8.896545477909967e-05\n",
      "step 692, loss = 6.560202018590644e-05\n",
      "step 693, loss = 6.675397889921442e-05\n",
      "step 694, loss = 6.651187140960246e-05\n",
      "step 695, loss = 5.8238649216946214e-05\n",
      "step 696, loss = 7.340710726566613e-05\n",
      "step 697, loss = 6.005552859278396e-05\n",
      "step 698, loss = 6.230455619515851e-05\n",
      "step 699, loss = 7.617723167641088e-05\n",
      "step 700, loss = 8.037695079110563e-05\n",
      "step 701, loss = 7.652032218175009e-05\n",
      "step 702, loss = 8.14418526715599e-05\n",
      "step 703, loss = 8.972476643975824e-05\n",
      "step 704, loss = 7.838077726773918e-05\n",
      "step 705, loss = 8.775711467023939e-05\n",
      "step 706, loss = 7.526238186983392e-05\n",
      "step 707, loss = 9.569006215315312e-05\n",
      "step 708, loss = 8.584246097598225e-05\n",
      "step 709, loss = 7.69866892369464e-05\n",
      "step 710, loss = 8.059745596256107e-05\n",
      "step 711, loss = 8.678561425767839e-05\n",
      "step 712, loss = 7.053926674416289e-05\n",
      "step 713, loss = 8.09067496447824e-05\n",
      "step 714, loss = 9.01925377547741e-05\n",
      "step 715, loss = 7.711566286161542e-05\n",
      "step 716, loss = 5.81988351768814e-05\n",
      "step 717, loss = 5.209821028984152e-05\n",
      "step 718, loss = 7.346610800595954e-05\n",
      "step 719, loss = 7.544606341980398e-05\n",
      "step 720, loss = 7.096849731169641e-05\n",
      "step 721, loss = 7.41362091503106e-05\n",
      "step 722, loss = 8.55032994877547e-05\n",
      "step 723, loss = 8.6313666542992e-05\n",
      "step 724, loss = 5.93371078139171e-05\n",
      "step 725, loss = 6.905938789714128e-05\n",
      "step 726, loss = 8.791789150564e-05\n",
      "step 727, loss = 6.229257996892557e-05\n",
      "step 728, loss = 8.089274342637509e-05\n",
      "step 729, loss = 9.380687697557732e-05\n",
      "step 730, loss = 0.00010679692786652595\n",
      "step 731, loss = 9.660765499575064e-05\n",
      "step 732, loss = 7.262791768880561e-05\n",
      "step 733, loss = 0.00010576248314464465\n",
      "step 734, loss = 7.294541137525812e-05\n",
      "step 735, loss = 6.757779920008034e-05\n",
      "step 736, loss = 6.819597183493897e-05\n",
      "step 737, loss = 6.617610051762313e-05\n",
      "step 738, loss = 7.653339707758278e-05\n",
      "step 739, loss = 5.501613486558199e-05\n",
      "step 740, loss = 7.323415775317699e-05\n",
      "step 741, loss = 8.87066635186784e-05\n",
      "step 742, loss = 7.519437349401414e-05\n",
      "step 743, loss = 5.0169248424936086e-05\n",
      "step 744, loss = 7.211824413388968e-05\n",
      "step 745, loss = 7.76255110395141e-05\n",
      "step 746, loss = 7.090496364980936e-05\n",
      "step 747, loss = 7.99286353867501e-05\n",
      "step 748, loss = 7.607758016092703e-05\n",
      "step 749, loss = 4.811222243006341e-05\n",
      "step 750, loss = 5.73554279981181e-05\n",
      "step 751, loss = 7.629842002643272e-05\n",
      "step 752, loss = 6.962832412682474e-05\n",
      "step 753, loss = 7.696739339735359e-05\n",
      "step 754, loss = 7.295079558389261e-05\n",
      "step 755, loss = 8.804694516584277e-05\n",
      "step 756, loss = 7.478245242964476e-05\n",
      "step 757, loss = 7.477110193576664e-05\n",
      "step 758, loss = 6.840725836809725e-05\n",
      "step 759, loss = 7.713043305557221e-05\n",
      "step 760, loss = 8.60233703861013e-05\n",
      "step 761, loss = 8.073472417891026e-05\n",
      "step 762, loss = 6.943409243831411e-05\n",
      "step 763, loss = 7.419169560307637e-05\n",
      "step 764, loss = 7.93825020082295e-05\n",
      "step 765, loss = 8.030184108065441e-05\n",
      "step 766, loss = 6.760827818652615e-05\n",
      "step 767, loss = 5.5990822147578e-05\n",
      "step 768, loss = 9.462019079364836e-05\n",
      "step 769, loss = 7.967673445818946e-05\n",
      "step 770, loss = 9.224986570188776e-05\n",
      "step 771, loss = 6.260409281821921e-05\n",
      "step 772, loss = 6.975268479436636e-05\n",
      "step 773, loss = 5.5240416259039193e-05\n",
      "step 774, loss = 6.775269866921008e-05\n",
      "step 775, loss = 8.916585647966713e-05\n",
      "step 776, loss = 8.603136666351929e-05\n",
      "step 777, loss = 8.236760186264291e-05\n",
      "step 778, loss = 7.662257121410221e-05\n",
      "step 779, loss = 7.102132076397538e-05\n",
      "step 780, loss = 6.676907651126385e-05\n",
      "step 781, loss = 9.185224189423025e-05\n",
      "step 782, loss = 8.143089507939294e-05\n",
      "step 783, loss = 7.041950448183343e-05\n",
      "step 784, loss = 6.04650194873102e-05\n",
      "step 785, loss = 9.37532022362575e-05\n",
      "step 786, loss = 7.201559492386878e-05\n",
      "step 787, loss = 8.36221151985228e-05\n",
      "step 788, loss = 8.401709783356637e-05\n",
      "step 789, loss = 6.61466910969466e-05\n",
      "step 790, loss = 8.307780080940574e-05\n",
      "step 791, loss = 7.801490573910996e-05\n",
      "step 792, loss = 5.9392328694229946e-05\n",
      "step 793, loss = 7.2400041972287e-05\n",
      "step 794, loss = 9.064199548447505e-05\n",
      "step 795, loss = 8.14572922536172e-05\n",
      "step 796, loss = 5.285437509883195e-05\n",
      "step 797, loss = 8.465233986498788e-05\n",
      "step 798, loss = 0.00010170831228606403\n",
      "step 799, loss = 7.107603596523404e-05\n",
      "step 800, loss = 0.0001117562351282686\n",
      "step 801, loss = 9.178509208140895e-05\n",
      "step 802, loss = 8.402057574130595e-05\n",
      "step 803, loss = 8.004158007679507e-05\n",
      "step 804, loss = 0.00010197276424150914\n",
      "step 805, loss = 7.765416376059875e-05\n",
      "step 806, loss = 6.017350824549794e-05\n",
      "step 807, loss = 6.837877299403772e-05\n",
      "step 808, loss = 7.214022480184212e-05\n",
      "step 809, loss = 8.165738836396486e-05\n",
      "step 810, loss = 8.60035652294755e-05\n",
      "step 811, loss = 0.00010365548223489895\n",
      "step 812, loss = 5.1613063988042995e-05\n",
      "step 813, loss = 6.898312858538702e-05\n",
      "step 814, loss = 6.539733294630423e-05\n",
      "step 815, loss = 6.205634417710826e-05\n",
      "step 816, loss = 8.075490040937439e-05\n",
      "step 817, loss = 9.51832698774524e-05\n",
      "step 818, loss = 8.328555122716352e-05\n",
      "step 819, loss = 7.763269968563691e-05\n",
      "step 820, loss = 6.946154462639242e-05\n",
      "step 821, loss = 6.797722744522616e-05\n",
      "step 822, loss = 8.660852472530678e-05\n",
      "step 823, loss = 7.65010918257758e-05\n",
      "step 824, loss = 7.961225492181256e-05\n",
      "step 825, loss = 7.550562440883368e-05\n",
      "step 826, loss = 8.286251249955967e-05\n",
      "step 827, loss = 9.788793249754235e-05\n",
      "step 828, loss = 7.489731797249988e-05\n",
      "step 829, loss = 7.757957064313814e-05\n",
      "step 830, loss = 7.294802344404161e-05\n",
      "step 831, loss = 6.763613055227324e-05\n",
      "step 832, loss = 6.222100637387484e-05\n",
      "step 833, loss = 8.550497295800596e-05\n",
      "step 834, loss = 9.738645167089999e-05\n",
      "step 835, loss = 6.70538647682406e-05\n",
      "step 836, loss = 7.829857349861413e-05\n",
      "step 837, loss = 7.009854743955657e-05\n",
      "step 838, loss = 7.127158460207283e-05\n",
      "step 839, loss = 5.4729582188883796e-05\n",
      "step 840, loss = 7.26561265764758e-05\n",
      "step 841, loss = 6.954876153031364e-05\n",
      "step 842, loss = 7.004852523095906e-05\n",
      "step 843, loss = 6.445228063967079e-05\n",
      "step 844, loss = 5.89933515584562e-05\n",
      "step 845, loss = 7.259002450155094e-05\n",
      "step 846, loss = 5.095527012599632e-05\n",
      "step 847, loss = 7.255063974298537e-05\n",
      "step 848, loss = 9.932895773090422e-05\n",
      "step 849, loss = 9.390483319293708e-05\n",
      "step 850, loss = 8.384446846321225e-05\n",
      "step 851, loss = 8.675186109030619e-05\n",
      "step 852, loss = 7.628224557265639e-05\n",
      "step 853, loss = 9.263105312129483e-05\n",
      "step 854, loss = 0.00010678955732146278\n",
      "step 855, loss = 9.257191413780674e-05\n",
      "step 856, loss = 6.260778900468722e-05\n",
      "step 857, loss = 7.899871707195416e-05\n",
      "step 858, loss = 9.616402530809864e-05\n",
      "step 859, loss = 6.779557588743046e-05\n",
      "step 860, loss = 6.35579926893115e-05\n",
      "step 861, loss = 7.04651974956505e-05\n",
      "step 862, loss = 7.244045991683379e-05\n",
      "step 863, loss = 8.922689448809251e-05\n",
      "step 864, loss = 6.736111390637234e-05\n",
      "step 865, loss = 8.590744255343452e-05\n",
      "step 866, loss = 6.343167478917167e-05\n",
      "step 867, loss = 4.619365427060984e-05\n",
      "step 868, loss = 5.4647374781779945e-05\n",
      "step 869, loss = 8.260775939561427e-05\n",
      "step 870, loss = 7.667221507290378e-05\n",
      "step 871, loss = 9.469452197663486e-05\n",
      "step 872, loss = 9.157050953945145e-05\n",
      "step 873, loss = 7.233946962514892e-05\n",
      "step 874, loss = 7.042224751785398e-05\n",
      "step 875, loss = 8.73471872182563e-05\n",
      "step 876, loss = 7.387902587652206e-05\n",
      "step 877, loss = 9.060485172085464e-05\n",
      "step 878, loss = 7.771896343911067e-05\n",
      "step 879, loss = 7.7536613389384e-05\n",
      "step 880, loss = 6.681688682874665e-05\n",
      "step 881, loss = 7.916798494989052e-05\n",
      "step 882, loss = 5.8616344176698476e-05\n",
      "step 883, loss = 7.02224497217685e-05\n",
      "step 884, loss = 9.506161586614326e-05\n",
      "step 885, loss = 7.721974543528631e-05\n",
      "step 886, loss = 6.914343248354271e-05\n",
      "step 887, loss = 6.485260382760316e-05\n",
      "step 888, loss = 8.05626914370805e-05\n",
      "step 889, loss = 7.510101568186656e-05\n",
      "step 890, loss = 8.480458200210705e-05\n",
      "step 891, loss = 7.58144014980644e-05\n",
      "step 892, loss = 6.803763972129673e-05\n",
      "step 893, loss = 6.778156239306554e-05\n",
      "step 894, loss = 7.241957791848108e-05\n",
      "step 895, loss = 7.457019091816619e-05\n",
      "step 896, loss = 7.071350410114974e-05\n",
      "step 897, loss = 4.978939250577241e-05\n",
      "step 898, loss = 7.242891297210008e-05\n",
      "step 899, loss = 0.00010394967102911323\n",
      "step 900, loss = 7.859087054384872e-05\n",
      "step 901, loss = 8.209680527215824e-05\n",
      "step 902, loss = 7.237877434818074e-05\n",
      "step 903, loss = 7.840239413781092e-05\n",
      "step 904, loss = 4.480977077037096e-05\n",
      "step 905, loss = 0.00010416446457384154\n",
      "step 906, loss = 5.540234633372165e-05\n",
      "step 907, loss = 7.571942114736885e-05\n",
      "step 908, loss = 7.987656135810539e-05\n",
      "step 909, loss = 7.789823575876653e-05\n",
      "step 910, loss = 7.595083297928795e-05\n",
      "step 911, loss = 9.354358189739287e-05\n",
      "step 912, loss = 9.780118853086606e-05\n",
      "step 913, loss = 8.087298920145258e-05\n",
      "step 914, loss = 7.430746336467564e-05\n",
      "step 915, loss = 7.2072449256666e-05\n",
      "step 916, loss = 6.494492117781192e-05\n",
      "step 917, loss = 6.963912164792418e-05\n",
      "step 918, loss = 7.205533620435745e-05\n",
      "step 919, loss = 9.52918198890984e-05\n",
      "step 920, loss = 6.58403878333047e-05\n",
      "step 921, loss = 8.624503971077502e-05\n",
      "step 922, loss = 8.682349289301783e-05\n",
      "step 923, loss = 7.142508547985926e-05\n",
      "step 924, loss = 8.570152567699552e-05\n",
      "step 925, loss = 6.64246836095117e-05\n",
      "step 926, loss = 6.089506496209651e-05\n",
      "step 927, loss = 8.620696462458e-05\n",
      "step 928, loss = 7.962922245496884e-05\n",
      "step 929, loss = 7.56266963435337e-05\n",
      "step 930, loss = 5.0297450798098e-05\n",
      "step 931, loss = 0.00010136258788406849\n",
      "step 932, loss = 7.940521027194336e-05\n",
      "step 933, loss = 6.710673187626526e-05\n",
      "step 934, loss = 8.603125752415508e-05\n",
      "step 935, loss = 7.513027230743319e-05\n",
      "step 936, loss = 7.178636587923393e-05\n",
      "step 937, loss = 6.273704639170319e-05\n",
      "step 938, loss = 8.951694326242432e-05\n",
      "step 939, loss = 6.72335663693957e-05\n",
      "step 940, loss = 8.49229545565322e-05\n",
      "step 941, loss = 8.429689478361979e-05\n",
      "step 942, loss = 8.140174031723291e-05\n",
      "step 943, loss = 6.585039955098182e-05\n",
      "step 944, loss = 5.549755223910324e-05\n",
      "step 945, loss = 7.667897443752736e-05\n",
      "step 946, loss = 6.992079579504207e-05\n",
      "step 947, loss = 7.774117693770677e-05\n",
      "step 948, loss = 8.92976313480176e-05\n",
      "step 949, loss = 6.550025136675686e-05\n",
      "step 950, loss = 8.170487126335502e-05\n",
      "step 951, loss = 7.45523939258419e-05\n",
      "step 952, loss = 7.62418276281096e-05\n",
      "step 953, loss = 6.476150883827358e-05\n",
      "step 954, loss = 8.608361531514674e-05\n",
      "step 955, loss = 6.903314351802692e-05\n",
      "step 956, loss = 8.006408461369574e-05\n",
      "step 957, loss = 7.835156429791823e-05\n",
      "step 958, loss = 0.00011781411740230396\n",
      "step 959, loss = 8.98034413694404e-05\n",
      "step 960, loss = 4.661468119593337e-05\n",
      "step 961, loss = 8.68594361236319e-05\n",
      "step 962, loss = 0.00011052884656237438\n",
      "step 963, loss = 5.620619049295783e-05\n",
      "step 964, loss = 8.692990377312526e-05\n",
      "step 965, loss = 9.122247865889221e-05\n",
      "step 966, loss = 8.297473686980084e-05\n",
      "step 967, loss = 6.6105138103012e-05\n",
      "step 968, loss = 4.897465987596661e-05\n",
      "step 969, loss = 0.00013355838018469512\n",
      "step 970, loss = 7.247025496326387e-05\n",
      "step 971, loss = 8.630047523183748e-05\n",
      "step 972, loss = 0.00010227687744190916\n",
      "step 973, loss = 6.26490218564868e-05\n",
      "step 974, loss = 8.896127110347152e-05\n",
      "step 975, loss = 5.1588420319603756e-05\n",
      "step 976, loss = 7.583857222925872e-05\n",
      "step 977, loss = 6.628374831052497e-05\n",
      "step 978, loss = 6.348107126541436e-05\n",
      "step 979, loss = 8.636795973870903e-05\n",
      "step 980, loss = 5.1739407354034483e-05\n",
      "step 981, loss = 8.641151362098753e-05\n",
      "step 982, loss = 9.039521683007479e-05\n",
      "step 983, loss = 6.697225762763992e-05\n",
      "step 984, loss = 7.192575867520645e-05\n",
      "step 985, loss = 6.112832488724962e-05\n",
      "step 986, loss = 6.455158290918916e-05\n",
      "step 987, loss = 7.738052954664454e-05\n",
      "step 988, loss = 7.907706458354369e-05\n",
      "step 989, loss = 6.684591062366962e-05\n",
      "step 990, loss = 5.4934418585617095e-05\n",
      "step 991, loss = 8.86183261172846e-05\n",
      "step 992, loss = 5.7696270232554525e-05\n",
      "step 993, loss = 6.777099770260975e-05\n",
      "step 994, loss = 9.309033339377493e-05\n",
      "step 995, loss = 8.077410893747583e-05\n",
      "step 996, loss = 9.90501866908744e-05\n",
      "step 997, loss = 6.060666055418551e-05\n",
      "step 998, loss = 5.9155081544304267e-05\n",
      "step 999, loss = 6.502387259388342e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "n = 1000\n",
    "\n",
    "model = Model()\n",
    "optim = jt.nn.Adam(model.parameters(), learning_rate)\n",
    "begin = time.time()\n",
    "\n",
    "for i,(x,y) in enumerate(get_data(n)):\n",
    "    pred_y = model(x)\n",
    "    dy = pred_y - y\n",
    "    loss = dy * dy\n",
    "    loss_mean = loss.mean()\n",
    "    optim.step(loss_mean)\n",
    "    print(f\"step {i}, loss = {loss_mean.data.sum()}\")\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8518223762512207 s has passed.\n"
     ]
    }
   ],
   "source": [
    "print(end - begin, \"s has passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 1, 0.01).reshape(-1, 1)\n",
    "y = x ** 2\n",
    "pred_y = model(jt.float32(x)).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, 'o', color='orange')\n",
    "plt.plot(x, pred_y, '*', color='violet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the same model with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torchModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(torchModel, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(1, 10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(10, 1)\n",
    "    def forward(self, x) :\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "def get_torch_data(n): # generate random data for training test.\n",
    "    for i in range(n):\n",
    "        x = np.random.random((batch_size, 1))\n",
    "        x = x.astype('float32')\n",
    "        y = x*x\n",
    "        yield torch.from_numpy(x), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 0.16309650242328644\n",
      "step 1, loss = 0.11804714053869247\n",
      "step 2, loss = 0.08904750645160675\n",
      "step 3, loss = 0.08818919956684113\n",
      "step 4, loss = 0.052456606179475784\n",
      "step 5, loss = 0.031884822994470596\n",
      "step 6, loss = 0.01837017573416233\n",
      "step 7, loss = 0.011293799616396427\n",
      "step 8, loss = 0.016621766611933708\n",
      "step 9, loss = 0.014382285997271538\n",
      "step 10, loss = 0.019010668620467186\n",
      "step 11, loss = 0.02317330427467823\n",
      "step 12, loss = 0.024090476334095\n",
      "step 13, loss = 0.026724984869360924\n",
      "step 14, loss = 0.032467156648635864\n",
      "step 15, loss = 0.030342187732458115\n",
      "step 16, loss = 0.029297837987542152\n",
      "step 17, loss = 0.024923497810959816\n",
      "step 18, loss = 0.020184747874736786\n",
      "step 19, loss = 0.023437809199094772\n",
      "step 20, loss = 0.0159891527146101\n",
      "step 21, loss = 0.013066780753433704\n",
      "step 22, loss = 0.011788538657128811\n",
      "step 23, loss = 0.011095870286226273\n",
      "step 24, loss = 0.009196611121296883\n",
      "step 25, loss = 0.013667299412190914\n",
      "step 26, loss = 0.01132118608802557\n",
      "step 27, loss = 0.010268867015838623\n",
      "step 28, loss = 0.013147929683327675\n",
      "step 29, loss = 0.011550736613571644\n",
      "step 30, loss = 0.012258417904376984\n",
      "step 31, loss = 0.012451792135834694\n",
      "step 32, loss = 0.009960159659385681\n",
      "step 33, loss = 0.009463177993893623\n",
      "step 34, loss = 0.005545611493289471\n",
      "step 35, loss = 0.0083102872595191\n",
      "step 36, loss = 0.008300519548356533\n",
      "step 37, loss = 0.013851280324161053\n",
      "step 38, loss = 0.01032053492963314\n",
      "step 39, loss = 0.00584821542724967\n",
      "step 40, loss = 0.006292037200182676\n",
      "step 41, loss = 0.011709235608577728\n",
      "step 42, loss = 0.008193738758563995\n",
      "step 43, loss = 0.007060663774609566\n",
      "step 44, loss = 0.008296170271933079\n",
      "step 45, loss = 0.00773257901892066\n",
      "step 46, loss = 0.007359168026596308\n",
      "step 47, loss = 0.00688664335757494\n",
      "step 48, loss = 0.0057983105070889\n",
      "step 49, loss = 0.008836766704916954\n",
      "step 50, loss = 0.0060515147633850574\n",
      "step 51, loss = 0.006040504667907953\n",
      "step 52, loss = 0.005510604940354824\n",
      "step 53, loss = 0.004702263046056032\n",
      "step 54, loss = 0.00712632667273283\n",
      "step 55, loss = 0.0038482188247144222\n",
      "step 56, loss = 0.006799991242587566\n",
      "step 57, loss = 0.00636767502874136\n",
      "step 58, loss = 0.008919176645576954\n",
      "step 59, loss = 0.006193750072270632\n",
      "step 60, loss = 0.00427683349698782\n",
      "step 61, loss = 0.005567498039454222\n",
      "step 62, loss = 0.004597972147166729\n",
      "step 63, loss = 0.00504731759428978\n",
      "step 64, loss = 0.005102100316435099\n",
      "step 65, loss = 0.005366042722016573\n",
      "step 66, loss = 0.006446100305765867\n",
      "step 67, loss = 0.005482379347085953\n",
      "step 68, loss = 0.004953586962074041\n",
      "step 69, loss = 0.0054296934977173805\n",
      "step 70, loss = 0.005482971202582121\n",
      "step 71, loss = 0.0053231120109558105\n",
      "step 72, loss = 0.005942380987107754\n",
      "step 73, loss = 0.00712671410292387\n",
      "step 74, loss = 0.005468856077641249\n",
      "step 75, loss = 0.006657700520008802\n",
      "step 76, loss = 0.004837573505938053\n",
      "step 77, loss = 0.007502783555537462\n",
      "step 78, loss = 0.006108066998422146\n",
      "step 79, loss = 0.006111800204962492\n",
      "step 80, loss = 0.004633508622646332\n",
      "step 81, loss = 0.006221456918865442\n",
      "step 82, loss = 0.005804277025163174\n",
      "step 83, loss = 0.005365923047065735\n",
      "step 84, loss = 0.005667941644787788\n",
      "step 85, loss = 0.007423958741128445\n",
      "step 86, loss = 0.005507689900696278\n",
      "step 87, loss = 0.005164213944226503\n",
      "step 88, loss = 0.0044433060102164745\n",
      "step 89, loss = 0.006058565340936184\n",
      "step 90, loss = 0.005767937283962965\n",
      "step 91, loss = 0.005160849075764418\n",
      "step 92, loss = 0.005371949635446072\n",
      "step 93, loss = 0.005763042718172073\n",
      "step 94, loss = 0.005242803134024143\n",
      "step 95, loss = 0.005338129587471485\n",
      "step 96, loss = 0.003835364943370223\n",
      "step 97, loss = 0.004778964910656214\n",
      "step 98, loss = 0.007782672066241503\n",
      "step 99, loss = 0.003533115377649665\n",
      "step 100, loss = 0.004730225075036287\n",
      "step 101, loss = 0.0072325728833675385\n",
      "step 102, loss = 0.004670445807278156\n",
      "step 103, loss = 0.004996516276150942\n",
      "step 104, loss = 0.005328970029950142\n",
      "step 105, loss = 0.005728540010750294\n",
      "step 106, loss = 0.005632371176034212\n",
      "step 107, loss = 0.006330189760774374\n",
      "step 108, loss = 0.006717073265463114\n",
      "step 109, loss = 0.005756226368248463\n",
      "step 110, loss = 0.004566923249512911\n",
      "step 111, loss = 0.005522218532860279\n",
      "step 112, loss = 0.005091164261102676\n",
      "step 113, loss = 0.005283427890390158\n",
      "step 114, loss = 0.005651721730828285\n",
      "step 115, loss = 0.005081855226308107\n",
      "step 116, loss = 0.006853616796433926\n",
      "step 117, loss = 0.006077110301703215\n",
      "step 118, loss = 0.0066278912127017975\n",
      "step 119, loss = 0.0063704694621264935\n",
      "step 120, loss = 0.005403449758887291\n",
      "step 121, loss = 0.0052818115800619125\n",
      "step 122, loss = 0.005159835796803236\n",
      "step 123, loss = 0.004852509591728449\n",
      "step 124, loss = 0.00490100821480155\n",
      "step 125, loss = 0.0072392928414046764\n",
      "step 126, loss = 0.006671592593193054\n",
      "step 127, loss = 0.0050615109503269196\n",
      "step 128, loss = 0.005435924045741558\n",
      "step 129, loss = 0.00600965041667223\n",
      "step 130, loss = 0.006496605463325977\n",
      "step 131, loss = 0.006325619760900736\n",
      "step 132, loss = 0.005629479419440031\n",
      "step 133, loss = 0.005346267018467188\n",
      "step 134, loss = 0.004055103752762079\n",
      "step 135, loss = 0.005198550410568714\n",
      "step 136, loss = 0.005033318419009447\n",
      "step 137, loss = 0.00503922626376152\n",
      "step 138, loss = 0.005234923213720322\n",
      "step 139, loss = 0.005078503862023354\n",
      "step 140, loss = 0.004592910874634981\n",
      "step 141, loss = 0.004626594018191099\n",
      "step 142, loss = 0.007358146831393242\n",
      "step 143, loss = 0.00728426082059741\n",
      "step 144, loss = 0.0052128201350569725\n",
      "step 145, loss = 0.005438936874270439\n",
      "step 146, loss = 0.004609961062669754\n",
      "step 147, loss = 0.00521636800840497\n",
      "step 148, loss = 0.005845383275300264\n",
      "step 149, loss = 0.005343173630535603\n",
      "step 150, loss = 0.0063232495449483395\n",
      "step 151, loss = 0.006969132460653782\n",
      "step 152, loss = 0.0051660542376339436\n",
      "step 153, loss = 0.0056022037751972675\n",
      "step 154, loss = 0.005609495565295219\n",
      "step 155, loss = 0.00505102938041091\n",
      "step 156, loss = 0.0056435726583004\n",
      "step 157, loss = 0.006780280265957117\n",
      "step 158, loss = 0.003784534288570285\n",
      "step 159, loss = 0.0049583762884140015\n",
      "step 160, loss = 0.004236890468746424\n",
      "step 161, loss = 0.005294024478644133\n",
      "step 162, loss = 0.0054163504391908646\n",
      "step 163, loss = 0.006194180343300104\n",
      "step 164, loss = 0.005265173967927694\n",
      "step 165, loss = 0.005799965467303991\n",
      "step 166, loss = 0.00567565718665719\n",
      "step 167, loss = 0.005997179541736841\n",
      "step 168, loss = 0.007199140731245279\n",
      "step 169, loss = 0.005468863993883133\n",
      "step 170, loss = 0.004862778354436159\n",
      "step 171, loss = 0.004447875544428825\n",
      "step 172, loss = 0.005289663095027208\n",
      "step 173, loss = 0.005914853885769844\n",
      "step 174, loss = 0.006560150533914566\n",
      "step 175, loss = 0.007573127280920744\n",
      "step 176, loss = 0.00613750983029604\n",
      "step 177, loss = 0.006480527576059103\n",
      "step 178, loss = 0.00579434260725975\n",
      "step 179, loss = 0.004960712045431137\n",
      "step 180, loss = 0.0037474145647138357\n",
      "step 181, loss = 0.005018596537411213\n",
      "step 182, loss = 0.006384862121194601\n",
      "step 183, loss = 0.005102658644318581\n",
      "step 184, loss = 0.004471353255212307\n",
      "step 185, loss = 0.005310702137649059\n",
      "step 186, loss = 0.005741702392697334\n",
      "step 187, loss = 0.005051454994827509\n",
      "step 188, loss = 0.005840267986059189\n",
      "step 189, loss = 0.005084633827209473\n",
      "step 190, loss = 0.006048566661775112\n",
      "step 191, loss = 0.006403058767318726\n",
      "step 192, loss = 0.007355787325650454\n",
      "step 193, loss = 0.007678167428821325\n",
      "step 194, loss = 0.005817110650241375\n",
      "step 195, loss = 0.00450547318905592\n",
      "step 196, loss = 0.006026493385434151\n",
      "step 197, loss = 0.005595456343144178\n",
      "step 198, loss = 0.004929247312247753\n",
      "step 199, loss = 0.004938701167702675\n",
      "step 200, loss = 0.005300926044583321\n",
      "step 201, loss = 0.005664349999278784\n",
      "step 202, loss = 0.006225393619388342\n",
      "step 203, loss = 0.00533524714410305\n",
      "step 204, loss = 0.006170597858726978\n",
      "step 205, loss = 0.006270470563322306\n",
      "step 206, loss = 0.006399547681212425\n",
      "step 207, loss = 0.005116413347423077\n",
      "step 208, loss = 0.006100882776081562\n",
      "step 209, loss = 0.006328179966658354\n",
      "step 210, loss = 0.00715204793959856\n",
      "step 211, loss = 0.006029491312801838\n",
      "step 212, loss = 0.005187812261283398\n",
      "step 213, loss = 0.004856071900576353\n",
      "step 214, loss = 0.0062189786694943905\n",
      "step 215, loss = 0.007579603232443333\n",
      "step 216, loss = 0.005475729703903198\n",
      "step 217, loss = 0.0063238865695893764\n",
      "step 218, loss = 0.005457667168229818\n",
      "step 219, loss = 0.00541134737432003\n",
      "step 220, loss = 0.005882975645363331\n",
      "step 221, loss = 0.00630517303943634\n",
      "step 222, loss = 0.005002114921808243\n",
      "step 223, loss = 0.005352440290153027\n",
      "step 224, loss = 0.005211216397583485\n",
      "step 225, loss = 0.003999159671366215\n",
      "step 226, loss = 0.0063475086353719234\n",
      "step 227, loss = 0.006383816711604595\n",
      "step 228, loss = 0.005674042738974094\n",
      "step 229, loss = 0.008339709602296352\n",
      "step 230, loss = 0.004404009785503149\n",
      "step 231, loss = 0.0054760100319981575\n",
      "step 232, loss = 0.005335588473826647\n",
      "step 233, loss = 0.006853265222162008\n",
      "step 234, loss = 0.007385819684714079\n",
      "step 235, loss = 0.004687918350100517\n",
      "step 236, loss = 0.006262147333472967\n",
      "step 237, loss = 0.005256501492112875\n",
      "step 238, loss = 0.0051296609453856945\n",
      "step 239, loss = 0.006513829343020916\n",
      "step 240, loss = 0.005304904654622078\n",
      "step 241, loss = 0.0068239509128034115\n",
      "step 242, loss = 0.004005739465355873\n",
      "step 243, loss = 0.005159750580787659\n",
      "step 244, loss = 0.006263573653995991\n",
      "step 245, loss = 0.0061088972724974155\n",
      "step 246, loss = 0.006117350421845913\n",
      "step 247, loss = 0.006062803789973259\n",
      "step 248, loss = 0.007263043895363808\n",
      "step 249, loss = 0.00558675779029727\n",
      "step 250, loss = 0.007027957588434219\n",
      "step 251, loss = 0.006513488944619894\n",
      "step 252, loss = 0.006470031570643187\n",
      "step 253, loss = 0.006378087215125561\n",
      "step 254, loss = 0.005728999152779579\n",
      "step 255, loss = 0.004835575819015503\n",
      "step 256, loss = 0.005433708429336548\n",
      "step 257, loss = 0.0059096054174005985\n",
      "step 258, loss = 0.005936203524470329\n",
      "step 259, loss = 0.005350349470973015\n",
      "step 260, loss = 0.005500737577676773\n",
      "step 261, loss = 0.0053459350019693375\n",
      "step 262, loss = 0.0051362598314881325\n",
      "step 263, loss = 0.0058154757134616375\n",
      "step 264, loss = 0.005547861568629742\n",
      "step 265, loss = 0.006344103254377842\n",
      "step 266, loss = 0.005472320131957531\n",
      "step 267, loss = 0.00530970748513937\n",
      "step 268, loss = 0.005546559579670429\n",
      "step 269, loss = 0.004070997703820467\n",
      "step 270, loss = 0.004389534704387188\n",
      "step 271, loss = 0.004975395277142525\n",
      "step 272, loss = 0.004936364479362965\n",
      "step 273, loss = 0.0055094752460718155\n",
      "step 274, loss = 0.006136848125606775\n",
      "step 275, loss = 0.005700503010302782\n",
      "step 276, loss = 0.007014676462858915\n",
      "step 277, loss = 0.00748461252078414\n",
      "step 278, loss = 0.005666960496455431\n",
      "step 279, loss = 0.004991999361664057\n",
      "step 280, loss = 0.006211169995367527\n",
      "step 281, loss = 0.005958223715424538\n",
      "step 282, loss = 0.004728576634079218\n",
      "step 283, loss = 0.004803430289030075\n",
      "step 284, loss = 0.005901670549064875\n",
      "step 285, loss = 0.0057994043454527855\n",
      "step 286, loss = 0.00438964506611228\n",
      "step 287, loss = 0.00634688138961792\n",
      "step 288, loss = 0.005959801841527224\n",
      "step 289, loss = 0.007334911730140448\n",
      "step 290, loss = 0.005017750896513462\n",
      "step 291, loss = 0.0051882280968129635\n",
      "step 292, loss = 0.004541516769677401\n",
      "step 293, loss = 0.006358240265399218\n",
      "step 294, loss = 0.005358533933758736\n",
      "step 295, loss = 0.005644833669066429\n",
      "step 296, loss = 0.005196227226406336\n",
      "step 297, loss = 0.0046496824361383915\n",
      "step 298, loss = 0.006533438339829445\n",
      "step 299, loss = 0.007509835530072451\n",
      "step 300, loss = 0.005625147372484207\n",
      "step 301, loss = 0.005917860195040703\n",
      "step 302, loss = 0.0072503965348005295\n",
      "step 303, loss = 0.006611678749322891\n",
      "step 304, loss = 0.005444089882075787\n",
      "step 305, loss = 0.0062831551767885685\n",
      "step 306, loss = 0.006550917401909828\n",
      "step 307, loss = 0.006702721118927002\n",
      "step 308, loss = 0.004844629671424627\n",
      "step 309, loss = 0.006055319216102362\n",
      "step 310, loss = 0.004988929256796837\n",
      "step 311, loss = 0.005429141689091921\n",
      "step 312, loss = 0.006284384522587061\n",
      "step 313, loss = 0.0065299514681100845\n",
      "step 314, loss = 0.006731415167450905\n",
      "step 315, loss = 0.0049074129201471806\n",
      "step 316, loss = 0.006495686713606119\n",
      "step 317, loss = 0.0055193412117660046\n",
      "step 318, loss = 0.00697381841018796\n",
      "step 319, loss = 0.005412058439105749\n",
      "step 320, loss = 0.006677186582237482\n",
      "step 321, loss = 0.006638181395828724\n",
      "step 322, loss = 0.008005681447684765\n",
      "step 323, loss = 0.004218578804284334\n",
      "step 324, loss = 0.005157236009836197\n",
      "step 325, loss = 0.004849566146731377\n",
      "step 326, loss = 0.006436330731958151\n",
      "step 327, loss = 0.008286915719509125\n",
      "step 328, loss = 0.006298848893493414\n",
      "step 329, loss = 0.005499524995684624\n",
      "step 330, loss = 0.005013477988541126\n",
      "step 331, loss = 0.00587103795260191\n",
      "step 332, loss = 0.005741131491959095\n",
      "step 333, loss = 0.006757047027349472\n",
      "step 334, loss = 0.0057450453750789165\n",
      "step 335, loss = 0.005624543875455856\n",
      "step 336, loss = 0.006174380891025066\n",
      "step 337, loss = 0.0048540858551859856\n",
      "step 338, loss = 0.006205749697983265\n",
      "step 339, loss = 0.005508216563612223\n",
      "step 340, loss = 0.005737559869885445\n",
      "step 341, loss = 0.0052843824960291386\n",
      "step 342, loss = 0.005837824195623398\n",
      "step 343, loss = 0.004951244220137596\n",
      "step 344, loss = 0.005398388486355543\n",
      "step 345, loss = 0.005380480084568262\n",
      "step 346, loss = 0.004507842473685741\n",
      "step 347, loss = 0.005532943177968264\n",
      "step 348, loss = 0.004315121565014124\n",
      "step 349, loss = 0.0050460039637982845\n",
      "step 350, loss = 0.008159598335623741\n",
      "step 351, loss = 0.004200748633593321\n",
      "step 352, loss = 0.0047815172001719475\n",
      "step 353, loss = 0.0049881888553500175\n",
      "step 354, loss = 0.005834827199578285\n",
      "step 355, loss = 0.004791382234543562\n",
      "step 356, loss = 0.0058210003189742565\n",
      "step 357, loss = 0.004484700039029121\n",
      "step 358, loss = 0.0054783993400633335\n",
      "step 359, loss = 0.0064163124188780785\n",
      "step 360, loss = 0.006155235692858696\n",
      "step 361, loss = 0.004221715964376926\n",
      "step 362, loss = 0.004844937939196825\n",
      "step 363, loss = 0.005856586154550314\n",
      "step 364, loss = 0.006572452839463949\n",
      "step 365, loss = 0.006155233830213547\n",
      "step 366, loss = 0.004758897703140974\n",
      "step 367, loss = 0.00482788635417819\n",
      "step 368, loss = 0.004825065843760967\n",
      "step 369, loss = 0.005846461281180382\n",
      "step 370, loss = 0.006732725538313389\n",
      "step 371, loss = 0.005553411319851875\n",
      "step 372, loss = 0.006265935488045216\n",
      "step 373, loss = 0.0062575270421803\n",
      "step 374, loss = 0.0051627811044454575\n",
      "step 375, loss = 0.006462992634624243\n",
      "step 376, loss = 0.005564563907682896\n",
      "step 377, loss = 0.005447538569569588\n",
      "step 378, loss = 0.004659409634768963\n",
      "step 379, loss = 0.006123221013695002\n",
      "step 380, loss = 0.005681577138602734\n",
      "step 381, loss = 0.00670374371111393\n",
      "step 382, loss = 0.005166936665773392\n",
      "step 383, loss = 0.006136806216090918\n",
      "step 384, loss = 0.0046899509616196156\n",
      "step 385, loss = 0.005644556134939194\n",
      "step 386, loss = 0.005629579536616802\n",
      "step 387, loss = 0.004634874407202005\n",
      "step 388, loss = 0.006657201796770096\n",
      "step 389, loss = 0.005820667836815119\n",
      "step 390, loss = 0.005563529673963785\n",
      "step 391, loss = 0.0057822223752737045\n",
      "step 392, loss = 0.006045660935342312\n",
      "step 393, loss = 0.004635728895664215\n",
      "step 394, loss = 0.004801756236702204\n",
      "step 395, loss = 0.006334841717034578\n",
      "step 396, loss = 0.005010657478123903\n",
      "step 397, loss = 0.006196187809109688\n",
      "step 398, loss = 0.004332230892032385\n",
      "step 399, loss = 0.005648827645927668\n",
      "step 400, loss = 0.004457963164895773\n",
      "step 401, loss = 0.005692990962415934\n",
      "step 402, loss = 0.004415395203977823\n",
      "step 403, loss = 0.00485406257212162\n",
      "step 404, loss = 0.005323253106325865\n",
      "step 405, loss = 0.006702281069010496\n",
      "step 406, loss = 0.004414959345012903\n",
      "step 407, loss = 0.006496617570519447\n",
      "step 408, loss = 0.00551765039563179\n",
      "step 409, loss = 0.004713371861726046\n",
      "step 410, loss = 0.004912299104034901\n",
      "step 411, loss = 0.004764819052070379\n",
      "step 412, loss = 0.0063021196983754635\n",
      "step 413, loss = 0.004860743880271912\n",
      "step 414, loss = 0.00399325555190444\n",
      "step 415, loss = 0.006189029198139906\n",
      "step 416, loss = 0.007721488364040852\n",
      "step 417, loss = 0.0051688686944544315\n",
      "step 418, loss = 0.005457031540572643\n",
      "step 419, loss = 0.0055990261025726795\n",
      "step 420, loss = 0.0051450361497700214\n",
      "step 421, loss = 0.005408783443272114\n",
      "step 422, loss = 0.004939079750329256\n",
      "step 423, loss = 0.007227490656077862\n",
      "step 424, loss = 0.004170547239482403\n",
      "step 425, loss = 0.004829532001167536\n",
      "step 426, loss = 0.005766512360423803\n",
      "step 427, loss = 0.005030143074691296\n",
      "step 428, loss = 0.005944530479609966\n",
      "step 429, loss = 0.006403661798685789\n",
      "step 430, loss = 0.004651321563869715\n",
      "step 431, loss = 0.003357300767675042\n",
      "step 432, loss = 0.005144317168742418\n",
      "step 433, loss = 0.006013012491166592\n",
      "step 434, loss = 0.006479526869952679\n",
      "step 435, loss = 0.005148109048604965\n",
      "step 436, loss = 0.006539695430546999\n",
      "step 437, loss = 0.005820414051413536\n",
      "step 438, loss = 0.005859498865902424\n",
      "step 439, loss = 0.005326007027179003\n",
      "step 440, loss = 0.005383134353905916\n",
      "step 441, loss = 0.006698084995150566\n",
      "step 442, loss = 0.004952081013470888\n",
      "step 443, loss = 0.004773598629981279\n",
      "step 444, loss = 0.005591569934040308\n",
      "step 445, loss = 0.0052195205353200436\n",
      "step 446, loss = 0.005112094338983297\n",
      "step 447, loss = 0.005695672240108252\n",
      "step 448, loss = 0.00469325203448534\n",
      "step 449, loss = 0.004398144315928221\n",
      "step 450, loss = 0.007892780937254429\n",
      "step 451, loss = 0.006741008721292019\n",
      "step 452, loss = 0.00468965619802475\n",
      "step 453, loss = 0.00571813527494669\n",
      "step 454, loss = 0.004210198298096657\n",
      "step 455, loss = 0.00512249069288373\n",
      "step 456, loss = 0.004653994459658861\n",
      "step 457, loss = 0.005513249430805445\n",
      "step 458, loss = 0.006825138349086046\n",
      "step 459, loss = 0.006074994802474976\n",
      "step 460, loss = 0.006431336514651775\n",
      "step 461, loss = 0.005231350660324097\n",
      "step 462, loss = 0.005402419716119766\n",
      "step 463, loss = 0.006453740410506725\n",
      "step 464, loss = 0.004633674398064613\n",
      "step 465, loss = 0.005800318904221058\n",
      "step 466, loss = 0.005521124694496393\n",
      "step 467, loss = 0.006435379385948181\n",
      "step 468, loss = 0.0065149893052875996\n",
      "step 469, loss = 0.005805982742458582\n",
      "step 470, loss = 0.006647670175880194\n",
      "step 471, loss = 0.005814041942358017\n",
      "step 472, loss = 0.006003614515066147\n",
      "step 473, loss = 0.006349897477775812\n",
      "step 474, loss = 0.004365867003798485\n",
      "step 475, loss = 0.004937129095196724\n",
      "step 476, loss = 0.006288252305239439\n",
      "step 477, loss = 0.005601718556135893\n",
      "step 478, loss = 0.005322449840605259\n",
      "step 479, loss = 0.005209244322031736\n",
      "step 480, loss = 0.0053419978357851505\n",
      "step 481, loss = 0.005442823749035597\n",
      "step 482, loss = 0.005278740078210831\n",
      "step 483, loss = 0.0054404656402766705\n",
      "step 484, loss = 0.005574852228164673\n",
      "step 485, loss = 0.004155522212386131\n",
      "step 486, loss = 0.005061279982328415\n",
      "step 487, loss = 0.005735327489674091\n",
      "step 488, loss = 0.005934635177254677\n",
      "step 489, loss = 0.005463128909468651\n",
      "step 490, loss = 0.007164611015468836\n",
      "step 491, loss = 0.006143081933259964\n",
      "step 492, loss = 0.0056554703041911125\n",
      "step 493, loss = 0.006748585496097803\n",
      "step 494, loss = 0.0070468829944729805\n",
      "step 495, loss = 0.004712067078799009\n",
      "step 496, loss = 0.00551175931468606\n",
      "step 497, loss = 0.006277760956436396\n",
      "step 498, loss = 0.005569702014327049\n",
      "step 499, loss = 0.005082135088741779\n",
      "step 500, loss = 0.006035636644810438\n",
      "step 501, loss = 0.005890357308089733\n",
      "step 502, loss = 0.005557188764214516\n",
      "step 503, loss = 0.006250895094126463\n",
      "step 504, loss = 0.0057580298744142056\n",
      "step 505, loss = 0.004513157531619072\n",
      "step 506, loss = 0.007146606221795082\n",
      "step 507, loss = 0.006232857704162598\n",
      "step 508, loss = 0.005567348562180996\n",
      "step 509, loss = 0.005686402320861816\n",
      "step 510, loss = 0.0070420545525848866\n",
      "step 511, loss = 0.005995885003358126\n",
      "step 512, loss = 0.00603424571454525\n",
      "step 513, loss = 0.005875942297279835\n",
      "step 514, loss = 0.005469849798828363\n",
      "step 515, loss = 0.0036420298274606466\n",
      "step 516, loss = 0.0054857321083545685\n",
      "step 517, loss = 0.005803033709526062\n",
      "step 518, loss = 0.004788578953593969\n",
      "step 519, loss = 0.0064814831130206585\n",
      "step 520, loss = 0.004623026587069035\n",
      "step 521, loss = 0.00788718182593584\n",
      "step 522, loss = 0.005712903570383787\n",
      "step 523, loss = 0.005604919046163559\n",
      "step 524, loss = 0.007483239751309156\n",
      "step 525, loss = 0.006084844470024109\n",
      "step 526, loss = 0.00671739224344492\n",
      "step 527, loss = 0.005194691941142082\n",
      "step 528, loss = 0.005744414869695902\n",
      "step 529, loss = 0.006592416204512119\n",
      "step 530, loss = 0.007278646342456341\n",
      "step 531, loss = 0.005677152890712023\n",
      "step 532, loss = 0.006080774124711752\n",
      "step 533, loss = 0.0062906681559979916\n",
      "step 534, loss = 0.005464677233248949\n",
      "step 535, loss = 0.006653657648712397\n",
      "step 536, loss = 0.006754888221621513\n",
      "step 537, loss = 0.005562540143728256\n",
      "step 538, loss = 0.007081195712089539\n",
      "step 539, loss = 0.0074888006784021854\n",
      "step 540, loss = 0.006992155686020851\n",
      "step 541, loss = 0.005900613032281399\n",
      "step 542, loss = 0.004695258568972349\n",
      "step 543, loss = 0.006662365980446339\n",
      "step 544, loss = 0.006370946764945984\n",
      "step 545, loss = 0.007642990443855524\n",
      "step 546, loss = 0.004545131232589483\n",
      "step 547, loss = 0.004904961213469505\n",
      "step 548, loss = 0.005058918613940477\n",
      "step 549, loss = 0.005675592925399542\n",
      "step 550, loss = 0.005305162165313959\n",
      "step 551, loss = 0.005490628536790609\n",
      "step 552, loss = 0.0061109550297260284\n",
      "step 553, loss = 0.0046236468479037285\n",
      "step 554, loss = 0.004914278630167246\n",
      "step 555, loss = 0.005350291728973389\n",
      "step 556, loss = 0.0046748206950724125\n",
      "step 557, loss = 0.005279370583593845\n",
      "step 558, loss = 0.005562193226069212\n",
      "step 559, loss = 0.006860264576971531\n",
      "step 560, loss = 0.004603673238307238\n",
      "step 561, loss = 0.00532064912840724\n",
      "step 562, loss = 0.006093338597565889\n",
      "step 563, loss = 0.0043446714989840984\n",
      "step 564, loss = 0.0056665376760065556\n",
      "step 565, loss = 0.005785152316093445\n",
      "step 566, loss = 0.004680377431213856\n",
      "step 567, loss = 0.004915512632578611\n",
      "step 568, loss = 0.0072265989147126675\n",
      "step 569, loss = 0.007161128334701061\n",
      "step 570, loss = 0.005061475560069084\n",
      "step 571, loss = 0.005124798510223627\n",
      "step 572, loss = 0.006081318948417902\n",
      "step 573, loss = 0.00442692032083869\n",
      "step 574, loss = 0.006348701193928719\n",
      "step 575, loss = 0.005941492505371571\n",
      "step 576, loss = 0.005501941312104464\n",
      "step 577, loss = 0.006260529160499573\n",
      "step 578, loss = 0.005983482114970684\n",
      "step 579, loss = 0.00460141571238637\n",
      "step 580, loss = 0.0042261057533323765\n",
      "step 581, loss = 0.004887651652097702\n",
      "step 582, loss = 0.008063869550824165\n",
      "step 583, loss = 0.005320054944604635\n",
      "step 584, loss = 0.004027788061648607\n",
      "step 585, loss = 0.004135883878916502\n",
      "step 586, loss = 0.005465633701533079\n",
      "step 587, loss = 0.00572784012183547\n",
      "step 588, loss = 0.005916230846196413\n",
      "step 589, loss = 0.004883787594735622\n",
      "step 590, loss = 0.005557240918278694\n",
      "step 591, loss = 0.005861011333763599\n",
      "step 592, loss = 0.0058508217334747314\n",
      "step 593, loss = 0.005041627213358879\n",
      "step 594, loss = 0.0065662371926009655\n",
      "step 595, loss = 0.005443851463496685\n",
      "step 596, loss = 0.008367691189050674\n",
      "step 597, loss = 0.003982387948781252\n",
      "step 598, loss = 0.006389510817825794\n",
      "step 599, loss = 0.0050852918066084385\n",
      "step 600, loss = 0.007448820862919092\n",
      "step 601, loss = 0.004698583856225014\n",
      "step 602, loss = 0.005922750104218721\n",
      "step 603, loss = 0.004453946370631456\n",
      "step 604, loss = 0.005195323843508959\n",
      "step 605, loss = 0.005394247826188803\n",
      "step 606, loss = 0.003963354043662548\n",
      "step 607, loss = 0.006621101871132851\n",
      "step 608, loss = 0.004327055066823959\n",
      "step 609, loss = 0.006268239114433527\n",
      "step 610, loss = 0.005447226110845804\n",
      "step 611, loss = 0.0048147644847631454\n",
      "step 612, loss = 0.004892479162663221\n",
      "step 613, loss = 0.006873039528727531\n",
      "step 614, loss = 0.005672762170433998\n",
      "step 615, loss = 0.0036167947109788656\n",
      "step 616, loss = 0.00504842447116971\n",
      "step 617, loss = 0.004921558313071728\n",
      "step 618, loss = 0.0061658998019993305\n",
      "step 619, loss = 0.005037405993789434\n",
      "step 620, loss = 0.004595210775732994\n",
      "step 621, loss = 0.004868687596172094\n",
      "step 622, loss = 0.005096262786537409\n",
      "step 623, loss = 0.005008661653846502\n",
      "step 624, loss = 0.005504188127815723\n",
      "step 625, loss = 0.005466827657073736\n",
      "step 626, loss = 0.006700763013213873\n",
      "step 627, loss = 0.0067550004459917545\n",
      "step 628, loss = 0.006013229489326477\n",
      "step 629, loss = 0.0043217637576162815\n",
      "step 630, loss = 0.004480736795812845\n",
      "step 631, loss = 0.005167365074157715\n",
      "step 632, loss = 0.0043767220340669155\n",
      "step 633, loss = 0.005839681252837181\n",
      "step 634, loss = 0.004578492604196072\n",
      "step 635, loss = 0.0056816888973116875\n",
      "step 636, loss = 0.006810364313423634\n",
      "step 637, loss = 0.004755493253469467\n",
      "step 638, loss = 0.005485843401402235\n",
      "step 639, loss = 0.005116564687341452\n",
      "step 640, loss = 0.006307920441031456\n",
      "step 641, loss = 0.005514024291187525\n",
      "step 642, loss = 0.004944012500345707\n",
      "step 643, loss = 0.005304924212396145\n",
      "step 644, loss = 0.005548339337110519\n",
      "step 645, loss = 0.0064514074474573135\n",
      "step 646, loss = 0.005859436001628637\n",
      "step 647, loss = 0.006290767807513475\n",
      "step 648, loss = 0.004686024971306324\n",
      "step 649, loss = 0.005767288617789745\n",
      "step 650, loss = 0.004975851625204086\n",
      "step 651, loss = 0.0066283829510211945\n",
      "step 652, loss = 0.0067636300809681416\n",
      "step 653, loss = 0.006681435741484165\n",
      "step 654, loss = 0.0038755633868277073\n",
      "step 655, loss = 0.005066652782261372\n",
      "step 656, loss = 0.004263206850737333\n",
      "step 657, loss = 0.0033277792390435934\n",
      "step 658, loss = 0.003471730975434184\n",
      "step 659, loss = 0.005860561039298773\n",
      "step 660, loss = 0.006327405571937561\n",
      "step 661, loss = 0.007674230728298426\n",
      "step 662, loss = 0.0062408363446593285\n",
      "step 663, loss = 0.00562068447470665\n",
      "step 664, loss = 0.00586882047355175\n",
      "step 665, loss = 0.005665985867381096\n",
      "step 666, loss = 0.005877260584384203\n",
      "step 667, loss = 0.0062035187147557735\n",
      "step 668, loss = 0.004220999777317047\n",
      "step 669, loss = 0.004775433335453272\n",
      "step 670, loss = 0.00404562009498477\n",
      "step 671, loss = 0.006642344407737255\n",
      "step 672, loss = 0.004378591664135456\n",
      "step 673, loss = 0.005078587681055069\n",
      "step 674, loss = 0.005486588459461927\n",
      "step 675, loss = 0.004473118577152491\n",
      "step 676, loss = 0.00564597686752677\n",
      "step 677, loss = 0.004579841624945402\n",
      "step 678, loss = 0.004833730403333902\n",
      "step 679, loss = 0.004588846117258072\n",
      "step 680, loss = 0.005687881726771593\n",
      "step 681, loss = 0.0058691371232271194\n",
      "step 682, loss = 0.005091297440230846\n",
      "step 683, loss = 0.003976214677095413\n",
      "step 684, loss = 0.006359898019582033\n",
      "step 685, loss = 0.0044748783111572266\n",
      "step 686, loss = 0.0032353487331420183\n",
      "step 687, loss = 0.004135752096772194\n",
      "step 688, loss = 0.004624979104846716\n",
      "step 689, loss = 0.004883288871496916\n",
      "step 690, loss = 0.00504237599670887\n",
      "step 691, loss = 0.004809672944247723\n",
      "step 692, loss = 0.004562810063362122\n",
      "step 693, loss = 0.004395697265863419\n",
      "step 694, loss = 0.0052334461361169815\n",
      "step 695, loss = 0.004827974829822779\n",
      "step 696, loss = 0.004236616659909487\n",
      "step 697, loss = 0.005287730135023594\n",
      "step 698, loss = 0.003801550017669797\n",
      "step 699, loss = 0.005206993781030178\n",
      "step 700, loss = 0.004138453863561153\n",
      "step 701, loss = 0.004563522525131702\n",
      "step 702, loss = 0.005260847974568605\n",
      "step 703, loss = 0.0028892578557133675\n",
      "step 704, loss = 0.00444980151951313\n",
      "step 705, loss = 0.004708725959062576\n",
      "step 706, loss = 0.0037870975211262703\n",
      "step 707, loss = 0.003904482815414667\n",
      "step 708, loss = 0.00415465235710144\n",
      "step 709, loss = 0.004211717750877142\n",
      "step 710, loss = 0.004171350970864296\n",
      "step 711, loss = 0.003159678541123867\n",
      "step 712, loss = 0.004821359645575285\n",
      "step 713, loss = 0.004985429812222719\n",
      "step 714, loss = 0.005004224833101034\n",
      "step 715, loss = 0.004275029059499502\n",
      "step 716, loss = 0.004791881423443556\n",
      "step 717, loss = 0.0038506954442709684\n",
      "step 718, loss = 0.004888656083494425\n",
      "step 719, loss = 0.003844811813905835\n",
      "step 720, loss = 0.003754623467102647\n",
      "step 721, loss = 0.004099949728697538\n",
      "step 722, loss = 0.0038616168312728405\n",
      "step 723, loss = 0.003423223504796624\n",
      "step 724, loss = 0.0038885739631950855\n",
      "step 725, loss = 0.004914816003292799\n",
      "step 726, loss = 0.004309068899601698\n",
      "step 727, loss = 0.0033460522536188364\n",
      "step 728, loss = 0.00479148468002677\n",
      "step 729, loss = 0.0037136003375053406\n",
      "step 730, loss = 0.0034763654693961143\n",
      "step 731, loss = 0.0038503834512084723\n",
      "step 732, loss = 0.003995102364569902\n",
      "step 733, loss = 0.003358399262651801\n",
      "step 734, loss = 0.003702217945829034\n",
      "step 735, loss = 0.0037354351952672005\n",
      "step 736, loss = 0.004310074262320995\n",
      "step 737, loss = 0.0036618737503886223\n",
      "step 738, loss = 0.0031698427628725767\n",
      "step 739, loss = 0.002988069085404277\n",
      "step 740, loss = 0.0034241413231939077\n",
      "step 741, loss = 0.004154089372605085\n",
      "step 742, loss = 0.003041669726371765\n",
      "step 743, loss = 0.0037081826012581587\n",
      "step 744, loss = 0.004205990582704544\n",
      "step 745, loss = 0.0030779431108385324\n",
      "step 746, loss = 0.0036717101465910673\n",
      "step 747, loss = 0.0035720840096473694\n",
      "step 748, loss = 0.003635709173977375\n",
      "step 749, loss = 0.002627669833600521\n",
      "step 750, loss = 0.004585163202136755\n",
      "step 751, loss = 0.00240035867318511\n",
      "step 752, loss = 0.002402336336672306\n",
      "step 753, loss = 0.0025222403928637505\n",
      "step 754, loss = 0.0033182338811457157\n",
      "step 755, loss = 0.002832521218806505\n",
      "step 756, loss = 0.0029149455949664116\n",
      "step 757, loss = 0.0022305736783891916\n",
      "step 758, loss = 0.0028518163599073887\n",
      "step 759, loss = 0.002808770164847374\n",
      "step 760, loss = 0.00254678912460804\n",
      "step 761, loss = 0.0027239620685577393\n",
      "step 762, loss = 0.0023940803948789835\n",
      "step 763, loss = 0.003501223400235176\n",
      "step 764, loss = 0.0024545348715037107\n",
      "step 765, loss = 0.0029107225127518177\n",
      "step 766, loss = 0.0019106781110167503\n",
      "step 767, loss = 0.002422499470412731\n",
      "step 768, loss = 0.002594610210508108\n",
      "step 769, loss = 0.0018909421050921082\n",
      "step 770, loss = 0.0019218422239646316\n",
      "step 771, loss = 0.002045479603111744\n",
      "step 772, loss = 0.0034822553861886263\n",
      "step 773, loss = 0.002675469499081373\n",
      "step 774, loss = 0.0031702027190476656\n",
      "step 775, loss = 0.002237198408693075\n",
      "step 776, loss = 0.003054501721635461\n",
      "step 777, loss = 0.002207944169640541\n",
      "step 778, loss = 0.0019455670844763517\n",
      "step 779, loss = 0.0016969683347269893\n",
      "step 780, loss = 0.0019233390921726823\n",
      "step 781, loss = 0.001526164123788476\n",
      "step 782, loss = 0.00221843458712101\n",
      "step 783, loss = 0.0021638243924826384\n",
      "step 784, loss = 0.002124025719240308\n",
      "step 785, loss = 0.0027267527766525745\n",
      "step 786, loss = 0.001842164434492588\n",
      "step 787, loss = 0.002277504187077284\n",
      "step 788, loss = 0.002281630877405405\n",
      "step 789, loss = 0.0024772093165665865\n",
      "step 790, loss = 0.0020953137427568436\n",
      "step 791, loss = 0.001831554458476603\n",
      "step 792, loss = 0.001882682554423809\n",
      "step 793, loss = 0.0015648838598281145\n",
      "step 794, loss = 0.0025073601864278316\n",
      "step 795, loss = 0.0025194999761879444\n",
      "step 796, loss = 0.002577281091362238\n",
      "step 797, loss = 0.0017997947288677096\n",
      "step 798, loss = 0.0019476244924589992\n",
      "step 799, loss = 0.0016390715027227998\n",
      "step 800, loss = 0.002030792646110058\n",
      "step 801, loss = 0.0018289329018443823\n",
      "step 802, loss = 0.0012381027918308973\n",
      "step 803, loss = 0.001848191604949534\n",
      "step 804, loss = 0.001118799322284758\n",
      "step 805, loss = 0.001063405885361135\n",
      "step 806, loss = 0.001828378182835877\n",
      "step 807, loss = 0.002085904823616147\n",
      "step 808, loss = 0.001530130160972476\n",
      "step 809, loss = 0.0013382610632106662\n",
      "step 810, loss = 0.0015526828356087208\n",
      "step 811, loss = 0.0013170274905860424\n",
      "step 812, loss = 0.0014986213063821197\n",
      "step 813, loss = 0.001200252678245306\n",
      "step 814, loss = 0.0009884029859676957\n",
      "step 815, loss = 0.0007417903980240226\n",
      "step 816, loss = 0.0011627663625404239\n",
      "step 817, loss = 0.0009454917744733393\n",
      "step 818, loss = 0.0013016066513955593\n",
      "step 819, loss = 0.0009761899709701538\n",
      "step 820, loss = 0.0009503135224804282\n",
      "step 821, loss = 0.0008203982724808156\n",
      "step 822, loss = 0.000988241401501\n",
      "step 823, loss = 0.0010898244800046086\n",
      "step 824, loss = 0.0010580471716821194\n",
      "step 825, loss = 0.0009448970668017864\n",
      "step 826, loss = 0.001015735324472189\n",
      "step 827, loss = 0.0008798621129244566\n",
      "step 828, loss = 0.000821331690531224\n",
      "step 829, loss = 0.0010498842457309365\n",
      "step 830, loss = 0.0008379254722967744\n",
      "step 831, loss = 0.0010657170787453651\n",
      "step 832, loss = 0.0012697866186499596\n",
      "step 833, loss = 0.0010940848151221871\n",
      "step 834, loss = 0.001008643419481814\n",
      "step 835, loss = 0.0011361376382410526\n",
      "step 836, loss = 0.001152066863141954\n",
      "step 837, loss = 0.0010599462548270822\n",
      "step 838, loss = 0.0008337463368661702\n",
      "step 839, loss = 0.0007052320870570838\n",
      "step 840, loss = 0.0009221759391948581\n",
      "step 841, loss = 0.0006590370321646333\n",
      "step 842, loss = 0.0010516769252717495\n",
      "step 843, loss = 0.0008889469318091869\n",
      "step 844, loss = 0.0009818820981308818\n",
      "step 845, loss = 0.0007579983794130385\n",
      "step 846, loss = 0.0009208854171447456\n",
      "step 847, loss = 0.0008275313884951174\n",
      "step 848, loss = 0.0009723384864628315\n",
      "step 849, loss = 0.0006051004165783525\n",
      "step 850, loss = 0.0005786666297353804\n",
      "step 851, loss = 0.0003576392191462219\n",
      "step 852, loss = 0.0007026332896202803\n",
      "step 853, loss = 0.0008676759898662567\n",
      "step 854, loss = 0.0008900173124857247\n",
      "step 855, loss = 0.0006772878114134073\n",
      "step 856, loss = 0.0005724543589167297\n",
      "step 857, loss = 0.0005758521147072315\n",
      "step 858, loss = 0.0008753413567319512\n",
      "step 859, loss = 0.000522125163115561\n",
      "step 860, loss = 0.0006157801835797727\n",
      "step 861, loss = 0.0006031561060808599\n",
      "step 862, loss = 0.0006412497023120522\n",
      "step 863, loss = 0.0007635169313289225\n",
      "step 864, loss = 0.0006971486145630479\n",
      "step 865, loss = 0.0008002063841558993\n",
      "step 866, loss = 0.0005834788898937404\n",
      "step 867, loss = 0.0005075287190265954\n",
      "step 868, loss = 0.000613890471868217\n",
      "step 869, loss = 0.0008152623195201159\n",
      "step 870, loss = 0.0006172009743750095\n",
      "step 871, loss = 0.00037333471118472517\n",
      "step 872, loss = 0.0005151944351382554\n",
      "step 873, loss = 0.0006266451673582196\n",
      "step 874, loss = 0.0007548415451310575\n",
      "step 875, loss = 0.0005608270294032991\n",
      "step 876, loss = 0.0006978086312301457\n",
      "step 877, loss = 0.0009655419271439314\n",
      "step 878, loss = 0.0005837213830091059\n",
      "step 879, loss = 0.000586366222705692\n",
      "step 880, loss = 0.0006462846649810672\n",
      "step 881, loss = 0.0006827777833677828\n",
      "step 882, loss = 0.0006291435565799475\n",
      "step 883, loss = 0.00032231223303824663\n",
      "step 884, loss = 0.0005672697443515062\n",
      "step 885, loss = 0.0003845416649710387\n",
      "step 886, loss = 0.0006208677077665925\n",
      "step 887, loss = 0.0004007082316093147\n",
      "step 888, loss = 0.0005280774203129113\n",
      "step 889, loss = 0.0006072039250284433\n",
      "step 890, loss = 0.00040105145308189094\n",
      "step 891, loss = 0.0005452230107039213\n",
      "step 892, loss = 0.000492717488668859\n",
      "step 893, loss = 0.0005328383995220065\n",
      "step 894, loss = 0.000470429309643805\n",
      "step 895, loss = 0.0006128448876552284\n",
      "step 896, loss = 0.0005723240319639444\n",
      "step 897, loss = 0.0004327369388192892\n",
      "step 898, loss = 0.0005213653203099966\n",
      "step 899, loss = 0.0005012882174924016\n",
      "step 900, loss = 0.00048389003495685756\n",
      "step 901, loss = 0.0005177975981496274\n",
      "step 902, loss = 0.0004389021487440914\n",
      "step 903, loss = 0.0003293419722467661\n",
      "step 904, loss = 0.0003812678623944521\n",
      "step 905, loss = 0.0004608813615050167\n",
      "step 906, loss = 0.0004619235114660114\n",
      "step 907, loss = 0.0003452157834544778\n",
      "step 908, loss = 0.0005833132308907807\n",
      "step 909, loss = 0.00034610225702635944\n",
      "step 910, loss = 0.0002376721822656691\n",
      "step 911, loss = 0.0004890459822490811\n",
      "step 912, loss = 0.00039172981632873416\n",
      "step 913, loss = 0.0004855218867305666\n",
      "step 914, loss = 0.0003732917830348015\n",
      "step 915, loss = 0.00044725913903675973\n",
      "step 916, loss = 0.0004393661511130631\n",
      "step 917, loss = 0.00038670323556289077\n",
      "step 918, loss = 0.00039920274866744876\n",
      "step 919, loss = 0.00041056927875615656\n",
      "step 920, loss = 0.00045318971388041973\n",
      "step 921, loss = 0.000357082491973415\n",
      "step 922, loss = 0.0003186074609402567\n",
      "step 923, loss = 0.0002364313550060615\n",
      "step 924, loss = 0.0004994935006834567\n",
      "step 925, loss = 0.0005010971217416227\n",
      "step 926, loss = 0.0004483386001084\n",
      "step 927, loss = 0.0005703424103558064\n",
      "step 928, loss = 0.00048416509525850415\n",
      "step 929, loss = 0.0004430318658705801\n",
      "step 930, loss = 0.0004299184074625373\n",
      "step 931, loss = 0.00037831999361515045\n",
      "step 932, loss = 0.0003565032675396651\n",
      "step 933, loss = 0.0003760203835554421\n",
      "step 934, loss = 0.000554095720872283\n",
      "step 935, loss = 0.00048478663666173816\n",
      "step 936, loss = 0.00035810741246677935\n",
      "step 937, loss = 0.00036439127870835364\n",
      "step 938, loss = 0.0003375718370079994\n",
      "step 939, loss = 0.00039740081410855055\n",
      "step 940, loss = 0.0003705515991896391\n",
      "step 941, loss = 0.0003577384341042489\n",
      "step 942, loss = 0.00032319777528755367\n",
      "step 943, loss = 0.0005181914893910289\n",
      "step 944, loss = 0.0003334660723339766\n",
      "step 945, loss = 0.0004042336659040302\n",
      "step 946, loss = 0.00050156912766397\n",
      "step 947, loss = 0.0004782532050739974\n",
      "step 948, loss = 0.00047897957847453654\n",
      "step 949, loss = 0.00044356632861308753\n",
      "step 950, loss = 0.00035890520666725934\n",
      "step 951, loss = 0.0003853993257507682\n",
      "step 952, loss = 0.0004236676904838532\n",
      "step 953, loss = 0.00035043995012529194\n",
      "step 954, loss = 0.0003159671032335609\n",
      "step 955, loss = 0.0003184240194968879\n",
      "step 956, loss = 0.000401838100515306\n",
      "step 957, loss = 0.0003166881506331265\n",
      "step 958, loss = 0.00026670016814023256\n",
      "step 959, loss = 0.0003445722395554185\n",
      "step 960, loss = 0.0002444624260533601\n",
      "step 961, loss = 0.0003454150282777846\n",
      "step 962, loss = 0.0004871387791354209\n",
      "step 963, loss = 0.0005200061714276671\n",
      "step 964, loss = 0.0004172254994045943\n",
      "step 965, loss = 0.00035506379208527505\n",
      "step 966, loss = 0.00047785305650904775\n",
      "step 967, loss = 0.0004534696345217526\n",
      "step 968, loss = 0.0003782814310397953\n",
      "step 969, loss = 0.0004116345080547035\n",
      "step 970, loss = 0.00038129676249809563\n",
      "step 971, loss = 0.0003153844445478171\n",
      "step 972, loss = 0.00048657081788405776\n",
      "step 973, loss = 0.0002992989611811936\n",
      "step 974, loss = 0.00032578673562966287\n",
      "step 975, loss = 0.0003655992040876299\n",
      "step 976, loss = 0.0003078860754612833\n",
      "step 977, loss = 0.00036001831176690757\n",
      "step 978, loss = 0.000329009402776137\n",
      "step 979, loss = 0.00041172667988575995\n",
      "step 980, loss = 0.00027063509332947433\n",
      "step 981, loss = 0.00041134736966341734\n",
      "step 982, loss = 0.00039705823292024434\n",
      "step 983, loss = 0.00038513605250045657\n",
      "step 984, loss = 0.00036573997931554914\n",
      "step 985, loss = 0.00029540329705923796\n",
      "step 986, loss = 0.00037522625643759966\n",
      "step 987, loss = 0.00043261461541987956\n",
      "step 988, loss = 0.00031907460652291775\n",
      "step 989, loss = 0.0002919650578405708\n",
      "step 990, loss = 0.0003694755141623318\n",
      "step 991, loss = 0.00042985277832485735\n",
      "step 992, loss = 0.00033214586437679827\n",
      "step 993, loss = 0.0002646259672474116\n",
      "step 994, loss = 0.00035971938632428646\n",
      "step 995, loss = 0.0003455519618000835\n",
      "step 996, loss = 0.00035168661270290613\n",
      "step 997, loss = 0.00038281138404272497\n",
      "step 998, loss = 0.000372773822164163\n",
      "step 999, loss = 0.00032131437910720706\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "n = 1000\n",
    "\n",
    "model = torchModel()\n",
    "optim = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "loss = torch.nn.MSELoss()\n",
    "begin = time.time()\n",
    "\n",
    "for i,(x,y) in enumerate(get_torch_data(n)):\n",
    "    optim.zero_grad()\n",
    "    pred_y = model(x)\n",
    "    batch_loss = loss(y, pred_y)\n",
    "    batch_loss.backward()\n",
    "    optim.step()\n",
    "    print(f\"step {i}, loss = {batch_loss.item()}\")\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7977049350738525 s has passed.\n"
     ]
    }
   ],
   "source": [
    "print(end - begin, \"s has passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 1, 0.01).reshape(-1, 1)\n",
    "x = x.astype('float32')\n",
    "y = x ** 2\n",
    "pred_y = model(torch.from_numpy(x)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, 'o', color='orange')\n",
    "plt.plot(x, pred_y, '*', color='violet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
