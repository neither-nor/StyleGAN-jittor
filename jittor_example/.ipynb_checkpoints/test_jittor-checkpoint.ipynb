{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:53.708455 68 compiler.py:839] Jittor(1.2.2.0) src: /home/wangxuheng/.pyenv/versions/3.7.0/lib/python3.7/site-packages/jittor\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:53.710655 68 compiler.py:840] cache_path: /home/wangxuheng/.cache/jittor/master/g++\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:53.740644 68 __init__.py:249] Found /usr/local/cuda/bin/nvcc(10.0.130) at /usr/local/cuda/bin/nvcc.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:53.819502 68 __init__.py:249] Found gdb(7.11.1) at /usr/bin/gdb.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:53.836058 68 __init__.py:249] Found addr2line(2.26.1) at /usr/bin/addr2line.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:53.875481 68 compiler.py:889] pybind_include: -I/home/wangxuheng/.pyenv/versions/3.7.0/include/python3.7m -I/home/wangxuheng/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pybind11/include\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i 1225 20:59:53.697475 76 __init__.py:249] Found g++(7.5.0) at /usr/bin/g++.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:53.903056 68 compiler.py:891] extension_suffix: .cpython-37m-x86_64-linux-gnu.so\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.085999 68 __init__.py:168] Total mem: 62.62GB, using 16 procs for compiling.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.233451 68 jit_compiler.cc:20] Load cc_path: /usr/bin/g++\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.416907 68 init.cc:51] Found cuda archs: [61,]\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.531575 68 __init__.py:249] Found mpicc(1.10.2) at /usr/bin/mpicc.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.581177 68 compiler.py:646] handle pyjt_include/home/wangxuheng/.pyenv/versions/3.7.0/lib/python3.7/site-packages/jittor/extern/mpi/inc/mpi_warper.h\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.622216 68 compile_extern.py:266] Downloading nccl...\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.710985 68 compile_extern.py:15] found /usr/local/cuda/include/cublas.h\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.714174 68 compile_extern.py:15] found /usr/local/cuda/lib64/libcublas.so\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file has been downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SYNC]\u001b[38;5;3m[w 1225 20:59:54.896843 68 compile_extern.py:140] CUDA found but cudnn is not loaded:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wangxuheng/.pyenv/versions/3.7.0/lib/python3.7/site-packages/jittor/compile_extern.py\", line 136, in setup_cuda_extern\n",
      "    setup_cuda_lib(lib_name)\n",
      "  File \"/home/wangxuheng/.pyenv/versions/3.7.0/lib/python3.7/site-packages/jittor/compile_extern.py\", line 156, in setup_cuda_lib\n",
      "    cuda_include_name = search_file([cuda_include, extra_include_path, \"/usr/include\"], lib_name+\".h\")\n",
      "  File \"/home/wangxuheng/.pyenv/versions/3.7.0/lib/python3.7/site-packages/jittor/compile_extern.py\", line 17, in search_file\n",
      "    LOG.f(f\"file {name} not found in {dirs}\")\n",
      "  File \"/home/wangxuheng/.pyenv/versions/3.7.0/lib/python3.7/site-packages/jittor_utils/__init__.py\", line 61, in f\n",
      "    def f(self, *msg): self._log('f', 0, *msg)\n",
      "  File \"/home/wangxuheng/.pyenv/versions/3.7.0/lib/python3.7/site-packages/jittor_utils/__init__.py\", line 46, in _log\n",
      "    cc.log(fileline, level, verbose, msg)\n",
      "RuntimeError: \u001b[38;5;1m[f 1225 20:59:54.896553 68 compile_extern.py:17] file cudnn.h not found in ['/usr/local/cuda/include', '/usr/local/cuda/targets/x86_64-linux/include', '/usr/include']\u001b[m\n",
      "\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.898075 68 compile_extern.py:15] found /usr/local/cuda/include/curand.h\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 1225 20:59:54.899124 68 compile_extern.py:15] found /usr/local/cuda/lib64/libcurand.so\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(jt.Module):\n",
    "    def __init__(self):\n",
    "        self.layer1 = jt.nn.Linear(1, 10)\n",
    "        self.relu = jt.nn.Relu() \n",
    "        self.layer2 = jt.nn.Linear(10, 1)\n",
    "    def execute (self,x) :\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "def get_data(n): # generate random data for training test.\n",
    "    for i in range(n):\n",
    "        x = np.random.rand(batch_size, 1)\n",
    "        y = x*x\n",
    "        yield jt.float32(x), jt.float32(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 0.13962189853191376\n",
      "step 1, loss = 0.1161079853773117\n",
      "step 2, loss = 0.10577141493558884\n",
      "step 3, loss = 0.12018366903066635\n",
      "step 4, loss = 0.10175701975822449\n",
      "step 5, loss = 0.06581489741802216\n",
      "step 6, loss = 0.07175969332456589\n",
      "step 7, loss = 0.0639205202460289\n",
      "step 8, loss = 0.04699935391545296\n",
      "step 9, loss = 0.04826698824763298\n",
      "step 10, loss = 0.045868076384067535\n",
      "step 11, loss = 0.04051041975617409\n",
      "step 12, loss = 0.037750620394945145\n",
      "step 13, loss = 0.041046321392059326\n",
      "step 14, loss = 0.03911610692739487\n",
      "step 15, loss = 0.025694318115711212\n",
      "step 16, loss = 0.02564825490117073\n",
      "step 17, loss = 0.026503419503569603\n",
      "step 18, loss = 0.024908768013119698\n",
      "step 19, loss = 0.01877368800342083\n",
      "step 20, loss = 0.013938759453594685\n",
      "step 21, loss = 0.014485787600278854\n",
      "step 22, loss = 0.010685865767300129\n",
      "step 23, loss = 0.007952848449349403\n",
      "step 24, loss = 0.005805192515254021\n",
      "step 25, loss = 0.005911797750741243\n",
      "step 26, loss = 0.003982853144407272\n",
      "step 27, loss = 0.002510815393179655\n",
      "step 28, loss = 0.001601015916094184\n",
      "step 29, loss = 0.004042887128889561\n",
      "step 30, loss = 0.002665721345692873\n",
      "step 31, loss = 0.00181859964504838\n",
      "step 32, loss = 0.0026752997655421495\n",
      "step 33, loss = 0.002689645392820239\n",
      "step 34, loss = 0.002813109662383795\n",
      "step 35, loss = 0.0021139539312571287\n",
      "step 36, loss = 0.0030967805068939924\n",
      "step 37, loss = 0.0030965302139520645\n",
      "step 38, loss = 0.0028166251722723246\n",
      "step 39, loss = 0.003375405678525567\n",
      "step 40, loss = 0.004103905521333218\n",
      "step 41, loss = 0.0032960681710392237\n",
      "step 42, loss = 0.0031519997864961624\n",
      "step 43, loss = 0.002676857402548194\n",
      "step 44, loss = 0.0035183315631002188\n",
      "step 45, loss = 0.003323807381093502\n",
      "step 46, loss = 0.002594954799860716\n",
      "step 47, loss = 0.0022531417198479176\n",
      "step 48, loss = 0.0024147836957126856\n",
      "step 49, loss = 0.002054705750197172\n",
      "step 50, loss = 0.001603884738869965\n",
      "step 51, loss = 0.00184036570135504\n",
      "step 52, loss = 0.0016839710297062993\n",
      "step 53, loss = 0.0018504156032577157\n",
      "step 54, loss = 0.0013875998556613922\n",
      "step 55, loss = 0.0015258305938914418\n",
      "step 56, loss = 0.0012313261395320296\n",
      "step 57, loss = 0.0011590247740969062\n",
      "step 58, loss = 0.00134561478625983\n",
      "step 59, loss = 0.0014783964725211263\n",
      "step 60, loss = 0.0012270850129425526\n",
      "step 61, loss = 0.0019116973271593451\n",
      "step 62, loss = 0.0016062046634033322\n",
      "step 63, loss = 0.001329972641542554\n",
      "step 64, loss = 0.0009408246842212975\n",
      "step 65, loss = 0.0009549694950692356\n",
      "step 66, loss = 0.001425686408765614\n",
      "step 67, loss = 0.001522629987448454\n",
      "step 68, loss = 0.0012310250895097852\n",
      "step 69, loss = 0.0016671322518959641\n",
      "step 70, loss = 0.0013475175946950912\n",
      "step 71, loss = 0.0009832022478803992\n",
      "step 72, loss = 0.001270871958695352\n",
      "step 73, loss = 0.0012770459288731217\n",
      "step 74, loss = 0.0010595264611765742\n",
      "step 75, loss = 0.001330668106675148\n",
      "step 76, loss = 0.0008984189480543137\n",
      "step 77, loss = 0.0008808895945549011\n",
      "step 78, loss = 0.0008926279260776937\n",
      "step 79, loss = 0.0011770195560529828\n",
      "step 80, loss = 0.0011324294609948993\n",
      "step 81, loss = 0.0008648267248645425\n",
      "step 82, loss = 0.0011526800226420164\n",
      "step 83, loss = 0.0007698678527958691\n",
      "step 84, loss = 0.0011806972324848175\n",
      "step 85, loss = 0.0008520715055055916\n",
      "step 86, loss = 0.0006276046624407172\n",
      "step 87, loss = 0.0008440102101303637\n",
      "step 88, loss = 0.0010158285731449723\n",
      "step 89, loss = 0.0009295782074332237\n",
      "step 90, loss = 0.0011106767924502492\n",
      "step 91, loss = 0.0006780477124266326\n",
      "step 92, loss = 0.000576591060962528\n",
      "step 93, loss = 0.0009786279406398535\n",
      "step 94, loss = 0.0008175731054507196\n",
      "step 95, loss = 0.0005716604646295309\n",
      "step 96, loss = 0.0007189699681475759\n",
      "step 97, loss = 0.0007919597555883229\n",
      "step 98, loss = 0.0007109575672075152\n",
      "step 99, loss = 0.0006614667363464832\n",
      "step 100, loss = 0.0004534670151770115\n",
      "step 101, loss = 0.0006766290753148496\n",
      "step 102, loss = 0.0007393695414066315\n",
      "step 103, loss = 0.0008409297442995012\n",
      "step 104, loss = 0.0007332242093980312\n",
      "step 105, loss = 0.000554957368876785\n",
      "step 106, loss = 0.0004571845638565719\n",
      "step 107, loss = 0.0005792992888018489\n",
      "step 108, loss = 0.00044935132609680295\n",
      "step 109, loss = 0.0005005312850698829\n",
      "step 110, loss = 0.00047652219654992223\n",
      "step 111, loss = 0.0007196090882644057\n",
      "step 112, loss = 0.00029060893575660884\n",
      "step 113, loss = 0.0004450005362741649\n",
      "step 114, loss = 0.0006502319010905921\n",
      "step 115, loss = 0.0005346990074031055\n",
      "step 116, loss = 0.0005670003010891378\n",
      "step 117, loss = 0.0005474694189615548\n",
      "step 118, loss = 0.0004844764480367303\n",
      "step 119, loss = 0.0005116930697113276\n",
      "step 120, loss = 0.00037291963235475123\n",
      "step 121, loss = 0.0005468630697578192\n",
      "step 122, loss = 0.0004465921374503523\n",
      "step 123, loss = 0.00039824211853556335\n",
      "step 124, loss = 0.0006519887247122824\n",
      "step 125, loss = 0.00027623289497569203\n",
      "step 126, loss = 0.000296360463835299\n",
      "step 127, loss = 0.000519054417964071\n",
      "step 128, loss = 0.00036258320324122906\n",
      "step 129, loss = 0.0003254215407650918\n",
      "step 130, loss = 0.0002596177801024169\n",
      "step 131, loss = 0.000356549076968804\n",
      "step 132, loss = 0.0004205431614536792\n",
      "step 133, loss = 0.0004212459607515484\n",
      "step 134, loss = 0.00042649192619137466\n",
      "step 135, loss = 0.0004818064335267991\n",
      "step 136, loss = 0.00053571438184008\n",
      "step 137, loss = 0.0002784069802146405\n",
      "step 138, loss = 0.0004939800128340721\n",
      "step 139, loss = 0.0002633433323353529\n",
      "step 140, loss = 0.0002475701621733606\n",
      "step 141, loss = 0.000494904350489378\n",
      "step 142, loss = 0.0003508361987769604\n",
      "step 143, loss = 0.0003394317755009979\n",
      "step 144, loss = 0.00022650383471045643\n",
      "step 145, loss = 0.00023833895102143288\n",
      "step 146, loss = 0.00037404621252790093\n",
      "step 147, loss = 0.0002786062250379473\n",
      "step 148, loss = 0.0003271001041866839\n",
      "step 149, loss = 0.00020190593204461038\n",
      "step 150, loss = 0.0003245942643843591\n",
      "step 151, loss = 0.0002476184454280883\n",
      "step 152, loss = 0.00028015952557325363\n",
      "step 153, loss = 0.0002648297231644392\n",
      "step 154, loss = 0.0003115230065304786\n",
      "step 155, loss = 0.00025932147400453687\n",
      "step 156, loss = 0.00030571132083423436\n",
      "step 157, loss = 0.00030763846007175744\n",
      "step 158, loss = 0.0003539891040418297\n",
      "step 159, loss = 0.0003010928921867162\n",
      "step 160, loss = 0.0002349247079109773\n",
      "step 161, loss = 0.00039046569145284593\n",
      "step 162, loss = 0.00024229742120951414\n",
      "step 163, loss = 0.0002752357395365834\n",
      "step 164, loss = 0.0004421673947945237\n",
      "step 165, loss = 0.0002825004339683801\n",
      "step 166, loss = 0.0001871794374892488\n",
      "step 167, loss = 0.00018968457879964262\n",
      "step 168, loss = 0.00011841228842968121\n",
      "step 169, loss = 0.00016625136777292937\n",
      "step 170, loss = 0.0002614791737869382\n",
      "step 171, loss = 0.0002819920773617923\n",
      "step 172, loss = 0.00021572767582256347\n",
      "step 173, loss = 0.00027151789981871843\n",
      "step 174, loss = 0.00019507673277985305\n",
      "step 175, loss = 0.0001318939757766202\n",
      "step 176, loss = 0.00018951890524476767\n",
      "step 177, loss = 0.0001690856006462127\n",
      "step 178, loss = 0.0001295497640967369\n",
      "step 179, loss = 0.00028852836112491786\n",
      "step 180, loss = 0.0002700649492908269\n",
      "step 181, loss = 0.00013850767572876066\n",
      "step 182, loss = 0.0002687357773538679\n",
      "step 183, loss = 0.00018073049432132393\n",
      "step 184, loss = 0.00018402596469968557\n",
      "step 185, loss = 0.0002755098685156554\n",
      "step 186, loss = 0.00022934909793548286\n",
      "step 187, loss = 0.00019690550107043236\n",
      "step 188, loss = 0.00014881594688631594\n",
      "step 189, loss = 0.00023605716705787927\n",
      "step 190, loss = 0.00015929491200949997\n",
      "step 191, loss = 0.0002028470189543441\n",
      "step 192, loss = 0.0001851480483310297\n",
      "step 193, loss = 0.0001267785410163924\n",
      "step 194, loss = 0.00012170360423624516\n",
      "step 195, loss = 0.00014018832007423043\n",
      "step 196, loss = 0.00017723874771036208\n",
      "step 197, loss = 0.00019196319044567645\n",
      "step 198, loss = 0.0001742331514833495\n",
      "step 199, loss = 0.00016411994874943048\n",
      "step 200, loss = 0.00016160089580807835\n",
      "step 201, loss = 0.00011705382348736748\n",
      "step 202, loss = 0.00028622426907531917\n",
      "step 203, loss = 0.00026658616843633354\n",
      "step 204, loss = 0.00011910298781003803\n",
      "step 205, loss = 0.00011277783050900325\n",
      "step 206, loss = 0.00012278066424187273\n",
      "step 207, loss = 0.00021736744383815676\n",
      "step 208, loss = 0.00014137892867438495\n",
      "step 209, loss = 7.543709216406569e-05\n",
      "step 210, loss = 0.00012915951083414257\n",
      "step 211, loss = 0.0001321798044955358\n",
      "step 212, loss = 0.00016458463505841792\n",
      "step 213, loss = 0.00011856608034577221\n",
      "step 214, loss = 0.0002569066418800503\n",
      "step 215, loss = 0.0001109262157115154\n",
      "step 216, loss = 0.00014160273713059723\n",
      "step 217, loss = 0.0001700202701613307\n",
      "step 218, loss = 0.00014487946464214474\n",
      "step 219, loss = 9.612054418539628e-05\n",
      "step 220, loss = 0.00018354169151280075\n",
      "step 221, loss = 0.00012348711607046425\n",
      "step 222, loss = 0.00014080162509344518\n",
      "step 223, loss = 0.00012549079838208854\n",
      "step 224, loss = 0.00011948621249757707\n",
      "step 225, loss = 0.00020350234990473837\n",
      "step 226, loss = 0.00018588163948152214\n",
      "step 227, loss = 0.0001847116946009919\n",
      "step 228, loss = 0.00017509216559119523\n",
      "step 229, loss = 0.00013420799223240465\n",
      "step 230, loss = 0.00015848489420022815\n",
      "step 231, loss = 0.0001519289071438834\n",
      "step 232, loss = 0.00013602730177808553\n",
      "step 233, loss = 0.00015072993119247258\n",
      "step 234, loss = 0.00018614351574797183\n",
      "step 235, loss = 0.0001335511915385723\n",
      "step 236, loss = 0.0001349232334177941\n",
      "step 237, loss = 8.050943142734468e-05\n",
      "step 238, loss = 7.864565850468352e-05\n",
      "step 239, loss = 0.00014545524027198553\n",
      "step 240, loss = 0.00010320790170226246\n",
      "step 241, loss = 0.0001257428084500134\n",
      "step 242, loss = 0.00014450876915361732\n",
      "step 243, loss = 0.00013121121446602046\n",
      "step 244, loss = 9.435768879484385e-05\n",
      "step 245, loss = 0.00012337607040535659\n",
      "step 246, loss = 0.0001715583202894777\n",
      "step 247, loss = 0.00010555123299127445\n",
      "step 248, loss = 0.00011988470942014828\n",
      "step 249, loss = 0.00012306770076975226\n",
      "step 250, loss = 0.00011925026046810672\n",
      "step 251, loss = 0.00011322466161800548\n",
      "step 252, loss = 0.00011645976337604225\n",
      "step 253, loss = 0.00010173513146582991\n",
      "step 254, loss = 7.231436029542238e-05\n",
      "step 255, loss = 7.773463585181162e-05\n",
      "step 256, loss = 0.00018544857448432595\n",
      "step 257, loss = 0.00011507673480082303\n",
      "step 258, loss = 0.0001362163166049868\n",
      "step 259, loss = 0.00010102026135427877\n",
      "step 260, loss = 0.000105590712337289\n",
      "step 261, loss = 0.00010417833982501179\n",
      "step 262, loss = 0.00011623117461567745\n",
      "step 263, loss = 0.0001588442682987079\n",
      "step 264, loss = 0.000125535560073331\n",
      "step 265, loss = 9.654075256548822e-05\n",
      "step 266, loss = 0.00013560426305048168\n",
      "step 267, loss = 0.00012201534264022484\n",
      "step 268, loss = 9.202836372423917e-05\n",
      "step 269, loss = 8.772944420343265e-05\n",
      "step 270, loss = 7.268664194270968e-05\n",
      "step 271, loss = 6.829531048424542e-05\n",
      "step 272, loss = 8.674783748574555e-05\n",
      "step 273, loss = 5.769245763076469e-05\n",
      "step 274, loss = 0.0001580427779117599\n",
      "step 275, loss = 0.0001299922150792554\n",
      "step 276, loss = 6.374372605932876e-05\n",
      "step 277, loss = 0.00011799803178291768\n",
      "step 278, loss = 0.00013683782890439034\n",
      "step 279, loss = 9.344780119135976e-05\n",
      "step 280, loss = 0.00010876069427467883\n",
      "step 281, loss = 0.00012024956231471151\n",
      "step 282, loss = 9.735208004713058e-05\n",
      "step 283, loss = 0.00010914878657786176\n",
      "step 284, loss = 9.81451739789918e-05\n",
      "step 285, loss = 6.647563714068383e-05\n",
      "step 286, loss = 4.567363066598773e-05\n",
      "step 287, loss = 6.123155617387965e-05\n",
      "step 288, loss = 0.00011923297279281542\n",
      "step 289, loss = 0.00014325899246614426\n",
      "step 290, loss = 0.0001043290612869896\n",
      "step 291, loss = 0.00011899356468347833\n",
      "step 292, loss = 8.77500933711417e-05\n",
      "step 293, loss = 8.567609620513394e-05\n",
      "step 294, loss = 7.732212543487549e-05\n",
      "step 295, loss = 9.926127677317709e-05\n",
      "step 296, loss = 6.418905832106248e-05\n",
      "step 297, loss = 0.00011045557766919956\n",
      "step 298, loss = 0.00012174777657492086\n",
      "step 299, loss = 8.704532956471667e-05\n",
      "step 300, loss = 0.0001746070629451424\n",
      "step 301, loss = 8.862136019160971e-05\n",
      "step 302, loss = 9.806638263398781e-05\n",
      "step 303, loss = 7.905145321274176e-05\n",
      "step 304, loss = 0.00010630764882080257\n",
      "step 305, loss = 0.0001038857371895574\n",
      "step 306, loss = 9.527735528536141e-05\n",
      "step 307, loss = 8.618913852842525e-05\n",
      "step 308, loss = 0.0001345969649264589\n",
      "step 309, loss = 0.00014590757200494409\n",
      "step 310, loss = 8.632177923573181e-05\n",
      "step 311, loss = 5.3708172345068306e-05\n",
      "step 312, loss = 0.000135169000714086\n",
      "step 313, loss = 0.0001289828069275245\n",
      "step 314, loss = 8.372085721930489e-05\n",
      "step 315, loss = 8.800234354566783e-05\n",
      "step 316, loss = 8.949100447352976e-05\n",
      "step 317, loss = 0.00010698220285121351\n",
      "step 318, loss = 7.725657633272931e-05\n",
      "step 319, loss = 8.709665416972712e-05\n",
      "step 320, loss = 9.69939210335724e-05\n",
      "step 321, loss = 0.00011014648771379143\n",
      "step 322, loss = 5.24413408129476e-05\n",
      "step 323, loss = 7.923238445073366e-05\n",
      "step 324, loss = 7.547689892817289e-05\n",
      "step 325, loss = 0.00010930810094578192\n",
      "step 326, loss = 7.497843762394041e-05\n",
      "step 327, loss = 9.918711293721572e-05\n",
      "step 328, loss = 0.00017186140757985413\n",
      "step 329, loss = 5.3166906582191586e-05\n",
      "step 330, loss = 5.650865568895824e-05\n",
      "step 331, loss = 9.847029286902398e-05\n",
      "step 332, loss = 9.71976260188967e-05\n",
      "step 333, loss = 9.212177974404767e-05\n",
      "step 334, loss = 8.588441414758563e-05\n",
      "step 335, loss = 7.955801265779883e-05\n",
      "step 336, loss = 0.00011156172695336863\n",
      "step 337, loss = 8.429545414401218e-05\n",
      "step 338, loss = 6.422504520742223e-05\n",
      "step 339, loss = 9.02705141925253e-05\n",
      "step 340, loss = 9.987819066736847e-05\n",
      "step 341, loss = 0.00013908404798712581\n",
      "step 342, loss = 6.785260484321043e-05\n",
      "step 343, loss = 9.258303907699883e-05\n",
      "step 344, loss = 7.9308170825243e-05\n",
      "step 345, loss = 0.00010908344847848639\n",
      "step 346, loss = 7.20464377081953e-05\n",
      "step 347, loss = 7.210083276731893e-05\n",
      "step 348, loss = 0.00010806673526531085\n",
      "step 349, loss = 0.0001056774563039653\n",
      "step 350, loss = 0.00011966241436311975\n",
      "step 351, loss = 8.945685112848878e-05\n",
      "step 352, loss = 7.89751939009875e-05\n",
      "step 353, loss = 7.864565850468352e-05\n",
      "step 354, loss = 9.305718413088471e-05\n",
      "step 355, loss = 0.00010086603288073093\n",
      "step 356, loss = 8.032800542423502e-05\n",
      "step 357, loss = 0.00010167147411266342\n",
      "step 358, loss = 8.759176125749946e-05\n",
      "step 359, loss = 8.548160985810682e-05\n",
      "step 360, loss = 5.7285025832243264e-05\n",
      "step 361, loss = 0.00010413779818918556\n",
      "step 362, loss = 9.880085417535156e-05\n",
      "step 363, loss = 7.301371806534007e-05\n",
      "step 364, loss = 9.92217828752473e-05\n",
      "step 365, loss = 9.970844985218719e-05\n",
      "step 366, loss = 8.903120760805905e-05\n",
      "step 367, loss = 9.92116765701212e-05\n",
      "step 368, loss = 7.528185233240947e-05\n",
      "step 369, loss = 9.930557280313224e-05\n",
      "step 370, loss = 0.0001039126145769842\n",
      "step 371, loss = 7.26896250853315e-05\n",
      "step 372, loss = 8.30504359328188e-05\n",
      "step 373, loss = 8.713002171134576e-05\n",
      "step 374, loss = 9.394226071890444e-05\n",
      "step 375, loss = 7.311950321309268e-05\n",
      "step 376, loss = 8.827776036923751e-05\n",
      "step 377, loss = 0.00011500624532345682\n",
      "step 378, loss = 6.20912978774868e-05\n",
      "step 379, loss = 8.74142860993743e-05\n",
      "step 380, loss = 9.732237958814949e-05\n",
      "step 381, loss = 8.006262214621529e-05\n",
      "step 382, loss = 9.452706581214443e-05\n",
      "step 383, loss = 8.60999061842449e-05\n",
      "step 384, loss = 0.00011423937394283712\n",
      "step 385, loss = 0.0001110834491555579\n",
      "step 386, loss = 8.830179285723716e-05\n",
      "step 387, loss = 8.236640860559419e-05\n",
      "step 388, loss = 7.123949762899429e-05\n",
      "step 389, loss = 9.401143324794248e-05\n",
      "step 390, loss = 8.831879677018151e-05\n",
      "step 391, loss = 9.346383740194142e-05\n",
      "step 392, loss = 6.83497783029452e-05\n",
      "step 393, loss = 6.541108450619504e-05\n",
      "step 394, loss = 7.309060310944915e-05\n",
      "step 395, loss = 0.00012736076314467937\n",
      "step 396, loss = 7.660467963432893e-05\n",
      "step 397, loss = 6.997317541390657e-05\n",
      "step 398, loss = 9.165012306766585e-05\n",
      "step 399, loss = 7.597620424348861e-05\n",
      "step 400, loss = 8.772627916187048e-05\n",
      "step 401, loss = 7.241178536787629e-05\n",
      "step 402, loss = 8.001113747013733e-05\n",
      "step 403, loss = 3.85410530725494e-05\n",
      "step 404, loss = 8.924205758376047e-05\n",
      "step 405, loss = 6.625588866882026e-05\n",
      "step 406, loss = 7.824409840395674e-05\n",
      "step 407, loss = 8.889143646229059e-05\n",
      "step 408, loss = 7.870181434554979e-05\n",
      "step 409, loss = 8.766281825955957e-05\n",
      "step 410, loss = 9.239649079972878e-05\n",
      "step 411, loss = 7.306434417841956e-05\n",
      "step 412, loss = 7.564805127913132e-05\n",
      "step 413, loss = 6.692226452287287e-05\n",
      "step 414, loss = 7.655301305931062e-05\n",
      "step 415, loss = 9.031056833919138e-05\n",
      "step 416, loss = 6.679612852167338e-05\n",
      "step 417, loss = 7.296371040865779e-05\n",
      "step 418, loss = 6.447784835472703e-05\n",
      "step 419, loss = 8.396133489441127e-05\n",
      "step 420, loss = 9.087868966162205e-05\n",
      "step 421, loss = 6.916734855622053e-05\n",
      "step 422, loss = 8.10120691312477e-05\n",
      "step 423, loss = 0.00010322827438358217\n",
      "step 424, loss = 0.00011604350584093481\n",
      "step 425, loss = 7.83584764576517e-05\n",
      "step 426, loss = 0.00010017068416345865\n",
      "step 427, loss = 9.334523201687261e-05\n",
      "step 428, loss = 9.652528387960047e-05\n",
      "step 429, loss = 8.135863754432648e-05\n",
      "step 430, loss = 0.00010355444101151079\n",
      "step 431, loss = 7.976367487572134e-05\n",
      "step 432, loss = 8.765367238083854e-05\n",
      "step 433, loss = 7.237884710775688e-05\n",
      "step 434, loss = 7.883638318162411e-05\n",
      "step 435, loss = 0.00010725199535954744\n",
      "step 436, loss = 8.554062515031546e-05\n",
      "step 437, loss = 0.00011051558976760134\n",
      "step 438, loss = 8.838688518153504e-05\n",
      "step 439, loss = 7.859170727897435e-05\n",
      "step 440, loss = 8.319262997247279e-05\n",
      "step 441, loss = 8.63703025970608e-05\n",
      "step 442, loss = 7.691747305216268e-05\n",
      "step 443, loss = 6.5785956394393e-05\n",
      "step 444, loss = 7.261796417878941e-05\n",
      "step 445, loss = 5.94077464484144e-05\n",
      "step 446, loss = 0.00010539218783378601\n",
      "step 447, loss = 5.895554932067171e-05\n",
      "step 448, loss = 6.274421320995316e-05\n",
      "step 449, loss = 8.228955994127318e-05\n",
      "step 450, loss = 6.852163642179221e-05\n",
      "step 451, loss = 6.708378350595012e-05\n",
      "step 452, loss = 0.00010337118874303997\n",
      "step 453, loss = 0.00011464203271316364\n",
      "step 454, loss = 9.723713446874171e-05\n",
      "step 455, loss = 8.639130101073533e-05\n",
      "step 456, loss = 9.119563037529588e-05\n",
      "step 457, loss = 0.00010709578782552853\n",
      "step 458, loss = 8.082202111836523e-05\n",
      "step 459, loss = 0.0001019006158458069\n",
      "step 460, loss = 9.641981159802526e-05\n",
      "step 461, loss = 7.793356053298339e-05\n",
      "step 462, loss = 0.0001016613095998764\n",
      "step 463, loss = 8.177043491741642e-05\n",
      "step 464, loss = 8.93213800736703e-05\n",
      "step 465, loss = 0.00010786486382130533\n",
      "step 466, loss = 7.057625043671578e-05\n",
      "step 467, loss = 5.564720049733296e-05\n",
      "step 468, loss = 8.11738646007143e-05\n",
      "step 469, loss = 7.696296961512417e-05\n",
      "step 470, loss = 9.371015767101198e-05\n",
      "step 471, loss = 6.151004345156252e-05\n",
      "step 472, loss = 8.342258661286905e-05\n",
      "step 473, loss = 8.956107922131196e-05\n",
      "step 474, loss = 7.707878830842674e-05\n",
      "step 475, loss = 8.382599480682984e-05\n",
      "step 476, loss = 5.839142613695003e-05\n",
      "step 477, loss = 6.410838250303641e-05\n",
      "step 478, loss = 9.733460319694132e-05\n",
      "step 479, loss = 7.886916137067601e-05\n",
      "step 480, loss = 6.029858559486456e-05\n",
      "step 481, loss = 8.165456529241055e-05\n",
      "step 482, loss = 7.942893716972321e-05\n",
      "step 483, loss = 7.157704385463148e-05\n",
      "step 484, loss = 8.422107202932239e-05\n",
      "step 485, loss = 7.085406832629815e-05\n",
      "step 486, loss = 7.88836186984554e-05\n",
      "step 487, loss = 6.271226448006928e-05\n",
      "step 488, loss = 8.350300049642101e-05\n",
      "step 489, loss = 9.362283162772655e-05\n",
      "step 490, loss = 6.403325824066997e-05\n",
      "step 491, loss = 0.0001034096276271157\n",
      "step 492, loss = 6.607372779399157e-05\n",
      "step 493, loss = 6.63942628307268e-05\n",
      "step 494, loss = 9.985471842810512e-05\n",
      "step 495, loss = 6.592927093151957e-05\n",
      "step 496, loss = 8.993309165816754e-05\n",
      "step 497, loss = 7.401305629173294e-05\n",
      "step 498, loss = 0.00011815625475719571\n",
      "step 499, loss = 6.799476250307634e-05\n",
      "step 500, loss = 5.887853694730438e-05\n",
      "step 501, loss = 4.651941344491206e-05\n",
      "step 502, loss = 0.00013208163727540523\n",
      "step 503, loss = 8.403976971749216e-05\n",
      "step 504, loss = 8.113145304378122e-05\n",
      "step 505, loss = 7.340198499150574e-05\n",
      "step 506, loss = 8.735152368899435e-05\n",
      "step 507, loss = 6.555084110004827e-05\n",
      "step 508, loss = 8.105642336886376e-05\n",
      "step 509, loss = 6.713636685162783e-05\n",
      "step 510, loss = 7.029474363662302e-05\n",
      "step 511, loss = 7.259896665345877e-05\n",
      "step 512, loss = 5.538133336813189e-05\n",
      "step 513, loss = 9.369150939164683e-05\n",
      "step 514, loss = 9.344224963570014e-05\n",
      "step 515, loss = 5.372854138840921e-05\n",
      "step 516, loss = 7.395338616333902e-05\n",
      "step 517, loss = 7.185852155089378e-05\n",
      "step 518, loss = 7.221663690870628e-05\n",
      "step 519, loss = 9.987653174903244e-05\n",
      "step 520, loss = 6.736686191288754e-05\n",
      "step 521, loss = 5.505350054590963e-05\n",
      "step 522, loss = 9.535293065709993e-05\n",
      "step 523, loss = 9.695591870695353e-05\n",
      "step 524, loss = 9.195331949740648e-05\n",
      "step 525, loss = 7.788000220898539e-05\n",
      "step 526, loss = 7.384731725323945e-05\n",
      "step 527, loss = 7.362845644820482e-05\n",
      "step 528, loss = 0.00010156653297599405\n",
      "step 529, loss = 7.58691894588992e-05\n",
      "step 530, loss = 4.5999597205081955e-05\n",
      "step 531, loss = 8.866749703884125e-05\n",
      "step 532, loss = 6.674230098724365e-05\n",
      "step 533, loss = 5.664308264385909e-05\n",
      "step 534, loss = 6.792673229938373e-05\n",
      "step 535, loss = 7.675577944610268e-05\n",
      "step 536, loss = 8.207117934944108e-05\n",
      "step 537, loss = 0.0001080416259355843\n",
      "step 538, loss = 5.8842131693381816e-05\n",
      "step 539, loss = 6.421941361622885e-05\n",
      "step 540, loss = 7.904469384811819e-05\n",
      "step 541, loss = 6.321524415398017e-05\n",
      "step 542, loss = 9.382508142152801e-05\n",
      "step 543, loss = 8.436465577688068e-05\n",
      "step 544, loss = 8.702014019945636e-05\n",
      "step 545, loss = 6.354609649861231e-05\n",
      "step 546, loss = 7.402974006254226e-05\n",
      "step 547, loss = 6.241024675546214e-05\n",
      "step 548, loss = 6.533939449582249e-05\n",
      "step 549, loss = 8.205536869354546e-05\n",
      "step 550, loss = 9.193836740450934e-05\n",
      "step 551, loss = 7.961435039760545e-05\n",
      "step 552, loss = 7.700050628045574e-05\n",
      "step 553, loss = 6.440438301069662e-05\n",
      "step 554, loss = 9.088970546144992e-05\n",
      "step 555, loss = 7.021598139544949e-05\n",
      "step 556, loss = 8.535431697964668e-05\n",
      "step 557, loss = 6.333798955893144e-05\n",
      "step 558, loss = 8.577305561630055e-05\n",
      "step 559, loss = 5.195070480112918e-05\n",
      "step 560, loss = 8.088150934781879e-05\n",
      "step 561, loss = 6.972819392103702e-05\n",
      "step 562, loss = 6.433945964090526e-05\n",
      "step 563, loss = 8.647834329167381e-05\n",
      "step 564, loss = 7.01975732226856e-05\n",
      "step 565, loss = 7.009735418250784e-05\n",
      "step 566, loss = 7.511037983931601e-05\n",
      "step 567, loss = 8.05363233666867e-05\n",
      "step 568, loss = 0.0001033800799632445\n",
      "step 569, loss = 6.549582758452743e-05\n",
      "step 570, loss = 6.854849925730377e-05\n",
      "step 571, loss = 7.02659526723437e-05\n",
      "step 572, loss = 7.546347478637472e-05\n",
      "step 573, loss = 7.259870471898466e-05\n",
      "step 574, loss = 6.940447201486677e-05\n",
      "step 575, loss = 7.803418702678755e-05\n",
      "step 576, loss = 7.81071066739969e-05\n",
      "step 577, loss = 7.146130519686267e-05\n",
      "step 578, loss = 8.25573515612632e-05\n",
      "step 579, loss = 6.764088902855292e-05\n",
      "step 580, loss = 6.515924178529531e-05\n",
      "step 581, loss = 8.761983917793259e-05\n",
      "step 582, loss = 5.358172347769141e-05\n",
      "step 583, loss = 8.125740714604035e-05\n",
      "step 584, loss = 6.455832772189751e-05\n",
      "step 585, loss = 7.846648077247664e-05\n",
      "step 586, loss = 5.397721179178916e-05\n",
      "step 587, loss = 6.826029857620597e-05\n",
      "step 588, loss = 7.150222518248484e-05\n",
      "step 589, loss = 6.84430415276438e-05\n",
      "step 590, loss = 5.130016143084504e-05\n",
      "step 591, loss = 8.922185224946588e-05\n",
      "step 592, loss = 7.588163134641945e-05\n",
      "step 593, loss = 9.106164361583069e-05\n",
      "step 594, loss = 7.501750224037096e-05\n",
      "step 595, loss = 7.873013964854181e-05\n",
      "step 596, loss = 7.310535875149071e-05\n",
      "step 597, loss = 7.898921467131004e-05\n",
      "step 598, loss = 6.286768621066585e-05\n",
      "step 599, loss = 5.848892396898009e-05\n",
      "step 600, loss = 6.924583431100473e-05\n",
      "step 601, loss = 9.623175719752908e-05\n",
      "step 602, loss = 5.944039367022924e-05\n",
      "step 603, loss = 7.753582031000406e-05\n",
      "step 604, loss = 7.93526487541385e-05\n",
      "step 605, loss = 8.375913603231311e-05\n",
      "step 606, loss = 9.549655806040391e-05\n",
      "step 607, loss = 9.231812146026641e-05\n",
      "step 608, loss = 6.947576912352815e-05\n",
      "step 609, loss = 5.8880286815110594e-05\n",
      "step 610, loss = 5.927198435529135e-05\n",
      "step 611, loss = 0.0001292059023398906\n",
      "step 612, loss = 0.00010160761303268373\n",
      "step 613, loss = 7.930392894195393e-05\n",
      "step 614, loss = 9.030158980749547e-05\n",
      "step 615, loss = 7.09860396455042e-05\n",
      "step 616, loss = 6.997866876190528e-05\n",
      "step 617, loss = 8.270973921753466e-05\n",
      "step 618, loss = 7.094479951774701e-05\n",
      "step 619, loss = 7.559056393802166e-05\n",
      "step 620, loss = 7.088088023010641e-05\n",
      "step 621, loss = 8.475897629978135e-05\n",
      "step 622, loss = 5.7627996284281835e-05\n",
      "step 623, loss = 7.786781497998163e-05\n",
      "step 624, loss = 7.616341463290155e-05\n",
      "step 625, loss = 7.430642290273681e-05\n",
      "step 626, loss = 9.2668182333e-05\n",
      "step 627, loss = 8.066256123129278e-05\n",
      "step 628, loss = 6.393318471964449e-05\n",
      "step 629, loss = 6.72887108521536e-05\n",
      "step 630, loss = 7.317015843000263e-05\n",
      "step 631, loss = 7.529098365921527e-05\n",
      "step 632, loss = 9.306623542215675e-05\n",
      "step 633, loss = 8.922670531319454e-05\n",
      "step 634, loss = 9.67134692473337e-05\n",
      "step 635, loss = 9.147405216936022e-05\n",
      "step 636, loss = 7.891756831668317e-05\n",
      "step 637, loss = 9.18850491871126e-05\n",
      "step 638, loss = 6.126606604084373e-05\n",
      "step 639, loss = 9.030847286339849e-05\n",
      "step 640, loss = 7.587850268464535e-05\n",
      "step 641, loss = 0.00011211164382984862\n",
      "step 642, loss = 9.249252616427839e-05\n",
      "step 643, loss = 7.859015750000253e-05\n",
      "step 644, loss = 9.894720278680325e-05\n",
      "step 645, loss = 0.00010194130300078541\n",
      "step 646, loss = 9.785490692593157e-05\n",
      "step 647, loss = 8.47496630740352e-05\n",
      "step 648, loss = 7.925724639790133e-05\n",
      "step 649, loss = 0.00010015418956754729\n",
      "step 650, loss = 7.430955156451091e-05\n",
      "step 651, loss = 9.19069570954889e-05\n",
      "step 652, loss = 6.296858191490173e-05\n",
      "step 653, loss = 7.129285222617909e-05\n",
      "step 654, loss = 7.188633753685281e-05\n",
      "step 655, loss = 8.275374420918524e-05\n",
      "step 656, loss = 7.394863496301696e-05\n",
      "step 657, loss = 6.614640733459964e-05\n",
      "step 658, loss = 9.678945934865624e-05\n",
      "step 659, loss = 9.531317482469603e-05\n",
      "step 660, loss = 9.756992949405685e-05\n",
      "step 661, loss = 9.590067929821089e-05\n",
      "step 662, loss = 6.280936213443056e-05\n",
      "step 663, loss = 6.69238215778023e-05\n",
      "step 664, loss = 6.945397035451606e-05\n",
      "step 665, loss = 6.2255458033178e-05\n",
      "step 666, loss = 5.5270236771320924e-05\n",
      "step 667, loss = 0.00010303801536792889\n",
      "step 668, loss = 5.961098213447258e-05\n",
      "step 669, loss = 7.344145706156269e-05\n",
      "step 670, loss = 6.583362846868113e-05\n",
      "step 671, loss = 8.402048842981458e-05\n",
      "step 672, loss = 7.454764272551984e-05\n",
      "step 673, loss = 7.597560033900663e-05\n",
      "step 674, loss = 7.138148794183508e-05\n",
      "step 675, loss = 8.760911441640928e-05\n",
      "step 676, loss = 5.938730828347616e-05\n",
      "step 677, loss = 7.686066965106875e-05\n",
      "step 678, loss = 0.00010722152364905924\n",
      "step 679, loss = 4.808439189218916e-05\n",
      "step 680, loss = 7.600685057695955e-05\n",
      "step 681, loss = 7.798897422617301e-05\n",
      "step 682, loss = 8.385536057176068e-05\n",
      "step 683, loss = 6.465551268775016e-05\n",
      "step 684, loss = 5.287670501274988e-05\n",
      "step 685, loss = 9.255767508875579e-05\n",
      "step 686, loss = 0.00011545519373612478\n",
      "step 687, loss = 6.160122575238347e-05\n",
      "step 688, loss = 6.75850169500336e-05\n",
      "step 689, loss = 6.926531932549551e-05\n",
      "step 690, loss = 6.974392454139888e-05\n",
      "step 691, loss = 7.014306174824014e-05\n",
      "step 692, loss = 7.622650446137413e-05\n",
      "step 693, loss = 6.663522071903571e-05\n",
      "step 694, loss = 9.223396773450077e-05\n",
      "step 695, loss = 5.03036062582396e-05\n",
      "step 696, loss = 5.7900273532141e-05\n",
      "step 697, loss = 6.907353235874325e-05\n",
      "step 698, loss = 7.85853189881891e-05\n",
      "step 699, loss = 5.835327101522125e-05\n",
      "step 700, loss = 6.157544703455642e-05\n",
      "step 701, loss = 9.936217247741297e-05\n",
      "step 702, loss = 6.123692583059892e-05\n",
      "step 703, loss = 6.529486563522369e-05\n",
      "step 704, loss = 6.661106453975663e-05\n",
      "step 705, loss = 5.966768731013872e-05\n",
      "step 706, loss = 7.443704816978425e-05\n",
      "step 707, loss = 7.184054993558675e-05\n",
      "step 708, loss = 6.726888386765495e-05\n",
      "step 709, loss = 8.643580804346129e-05\n",
      "step 710, loss = 6.057930295355618e-05\n",
      "step 711, loss = 7.279229612322524e-05\n",
      "step 712, loss = 7.188862218754366e-05\n",
      "step 713, loss = 8.491164771839976e-05\n",
      "step 714, loss = 6.308346928562969e-05\n",
      "step 715, loss = 7.625934813404456e-05\n",
      "step 716, loss = 0.00010273470252286643\n",
      "step 717, loss = 8.433382026851177e-05\n",
      "step 718, loss = 7.287433254532516e-05\n",
      "step 719, loss = 6.756743096048012e-05\n",
      "step 720, loss = 7.420030306093395e-05\n",
      "step 721, loss = 8.564005111111328e-05\n",
      "step 722, loss = 7.477124017896131e-05\n",
      "step 723, loss = 6.382117862813175e-05\n",
      "step 724, loss = 8.286501542897895e-05\n",
      "step 725, loss = 7.903155346866697e-05\n",
      "step 726, loss = 6.404864689102396e-05\n",
      "step 727, loss = 7.956069021020085e-05\n",
      "step 728, loss = 6.83914840919897e-05\n",
      "step 729, loss = 6.125694198999554e-05\n",
      "step 730, loss = 8.232152322307229e-05\n",
      "step 731, loss = 6.259579822653905e-05\n",
      "step 732, loss = 7.3050003265962e-05\n",
      "step 733, loss = 8.791057916823775e-05\n",
      "step 734, loss = 7.706609176238999e-05\n",
      "step 735, loss = 7.283668674062937e-05\n",
      "step 736, loss = 6.184713129187003e-05\n",
      "step 737, loss = 7.720338180661201e-05\n",
      "step 738, loss = 8.513593638781458e-05\n",
      "step 739, loss = 7.953423482831568e-05\n",
      "step 740, loss = 7.103398820618168e-05\n",
      "step 741, loss = 5.822505409014411e-05\n",
      "step 742, loss = 8.325669477926567e-05\n",
      "step 743, loss = 7.099096546880901e-05\n",
      "step 744, loss = 8.16294050309807e-05\n",
      "step 745, loss = 6.796747766202316e-05\n",
      "step 746, loss = 8.616034028818831e-05\n",
      "step 747, loss = 7.059243216644973e-05\n",
      "step 748, loss = 7.87454191595316e-05\n",
      "step 749, loss = 6.271553138503805e-05\n",
      "step 750, loss = 8.68484130478464e-05\n",
      "step 751, loss = 7.904873200459406e-05\n",
      "step 752, loss = 6.576443411177024e-05\n",
      "step 753, loss = 6.41120204818435e-05\n",
      "step 754, loss = 5.882615005248226e-05\n",
      "step 755, loss = 9.178956679534167e-05\n",
      "step 756, loss = 7.07876097294502e-05\n",
      "step 757, loss = 9.513500845059752e-05\n",
      "step 758, loss = 8.005540439626202e-05\n",
      "step 759, loss = 6.636488978983834e-05\n",
      "step 760, loss = 8.273152343463153e-05\n",
      "step 761, loss = 7.5818708864972e-05\n",
      "step 762, loss = 5.3422441851580516e-05\n",
      "step 763, loss = 7.071981235640123e-05\n",
      "step 764, loss = 9.861875150818378e-05\n",
      "step 765, loss = 0.00014348897093441337\n",
      "step 766, loss = 6.826476601418108e-05\n",
      "step 767, loss = 7.554745388915762e-05\n",
      "step 768, loss = 8.552688086638227e-05\n",
      "step 769, loss = 7.364890916505828e-05\n",
      "step 770, loss = 7.586166611872613e-05\n",
      "step 771, loss = 8.601491572335362e-05\n",
      "step 772, loss = 5.441135726869106e-05\n",
      "step 773, loss = 9.042910824064165e-05\n",
      "step 774, loss = 9.165832307189703e-05\n",
      "step 775, loss = 7.092699524946511e-05\n",
      "step 776, loss = 7.303833990590647e-05\n",
      "step 777, loss = 8.70329313329421e-05\n",
      "step 778, loss = 9.574397699907422e-05\n",
      "step 779, loss = 0.00010534383181948215\n",
      "step 780, loss = 6.132219277787954e-05\n",
      "step 781, loss = 7.471688149962574e-05\n",
      "step 782, loss = 0.00010057986946776509\n",
      "step 783, loss = 7.417220331262797e-05\n",
      "step 784, loss = 0.00010520347859710455\n",
      "step 785, loss = 7.378504233201966e-05\n",
      "step 786, loss = 9.573107672622427e-05\n",
      "step 787, loss = 9.31655740714632e-05\n",
      "step 788, loss = 9.069731459021568e-05\n",
      "step 789, loss = 8.120985148707405e-05\n",
      "step 790, loss = 6.551331898663193e-05\n",
      "step 791, loss = 9.850678179645911e-05\n",
      "step 792, loss = 0.00011085446021752432\n",
      "step 793, loss = 5.134106322657317e-05\n",
      "step 794, loss = 4.779777009389363e-05\n",
      "step 795, loss = 6.193601439008489e-05\n",
      "step 796, loss = 7.578238728456199e-05\n",
      "step 797, loss = 6.678110366920009e-05\n",
      "step 798, loss = 7.79799374868162e-05\n",
      "step 799, loss = 9.476186096435413e-05\n",
      "step 800, loss = 8.97258723853156e-05\n",
      "step 801, loss = 7.707267650403082e-05\n",
      "step 802, loss = 7.178543455665931e-05\n",
      "step 803, loss = 8.609468932263553e-05\n",
      "step 804, loss = 8.703624916961417e-05\n",
      "step 805, loss = 7.27598526282236e-05\n",
      "step 806, loss = 5.777757178293541e-05\n",
      "step 807, loss = 7.637490489287302e-05\n",
      "step 808, loss = 8.327711111633107e-05\n",
      "step 809, loss = 6.544704228872433e-05\n",
      "step 810, loss = 7.260168786160648e-05\n",
      "step 811, loss = 7.114997424650937e-05\n",
      "step 812, loss = 6.838925037300214e-05\n",
      "step 813, loss = 8.217057620640844e-05\n",
      "step 814, loss = 6.658671190962195e-05\n",
      "step 815, loss = 8.249150414485484e-05\n",
      "step 816, loss = 7.370689854724333e-05\n",
      "step 817, loss = 6.54200921417214e-05\n",
      "step 818, loss = 6.0338508774293587e-05\n",
      "step 819, loss = 7.269874913617969e-05\n",
      "step 820, loss = 8.524188888259232e-05\n",
      "step 821, loss = 7.405952783301473e-05\n",
      "step 822, loss = 8.793763117864728e-05\n",
      "step 823, loss = 7.146214920794591e-05\n",
      "step 824, loss = 7.354938134085387e-05\n",
      "step 825, loss = 6.952534022275358e-05\n",
      "step 826, loss = 7.79869151301682e-05\n",
      "step 827, loss = 9.512253745924681e-05\n",
      "step 828, loss = 8.889125456335023e-05\n",
      "step 829, loss = 7.408097008010373e-05\n",
      "step 830, loss = 7.70549158914946e-05\n",
      "step 831, loss = 7.947812991915271e-05\n",
      "step 832, loss = 9.432161459699273e-05\n",
      "step 833, loss = 7.191053009591997e-05\n",
      "step 834, loss = 6.735535862389952e-05\n",
      "step 835, loss = 9.281920938519761e-05\n",
      "step 836, loss = 6.197974289534613e-05\n",
      "step 837, loss = 9.308855078415945e-05\n",
      "step 838, loss = 7.759451546007767e-05\n",
      "step 839, loss = 6.67891072225757e-05\n",
      "step 840, loss = 9.488892101217061e-05\n",
      "step 841, loss = 9.871702786767855e-05\n",
      "step 842, loss = 7.935879693832248e-05\n",
      "step 843, loss = 5.0359423767076805e-05\n",
      "step 844, loss = 6.63535829517059e-05\n",
      "step 845, loss = 9.579494508216158e-05\n",
      "step 846, loss = 9.989214595407248e-05\n",
      "step 847, loss = 6.935855344636366e-05\n",
      "step 848, loss = 9.09126247279346e-05\n",
      "step 849, loss = 0.00010709724301705137\n",
      "step 850, loss = 9.089927334571257e-05\n",
      "step 851, loss = 6.524451600853354e-05\n",
      "step 852, loss = 8.587606134824455e-05\n",
      "step 853, loss = 7.794256089255214e-05\n",
      "step 854, loss = 0.00010043058136943728\n",
      "step 855, loss = 7.032260327832773e-05\n",
      "step 856, loss = 6.806860619690269e-05\n",
      "step 857, loss = 8.336044993484393e-05\n",
      "step 858, loss = 7.771475065965205e-05\n",
      "step 859, loss = 6.640274659730494e-05\n",
      "step 860, loss = 6.950935494387522e-05\n",
      "step 861, loss = 9.165480150841177e-05\n",
      "step 862, loss = 7.708476186962798e-05\n",
      "step 863, loss = 8.592055382905528e-05\n",
      "step 864, loss = 5.9621681430144235e-05\n",
      "step 865, loss = 6.875484541524202e-05\n",
      "step 866, loss = 6.885208858875558e-05\n",
      "step 867, loss = 5.378931018640287e-05\n",
      "step 868, loss = 9.886139741865918e-05\n",
      "step 869, loss = 7.313063542824239e-05\n",
      "step 870, loss = 0.0001111931778723374\n",
      "step 871, loss = 8.28900738270022e-05\n",
      "step 872, loss = 8.155441173585132e-05\n",
      "step 873, loss = 8.630601951153949e-05\n",
      "step 874, loss = 7.82086281105876e-05\n",
      "step 875, loss = 8.101199637167156e-05\n",
      "step 876, loss = 8.630949741927907e-05\n",
      "step 877, loss = 6.885678885737434e-05\n",
      "step 878, loss = 6.389990448951721e-05\n",
      "step 879, loss = 7.61901173973456e-05\n",
      "step 880, loss = 7.340190495597199e-05\n",
      "step 881, loss = 7.632740744156763e-05\n",
      "step 882, loss = 6.890308577567339e-05\n",
      "step 883, loss = 5.956360837444663e-05\n",
      "step 884, loss = 6.353124626912177e-05\n",
      "step 885, loss = 7.01313401805237e-05\n",
      "step 886, loss = 6.173026486067101e-05\n",
      "step 887, loss = 7.585746061522514e-05\n",
      "step 888, loss = 6.114031566539779e-05\n",
      "step 889, loss = 8.822800009511411e-05\n",
      "step 890, loss = 8.666793291922659e-05\n",
      "step 891, loss = 6.600940105272457e-05\n",
      "step 892, loss = 6.606545503018424e-05\n",
      "step 893, loss = 7.489261770388111e-05\n",
      "step 894, loss = 7.428729441016912e-05\n",
      "step 895, loss = 5.767431139247492e-05\n",
      "step 896, loss = 7.786363858031109e-05\n",
      "step 897, loss = 7.019697659416124e-05\n",
      "step 898, loss = 6.949061207706109e-05\n",
      "step 899, loss = 7.210501644294709e-05\n",
      "step 900, loss = 6.984066567383707e-05\n",
      "step 901, loss = 7.369605009444058e-05\n",
      "step 902, loss = 7.127460412448272e-05\n",
      "step 903, loss = 7.174080383265391e-05\n",
      "step 904, loss = 8.309636177727953e-05\n",
      "step 905, loss = 6.3194333051797e-05\n",
      "step 906, loss = 5.700750261894427e-05\n",
      "step 907, loss = 7.376584107987583e-05\n",
      "step 908, loss = 6.615803431486711e-05\n",
      "step 909, loss = 7.218847167678177e-05\n",
      "step 910, loss = 8.623485337011516e-05\n",
      "step 911, loss = 7.725336763542145e-05\n",
      "step 912, loss = 8.213097316911444e-05\n",
      "step 913, loss = 8.478761446895078e-05\n",
      "step 914, loss = 5.833878094563261e-05\n",
      "step 915, loss = 7.747847121208906e-05\n",
      "step 916, loss = 8.702288323547691e-05\n",
      "step 917, loss = 6.0172591474838555e-05\n",
      "step 918, loss = 6.049797957530245e-05\n",
      "step 919, loss = 6.013499296386726e-05\n",
      "step 920, loss = 0.00010520674550207332\n",
      "step 921, loss = 6.728907465003431e-05\n",
      "step 922, loss = 6.697517528664321e-05\n",
      "step 923, loss = 7.898313197074458e-05\n",
      "step 924, loss = 7.715702668065205e-05\n",
      "step 925, loss = 8.488650200888515e-05\n",
      "step 926, loss = 5.900049291085452e-05\n",
      "step 927, loss = 8.755632006796077e-05\n",
      "step 928, loss = 7.099471258698031e-05\n",
      "step 929, loss = 7.54332504584454e-05\n",
      "step 930, loss = 5.609413346974179e-05\n",
      "step 931, loss = 7.204859866760671e-05\n",
      "step 932, loss = 6.206709804246202e-05\n",
      "step 933, loss = 6.747272709617391e-05\n",
      "step 934, loss = 7.947010453790426e-05\n",
      "step 935, loss = 7.428693061228842e-05\n",
      "step 936, loss = 6.922302418388426e-05\n",
      "step 937, loss = 7.579605153296143e-05\n",
      "step 938, loss = 7.719726272625849e-05\n",
      "step 939, loss = 7.13877598172985e-05\n",
      "step 940, loss = 8.540970884496346e-05\n",
      "step 941, loss = 7.045223901513964e-05\n",
      "step 942, loss = 5.89210721955169e-05\n",
      "step 943, loss = 8.177080599125475e-05\n",
      "step 944, loss = 6.391244824044406e-05\n",
      "step 945, loss = 5.1923387218266726e-05\n",
      "step 946, loss = 7.329096843022853e-05\n",
      "step 947, loss = 7.766457565594465e-05\n",
      "step 948, loss = 6.911345553817227e-05\n",
      "step 949, loss = 8.074606739683077e-05\n",
      "step 950, loss = 6.77158750477247e-05\n",
      "step 951, loss = 7.645614823559299e-05\n",
      "step 952, loss = 6.920768646523356e-05\n",
      "step 953, loss = 7.01218014000915e-05\n",
      "step 954, loss = 6.416894029825926e-05\n",
      "step 955, loss = 7.596678915433586e-05\n",
      "step 956, loss = 9.036730625666678e-05\n",
      "step 957, loss = 8.022676774999127e-05\n",
      "step 958, loss = 5.357404370442964e-05\n",
      "step 959, loss = 7.011018169578165e-05\n",
      "step 960, loss = 7.80140035203658e-05\n",
      "step 961, loss = 6.452541856560856e-05\n",
      "step 962, loss = 6.635814497713e-05\n",
      "step 963, loss = 5.6607885198900476e-05\n",
      "step 964, loss = 8.976305980468169e-05\n",
      "step 965, loss = 0.00011892510519828647\n",
      "step 966, loss = 6.800737901357934e-05\n",
      "step 967, loss = 7.845719665056095e-05\n",
      "step 968, loss = 7.702285802224651e-05\n",
      "step 969, loss = 7.928071136120707e-05\n",
      "step 970, loss = 8.693984273122624e-05\n",
      "step 971, loss = 0.00011252959666308016\n",
      "step 972, loss = 5.5586562666576356e-05\n",
      "step 973, loss = 9.893192327581346e-05\n",
      "step 974, loss = 8.370710565941408e-05\n",
      "step 975, loss = 0.00011352668661857024\n",
      "step 976, loss = 7.227080641314387e-05\n",
      "step 977, loss = 7.8522578405682e-05\n",
      "step 978, loss = 9.366595622850582e-05\n",
      "step 979, loss = 8.706566586624831e-05\n",
      "step 980, loss = 6.005528484820388e-05\n",
      "step 981, loss = 7.858573371777311e-05\n",
      "step 982, loss = 6.151236448204145e-05\n",
      "step 983, loss = 5.518854595720768e-05\n",
      "step 984, loss = 7.076704787323251e-05\n",
      "step 985, loss = 6.807605677749962e-05\n",
      "step 986, loss = 6.847485929029062e-05\n",
      "step 987, loss = 5.715203224099241e-05\n",
      "step 988, loss = 9.259655053028837e-05\n",
      "step 989, loss = 6.590022530872375e-05\n",
      "step 990, loss = 8.468244777759537e-05\n",
      "step 991, loss = 6.974616553634405e-05\n",
      "step 992, loss = 7.605549762956798e-05\n",
      "step 993, loss = 6.711243622703478e-05\n",
      "step 994, loss = 5.831632734043524e-05\n",
      "step 995, loss = 0.00011464254930615425\n",
      "step 996, loss = 8.498915121890604e-05\n",
      "step 997, loss = 7.362382166320458e-05\n",
      "step 998, loss = 5.255021096672863e-05\n",
      "step 999, loss = 8.888517186278477e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "n = 1000\n",
    "\n",
    "model = Model()\n",
    "optim = jt.nn.Adam(model.parameters(), learning_rate)\n",
    "begin = time.time()\n",
    "\n",
    "for i,(x,y) in enumerate(get_data(n)):\n",
    "    pred_y = model(x)\n",
    "    dy = pred_y - y\n",
    "    loss = dy * dy\n",
    "    loss_mean = loss.mean()\n",
    "    optim.step(loss_mean)\n",
    "    print(f\"step {i}, loss = {loss_mean.data.sum()}\")\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4234457015991211 s has passed.\n"
     ]
    }
   ],
   "source": [
    "print(end - begin, \"s has passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 1, 0.01).reshape(-1, 1)\n",
    "y = x ** 2\n",
    "pred_y = model(jt.float32(x)).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, 'o', color='orange')\n",
    "plt.plot(x, pred_y, '*', color='violet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the same model with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torchModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(torchModel, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(1, 10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(10, 1)\n",
    "    def forward(self, x) :\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "def get_torch_data(n): # generate random data for training test.\n",
    "    for i in range(n):\n",
    "        x = np.random.rand(batch_size, 1)\n",
    "        x = x.astype('float32')\n",
    "        y = x*x\n",
    "        yield torch.from_numpy(x), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 0.7859153747558594\n",
      "step 1, loss = 0.7705705165863037\n",
      "step 2, loss = 0.6442081332206726\n",
      "step 3, loss = 0.6362539529800415\n",
      "step 4, loss = 0.3119353652000427\n",
      "step 5, loss = 0.4429492950439453\n",
      "step 6, loss = 0.30837762355804443\n",
      "step 7, loss = 0.1648431420326233\n",
      "step 8, loss = 0.14141987264156342\n",
      "step 9, loss = 0.1764698624610901\n",
      "step 10, loss = 0.11543026566505432\n",
      "step 11, loss = 0.06435520946979523\n",
      "step 12, loss = 0.08248403668403625\n",
      "step 13, loss = 0.05172397196292877\n",
      "step 14, loss = 0.04292145371437073\n",
      "step 15, loss = 0.04910488426685333\n",
      "step 16, loss = 0.03954558074474335\n",
      "step 17, loss = 0.04208924248814583\n",
      "step 18, loss = 0.05084012821316719\n",
      "step 19, loss = 0.061165593564510345\n",
      "step 20, loss = 0.07258296012878418\n",
      "step 21, loss = 0.06839385628700256\n",
      "step 22, loss = 0.06427064538002014\n",
      "step 23, loss = 0.08223280310630798\n",
      "step 24, loss = 0.08436441421508789\n",
      "step 25, loss = 0.07160846889019012\n",
      "step 26, loss = 0.06663200259208679\n",
      "step 27, loss = 0.060245122760534286\n",
      "step 28, loss = 0.050322338938713074\n",
      "step 29, loss = 0.054026756435632706\n",
      "step 30, loss = 0.058604251593351364\n",
      "step 31, loss = 0.039033014327287674\n",
      "step 32, loss = 0.03889204561710358\n",
      "step 33, loss = 0.045829888433218\n",
      "step 34, loss = 0.03228617459535599\n",
      "step 35, loss = 0.03476898744702339\n",
      "step 36, loss = 0.03367691859602928\n",
      "step 37, loss = 0.03052489459514618\n",
      "step 38, loss = 0.032841261476278305\n",
      "step 39, loss = 0.0315193235874176\n",
      "step 40, loss = 0.03624128922820091\n",
      "step 41, loss = 0.03249727189540863\n",
      "step 42, loss = 0.02659042365849018\n",
      "step 43, loss = 0.026620006188750267\n",
      "step 44, loss = 0.02553274668753147\n",
      "step 45, loss = 0.04358012229204178\n",
      "step 46, loss = 0.0448216013610363\n",
      "step 47, loss = 0.03643995150923729\n",
      "step 48, loss = 0.029484350234270096\n",
      "step 49, loss = 0.026811763644218445\n",
      "step 50, loss = 0.02587248384952545\n",
      "step 51, loss = 0.029154567047953606\n",
      "step 52, loss = 0.02358066290616989\n",
      "step 53, loss = 0.018742477521300316\n",
      "step 54, loss = 0.026681652292609215\n",
      "step 55, loss = 0.022811079397797585\n",
      "step 56, loss = 0.01554927695542574\n",
      "step 57, loss = 0.023883156478405\n",
      "step 58, loss = 0.02205328457057476\n",
      "step 59, loss = 0.022144541144371033\n",
      "step 60, loss = 0.01803344301879406\n",
      "step 61, loss = 0.01892353594303131\n",
      "step 62, loss = 0.017968736588954926\n",
      "step 63, loss = 0.01764865033328533\n",
      "step 64, loss = 0.016478292644023895\n",
      "step 65, loss = 0.014316098764538765\n",
      "step 66, loss = 0.017714427784085274\n",
      "step 67, loss = 0.01441151648759842\n",
      "step 68, loss = 0.014972507022321224\n",
      "step 69, loss = 0.011426734738051891\n",
      "step 70, loss = 0.013474252074956894\n",
      "step 71, loss = 0.009250345639884472\n",
      "step 72, loss = 0.01337160263210535\n",
      "step 73, loss = 0.010790355503559113\n",
      "step 74, loss = 0.006979266181588173\n",
      "step 75, loss = 0.00981044489890337\n",
      "step 76, loss = 0.012158398516476154\n",
      "step 77, loss = 0.012415139935910702\n",
      "step 78, loss = 0.00978690292686224\n",
      "step 79, loss = 0.010034114122390747\n",
      "step 80, loss = 0.008008922450244427\n",
      "step 81, loss = 0.011233948171138763\n",
      "step 82, loss = 0.009589022025465965\n",
      "step 83, loss = 0.007477080915123224\n",
      "step 84, loss = 0.007384767755866051\n",
      "step 85, loss = 0.006802558433264494\n",
      "step 86, loss = 0.007515148725360632\n",
      "step 87, loss = 0.008198992349207401\n",
      "step 88, loss = 0.0073509784415364265\n",
      "step 89, loss = 0.0086097726598382\n",
      "step 90, loss = 0.007798806298524141\n",
      "step 91, loss = 0.005850731860846281\n",
      "step 92, loss = 0.0046972413547337055\n",
      "step 93, loss = 0.004681774415075779\n",
      "step 94, loss = 0.006829325575381517\n",
      "step 95, loss = 0.005489814095199108\n",
      "step 96, loss = 0.005463338457047939\n",
      "step 97, loss = 0.005748072639107704\n",
      "step 98, loss = 0.006633053533732891\n",
      "step 99, loss = 0.005482689943164587\n",
      "step 100, loss = 0.0041096447966992855\n",
      "step 101, loss = 0.004906027112156153\n",
      "step 102, loss = 0.004293402191251516\n",
      "step 103, loss = 0.004032553639262915\n",
      "step 104, loss = 0.004281002562493086\n",
      "step 105, loss = 0.004324052482843399\n",
      "step 106, loss = 0.004322094842791557\n",
      "step 107, loss = 0.003928858786821365\n",
      "step 108, loss = 0.00336888013407588\n",
      "step 109, loss = 0.0038653856609016657\n",
      "step 110, loss = 0.0033997762948274612\n",
      "step 111, loss = 0.0044409241527318954\n",
      "step 112, loss = 0.0024077289272099733\n",
      "step 113, loss = 0.0027482095174491405\n",
      "step 114, loss = 0.002685458632186055\n",
      "step 115, loss = 0.004058170132339001\n",
      "step 116, loss = 0.004277542699128389\n",
      "step 117, loss = 0.0031827925704419613\n",
      "step 118, loss = 0.002622466767206788\n",
      "step 119, loss = 0.003142071422189474\n",
      "step 120, loss = 0.002358198631554842\n",
      "step 121, loss = 0.0017701095202937722\n",
      "step 122, loss = 0.002474353648722172\n",
      "step 123, loss = 0.0024509986396878958\n",
      "step 124, loss = 0.002487002406269312\n",
      "step 125, loss = 0.002188353566452861\n",
      "step 126, loss = 0.0016810934757813811\n",
      "step 127, loss = 0.00221946369856596\n",
      "step 128, loss = 0.0015348014421761036\n",
      "step 129, loss = 0.0020062951371073723\n",
      "step 130, loss = 0.0022124554961919785\n",
      "step 131, loss = 0.002868811832740903\n",
      "step 132, loss = 0.001841511926613748\n",
      "step 133, loss = 0.0021041417494416237\n",
      "step 134, loss = 0.003223748877644539\n",
      "step 135, loss = 0.003585791913792491\n",
      "step 136, loss = 0.001949466299265623\n",
      "step 137, loss = 0.0023696492426097393\n",
      "step 138, loss = 0.0018403189023956656\n",
      "step 139, loss = 0.002127633662894368\n",
      "step 140, loss = 0.0024214712902903557\n",
      "step 141, loss = 0.0023949192836880684\n",
      "step 142, loss = 0.002483114367350936\n",
      "step 143, loss = 0.0017666672356426716\n",
      "step 144, loss = 0.0019067047396674752\n",
      "step 145, loss = 0.0015119670424610376\n",
      "step 146, loss = 0.0013386887731030583\n",
      "step 147, loss = 0.0016844780184328556\n",
      "step 148, loss = 0.001253704889677465\n",
      "step 149, loss = 0.002072084927931428\n",
      "step 150, loss = 0.0007242411957122386\n",
      "step 151, loss = 0.0017510050674900413\n",
      "step 152, loss = 0.0015170009573921561\n",
      "step 153, loss = 0.0015812157653272152\n",
      "step 154, loss = 0.002252488397061825\n",
      "step 155, loss = 0.0016793198883533478\n",
      "step 156, loss = 0.0014911998296156526\n",
      "step 157, loss = 0.001015710411593318\n",
      "step 158, loss = 0.001228676876053214\n",
      "step 159, loss = 0.0012162572238594294\n",
      "step 160, loss = 0.0014665095368400216\n",
      "step 161, loss = 0.0014959705295041203\n",
      "step 162, loss = 0.001014189445413649\n",
      "step 163, loss = 0.0011915526119992137\n",
      "step 164, loss = 0.0008992844959720969\n",
      "step 165, loss = 0.0014563427539542317\n",
      "step 166, loss = 0.0012267041020095348\n",
      "step 167, loss = 0.0011562641011551023\n",
      "step 168, loss = 0.001161731081083417\n",
      "step 169, loss = 0.0012739901430904865\n",
      "step 170, loss = 0.0012361938133835793\n",
      "step 171, loss = 0.0010192720219492912\n",
      "step 172, loss = 0.0016867315862327814\n",
      "step 173, loss = 0.0010722464649006724\n",
      "step 174, loss = 0.001336374320089817\n",
      "step 175, loss = 0.0010902791982516646\n",
      "step 176, loss = 0.0011832304298877716\n",
      "step 177, loss = 0.001047252444550395\n",
      "step 178, loss = 0.0006247397977858782\n",
      "step 179, loss = 0.0008244441705755889\n",
      "step 180, loss = 0.001207864610478282\n",
      "step 181, loss = 0.0006635929457843304\n",
      "step 182, loss = 0.0008554731030017138\n",
      "step 183, loss = 0.0007721089059486985\n",
      "step 184, loss = 0.0009502215543761849\n",
      "step 185, loss = 0.0008838546928018332\n",
      "step 186, loss = 0.0011855950579047203\n",
      "step 187, loss = 0.0009700464433990419\n",
      "step 188, loss = 0.0007749040960334241\n",
      "step 189, loss = 0.0010045323288068175\n",
      "step 190, loss = 0.0010431213304400444\n",
      "step 191, loss = 0.0009244199609383941\n",
      "step 192, loss = 0.0008135828538797796\n",
      "step 193, loss = 0.0009376361267641187\n",
      "step 194, loss = 0.0005914310459047556\n",
      "step 195, loss = 0.000447520287707448\n",
      "step 196, loss = 0.0010074828751385212\n",
      "step 197, loss = 0.0005143166636116803\n",
      "step 198, loss = 0.0008060674881562591\n",
      "step 199, loss = 0.0009001418366096914\n",
      "step 200, loss = 0.00044750809320248663\n",
      "step 201, loss = 0.0007863713544793427\n",
      "step 202, loss = 0.0010651199845597148\n",
      "step 203, loss = 0.0005013698246330023\n",
      "step 204, loss = 0.0005253446870483458\n",
      "step 205, loss = 0.00035953809856437147\n",
      "step 206, loss = 0.0004949504509568214\n",
      "step 207, loss = 0.0009617028990760446\n",
      "step 208, loss = 0.0006260780501179397\n",
      "step 209, loss = 0.0006956537254154682\n",
      "step 210, loss = 0.0006764292484149337\n",
      "step 211, loss = 0.0009539283346384764\n",
      "step 212, loss = 0.0008313179714605212\n",
      "step 213, loss = 0.0005421836394816637\n",
      "step 214, loss = 0.0005759424529969692\n",
      "step 215, loss = 0.0005811540177091956\n",
      "step 216, loss = 0.0007279959972947836\n",
      "step 217, loss = 0.0004515231412369758\n",
      "step 218, loss = 0.0008428462315350771\n",
      "step 219, loss = 0.00045479246182367206\n",
      "step 220, loss = 0.0007090225699357688\n",
      "step 221, loss = 0.0007768380455672741\n",
      "step 222, loss = 0.0007122193346731365\n",
      "step 223, loss = 0.0007132317987270653\n",
      "step 224, loss = 0.000528889533597976\n",
      "step 225, loss = 0.0005075035151094198\n",
      "step 226, loss = 0.0002958642435260117\n",
      "step 227, loss = 0.0006272327154874802\n",
      "step 228, loss = 0.00046054483391344547\n",
      "step 229, loss = 0.000591089716181159\n",
      "step 230, loss = 0.000500136346090585\n",
      "step 231, loss = 0.0006573056452907622\n",
      "step 232, loss = 0.0005748650291934609\n",
      "step 233, loss = 0.0004507649573497474\n",
      "step 234, loss = 0.00044895641622133553\n",
      "step 235, loss = 0.0003697086067404598\n",
      "step 236, loss = 0.00041696667904034257\n",
      "step 237, loss = 0.0006645256071351469\n",
      "step 238, loss = 0.00041628506733104587\n",
      "step 239, loss = 0.0003756957594305277\n",
      "step 240, loss = 0.0004013748839497566\n",
      "step 241, loss = 0.0004033464065287262\n",
      "step 242, loss = 0.0004801379982382059\n",
      "step 243, loss = 0.0004525800468400121\n",
      "step 244, loss = 0.0003933555562980473\n",
      "step 245, loss = 0.0004096189804840833\n",
      "step 246, loss = 0.00046262433170340955\n",
      "step 247, loss = 0.0003007076447829604\n",
      "step 248, loss = 0.0006240455550141633\n",
      "step 249, loss = 0.0007659883704036474\n",
      "step 250, loss = 0.0006314052152447402\n",
      "step 251, loss = 0.0005032552871853113\n",
      "step 252, loss = 0.0007204171270132065\n",
      "step 253, loss = 0.0004497166955843568\n",
      "step 254, loss = 0.0002783576783258468\n",
      "step 255, loss = 0.00043083049240522087\n",
      "step 256, loss = 0.00043357606045901775\n",
      "step 257, loss = 0.0005976955289952457\n",
      "step 258, loss = 0.0004087038105353713\n",
      "step 259, loss = 0.00047211500350385904\n",
      "step 260, loss = 0.0003772549389395863\n",
      "step 261, loss = 0.0004308984789531678\n",
      "step 262, loss = 0.00041927953134290874\n",
      "step 263, loss = 0.0004982730024494231\n",
      "step 264, loss = 0.0002836888888850808\n",
      "step 265, loss = 0.0005082755815237761\n",
      "step 266, loss = 0.0004764265613630414\n",
      "step 267, loss = 0.0004035856982227415\n",
      "step 268, loss = 0.0004745862679556012\n",
      "step 269, loss = 0.0003727863368112594\n",
      "step 270, loss = 0.0002971166977658868\n",
      "step 271, loss = 0.0004543922550510615\n",
      "step 272, loss = 0.0003650523431133479\n",
      "step 273, loss = 0.0003572161076590419\n",
      "step 274, loss = 0.00031344263697974384\n",
      "step 275, loss = 0.0002670366957318038\n",
      "step 276, loss = 0.00035553943598642945\n",
      "step 277, loss = 0.0003695493214763701\n",
      "step 278, loss = 0.00036239379551261663\n",
      "step 279, loss = 0.00042618211591616273\n",
      "step 280, loss = 0.00044893077574670315\n",
      "step 281, loss = 0.0003963538329117\n",
      "step 282, loss = 0.000409208529163152\n",
      "step 283, loss = 0.000524858187418431\n",
      "step 284, loss = 0.0003507283690851182\n",
      "step 285, loss = 0.0003722936671692878\n",
      "step 286, loss = 0.0004455645685084164\n",
      "step 287, loss = 0.0002702523779589683\n",
      "step 288, loss = 0.00041225209133699536\n",
      "step 289, loss = 0.00036196495057083666\n",
      "step 290, loss = 0.0003401642316021025\n",
      "step 291, loss = 0.00032284040935337543\n",
      "step 292, loss = 0.00028032436966896057\n",
      "step 293, loss = 0.00038476550253108144\n",
      "step 294, loss = 0.0002916454104706645\n",
      "step 295, loss = 0.00030276080360636115\n",
      "step 296, loss = 0.0003499056037981063\n",
      "step 297, loss = 0.0002678622549865395\n",
      "step 298, loss = 0.00029678241116926074\n",
      "step 299, loss = 0.00031148127163760364\n",
      "step 300, loss = 0.0003016561095137149\n",
      "step 301, loss = 0.00036774002364836633\n",
      "step 302, loss = 0.0005518265534192324\n",
      "step 303, loss = 0.00037771963980048895\n",
      "step 304, loss = 0.00030503678135573864\n",
      "step 305, loss = 0.00029395916499197483\n",
      "step 306, loss = 0.00026357482420280576\n",
      "step 307, loss = 0.00026177606196142733\n",
      "step 308, loss = 0.000312534422846511\n",
      "step 309, loss = 0.0002621915773488581\n",
      "step 310, loss = 0.00021448457846418023\n",
      "step 311, loss = 0.00038670081994496286\n",
      "step 312, loss = 0.000276886741630733\n",
      "step 313, loss = 0.00030218446045182645\n",
      "step 314, loss = 0.00019128383428324014\n",
      "step 315, loss = 0.00025015120627358556\n",
      "step 316, loss = 0.00036391912726685405\n",
      "step 317, loss = 0.0003100282629020512\n",
      "step 318, loss = 0.0002928237954620272\n",
      "step 319, loss = 0.00027478468837216496\n",
      "step 320, loss = 0.00023072335170581937\n",
      "step 321, loss = 0.0003749766619876027\n",
      "step 322, loss = 0.0005554096424020827\n",
      "step 323, loss = 0.00027588382363319397\n",
      "step 324, loss = 0.00036206841468811035\n",
      "step 325, loss = 0.00021710898727178574\n",
      "step 326, loss = 0.00032933324109762907\n",
      "step 327, loss = 0.00036308998824097216\n",
      "step 328, loss = 0.0003008263884112239\n",
      "step 329, loss = 0.00033085528411902487\n",
      "step 330, loss = 0.00027754378970712423\n",
      "step 331, loss = 0.00022306466416921467\n",
      "step 332, loss = 0.00028051738627254963\n",
      "step 333, loss = 0.00023886181588750333\n",
      "step 334, loss = 0.0003025750629603863\n",
      "step 335, loss = 0.00026982257259078324\n",
      "step 336, loss = 0.00025794337852858007\n",
      "step 337, loss = 0.00036788417492061853\n",
      "step 338, loss = 0.00021562090842053294\n",
      "step 339, loss = 0.00024366386060137302\n",
      "step 340, loss = 0.0003010086657013744\n",
      "step 341, loss = 0.000341341074090451\n",
      "step 342, loss = 0.0002613466640468687\n",
      "step 343, loss = 0.00021475095127243549\n",
      "step 344, loss = 0.00031873525585979223\n",
      "step 345, loss = 0.0002561621950007975\n",
      "step 346, loss = 0.000246101466473192\n",
      "step 347, loss = 0.00023933286138344556\n",
      "step 348, loss = 0.0002924022846855223\n",
      "step 349, loss = 0.0003808236215263605\n",
      "step 350, loss = 0.00024801347171887755\n",
      "step 351, loss = 0.00020933734776917845\n",
      "step 352, loss = 0.00017402479716110975\n",
      "step 353, loss = 0.00022619933588430285\n",
      "step 354, loss = 0.00016643245180603117\n",
      "step 355, loss = 0.00015962039469741285\n",
      "step 356, loss = 0.00024501339066773653\n",
      "step 357, loss = 0.0002631314564496279\n",
      "step 358, loss = 0.00021809004829265177\n",
      "step 359, loss = 0.0003168871335219592\n",
      "step 360, loss = 0.0002649580710567534\n",
      "step 361, loss = 0.000257134553976357\n",
      "step 362, loss = 0.0001436759193893522\n",
      "step 363, loss = 0.0002198668516939506\n",
      "step 364, loss = 0.0002474679204169661\n",
      "step 365, loss = 0.0002184581389883533\n",
      "step 366, loss = 0.00024743701214902103\n",
      "step 367, loss = 0.00027969974325969815\n",
      "step 368, loss = 0.0002215852146036923\n",
      "step 369, loss = 0.0002868244773708284\n",
      "step 370, loss = 0.0002569802163634449\n",
      "step 371, loss = 0.00026701093884184957\n",
      "step 372, loss = 0.0002946117310784757\n",
      "step 373, loss = 0.00023119171964935958\n",
      "step 374, loss = 0.00023886087001301348\n",
      "step 375, loss = 0.00028131186263635755\n",
      "step 376, loss = 0.00018359124078415334\n",
      "step 377, loss = 0.00024296954507008195\n",
      "step 378, loss = 0.00022531900322064757\n",
      "step 379, loss = 0.00018432758224662393\n",
      "step 380, loss = 0.00021720555378124118\n",
      "step 381, loss = 0.00014691654359921813\n",
      "step 382, loss = 0.00017814702005125582\n",
      "step 383, loss = 0.00019926513778045774\n",
      "step 384, loss = 0.00022667860321234912\n",
      "step 385, loss = 0.00024488772032782435\n",
      "step 386, loss = 0.000220450630877167\n",
      "step 387, loss = 0.0002475765359122306\n",
      "step 388, loss = 0.00024211850541178137\n",
      "step 389, loss = 0.00023062285617925227\n",
      "step 390, loss = 0.00028042352641932666\n",
      "step 391, loss = 0.00020748807583004236\n",
      "step 392, loss = 0.00023080100072547793\n",
      "step 393, loss = 0.0001687829935690388\n",
      "step 394, loss = 0.0002068663452519104\n",
      "step 395, loss = 0.00019100718782283366\n",
      "step 396, loss = 0.00017208207282237709\n",
      "step 397, loss = 0.0002231918479083106\n",
      "step 398, loss = 0.00019481799972709268\n",
      "step 399, loss = 0.00020174644305370748\n",
      "step 400, loss = 0.00023684978077653795\n",
      "step 401, loss = 0.0002188835060223937\n",
      "step 402, loss = 0.0002202131727244705\n",
      "step 403, loss = 0.0002178766008000821\n",
      "step 404, loss = 0.0002792633604258299\n",
      "step 405, loss = 0.00022689541219733655\n",
      "step 406, loss = 0.00018174103752244264\n",
      "step 407, loss = 0.00024994349223561585\n",
      "step 408, loss = 0.00018795178038999438\n",
      "step 409, loss = 0.00027055692044086754\n",
      "step 410, loss = 0.00013351415691431612\n",
      "step 411, loss = 0.00022065435769036412\n",
      "step 412, loss = 0.0002479009563103318\n",
      "step 413, loss = 0.0002720804768614471\n",
      "step 414, loss = 0.00018618196190800518\n",
      "step 415, loss = 0.00028595843468792737\n",
      "step 416, loss = 0.00022318724950309843\n",
      "step 417, loss = 0.00023513963969890028\n",
      "step 418, loss = 0.00016987444541882724\n",
      "step 419, loss = 0.00014242496399674565\n",
      "step 420, loss = 0.00020800417405553162\n",
      "step 421, loss = 0.0001830537657951936\n",
      "step 422, loss = 0.00016772114031482488\n",
      "step 423, loss = 0.0002511001657694578\n",
      "step 424, loss = 0.0001935783657245338\n",
      "step 425, loss = 0.00022917642490938306\n",
      "step 426, loss = 0.00018560148600954562\n",
      "step 427, loss = 0.00018898236157838255\n",
      "step 428, loss = 0.00021389618632383645\n",
      "step 429, loss = 0.0002031252661254257\n",
      "step 430, loss = 0.0002744956873357296\n",
      "step 431, loss = 0.0002176289854105562\n",
      "step 432, loss = 0.00023040805535856634\n",
      "step 433, loss = 0.00023342955682892352\n",
      "step 434, loss = 0.00017080620455089957\n",
      "step 435, loss = 0.00019783771131187677\n",
      "step 436, loss = 0.00023194070672616363\n",
      "step 437, loss = 0.00018348261073697358\n",
      "step 438, loss = 0.00022351331426762044\n",
      "step 439, loss = 0.0002262995403725654\n",
      "step 440, loss = 0.0001743459579301998\n",
      "step 441, loss = 0.00014716730220243335\n",
      "step 442, loss = 0.00020068342564627528\n",
      "step 443, loss = 0.0002509928017389029\n",
      "step 444, loss = 0.00022071239072829485\n",
      "step 445, loss = 0.00019781537412200123\n",
      "step 446, loss = 0.00024601686163805425\n",
      "step 447, loss = 0.00023356793099083006\n",
      "step 448, loss = 0.0002345873072044924\n",
      "step 449, loss = 0.00022551239817403257\n",
      "step 450, loss = 0.0001525075495010242\n",
      "step 451, loss = 0.00019939594494644552\n",
      "step 452, loss = 0.00017856106569524854\n",
      "step 453, loss = 0.0002087054745061323\n",
      "step 454, loss = 0.0001909671409521252\n",
      "step 455, loss = 0.00021857112005818635\n",
      "step 456, loss = 0.00016454624710604548\n",
      "step 457, loss = 0.00020758420578204095\n",
      "step 458, loss = 0.0002703382633626461\n",
      "step 459, loss = 0.0001851666165748611\n",
      "step 460, loss = 0.00023178590345196426\n",
      "step 461, loss = 0.00026017281925305724\n",
      "step 462, loss = 0.00019560568034648895\n",
      "step 463, loss = 0.0001805076317396015\n",
      "step 464, loss = 0.0001863832731032744\n",
      "step 465, loss = 0.0001559703960083425\n",
      "step 466, loss = 0.0002147608029190451\n",
      "step 467, loss = 0.0001794232812244445\n",
      "step 468, loss = 0.00020799279445782304\n",
      "step 469, loss = 0.00018444449233356863\n",
      "step 470, loss = 0.00019610353047028184\n",
      "step 471, loss = 0.00021541157911997288\n",
      "step 472, loss = 0.0002568926429376006\n",
      "step 473, loss = 0.00012792862253263593\n",
      "step 474, loss = 0.00018037371046375483\n",
      "step 475, loss = 0.00018027449550572783\n",
      "step 476, loss = 0.00019920522754546255\n",
      "step 477, loss = 0.00018764320702757686\n",
      "step 478, loss = 0.00016725173918530345\n",
      "step 479, loss = 0.00017963627760764211\n",
      "step 480, loss = 0.00018466886831447482\n",
      "step 481, loss = 0.00018740829546004534\n",
      "step 482, loss = 0.00015333780902437866\n",
      "step 483, loss = 0.00021516047127079219\n",
      "step 484, loss = 0.0002186965139117092\n",
      "step 485, loss = 0.00020865756960120052\n",
      "step 486, loss = 0.00016760827566031367\n",
      "step 487, loss = 0.0001318565773544833\n",
      "step 488, loss = 0.00015019206330180168\n",
      "step 489, loss = 0.00015690569125581533\n",
      "step 490, loss = 0.0002101358404615894\n",
      "step 491, loss = 0.0002128340129274875\n",
      "step 492, loss = 0.00019631179748103023\n",
      "step 493, loss = 0.00020013522589579225\n",
      "step 494, loss = 0.0002033000928349793\n",
      "step 495, loss = 0.0001359866582788527\n",
      "step 496, loss = 0.0002074547519441694\n",
      "step 497, loss = 0.00013763605966232717\n",
      "step 498, loss = 0.00022916820307727903\n",
      "step 499, loss = 0.00018357776571065187\n",
      "step 500, loss = 0.0001579392555868253\n",
      "step 501, loss = 0.0002006452705245465\n",
      "step 502, loss = 0.00015643250662833452\n",
      "step 503, loss = 0.00012165030784672126\n",
      "step 504, loss = 0.0002509143960196525\n",
      "step 505, loss = 0.00016667242744006217\n",
      "step 506, loss = 0.00013556447811424732\n",
      "step 507, loss = 0.00014510407345369458\n",
      "step 508, loss = 0.00015484439791180193\n",
      "step 509, loss = 0.0002945844316855073\n",
      "step 510, loss = 0.00013549897994380444\n",
      "step 511, loss = 0.00020344067888800055\n",
      "step 512, loss = 0.00019936598255299032\n",
      "step 513, loss = 0.00016022766067180783\n",
      "step 514, loss = 0.00015924185572657734\n",
      "step 515, loss = 0.00016823664191178977\n",
      "step 516, loss = 0.00018586096120998263\n",
      "step 517, loss = 0.00016175871132873\n",
      "step 518, loss = 0.00017342930368613452\n",
      "step 519, loss = 0.00014155574899632484\n",
      "step 520, loss = 0.000174707718542777\n",
      "step 521, loss = 0.00016617728397250175\n",
      "step 522, loss = 0.00022758795239496976\n",
      "step 523, loss = 0.00015079643344506621\n",
      "step 524, loss = 0.0001922711671795696\n",
      "step 525, loss = 0.00017755247245077044\n",
      "step 526, loss = 0.00020703228074125946\n",
      "step 527, loss = 0.00015623620129190385\n",
      "step 528, loss = 0.0001700245775282383\n",
      "step 529, loss = 0.00016532413428649306\n",
      "step 530, loss = 0.0001950400765053928\n",
      "step 531, loss = 0.0001983491820283234\n",
      "step 532, loss = 0.00021255481988191605\n",
      "step 533, loss = 0.0001973276084754616\n",
      "step 534, loss = 0.000178695801878348\n",
      "step 535, loss = 0.0001496169134043157\n",
      "step 536, loss = 0.00012621108908206224\n",
      "step 537, loss = 0.00017510089674033225\n",
      "step 538, loss = 0.00016042569768615067\n",
      "step 539, loss = 0.00018648449622560292\n",
      "step 540, loss = 0.00016922721988521516\n",
      "step 541, loss = 0.00021484267199411988\n",
      "step 542, loss = 0.00015621809870935977\n",
      "step 543, loss = 0.0001685337338130921\n",
      "step 544, loss = 0.00016064914234448224\n",
      "step 545, loss = 0.0001514570030849427\n",
      "step 546, loss = 0.00014874746557325125\n",
      "step 547, loss = 0.0001584802521392703\n",
      "step 548, loss = 0.00011475041537778452\n",
      "step 549, loss = 0.000139288924401626\n",
      "step 550, loss = 0.00020812911679968238\n",
      "step 551, loss = 0.00013929932902101427\n",
      "step 552, loss = 0.00013338019198272377\n",
      "step 553, loss = 0.000168659258633852\n",
      "step 554, loss = 0.0001677202235441655\n",
      "step 555, loss = 0.00014729599934071302\n",
      "step 556, loss = 0.00014086002192925662\n",
      "step 557, loss = 0.00015337162767536938\n",
      "step 558, loss = 0.00019428221276029944\n",
      "step 559, loss = 0.00015273083408828825\n",
      "step 560, loss = 0.000151443513459526\n",
      "step 561, loss = 0.00019116754992865026\n",
      "step 562, loss = 0.00016622472321614623\n",
      "step 563, loss = 0.00019849621457979083\n",
      "step 564, loss = 0.0001935674372361973\n",
      "step 565, loss = 0.00015305110719054937\n",
      "step 566, loss = 0.00011574656673474237\n",
      "step 567, loss = 0.0001107036296161823\n",
      "step 568, loss = 0.00015564919158350676\n",
      "step 569, loss = 0.00016677916573826224\n",
      "step 570, loss = 0.00022634452034253627\n",
      "step 571, loss = 0.00011710580292856321\n",
      "step 572, loss = 0.00023009061987977475\n",
      "step 573, loss = 0.00018444343004375696\n",
      "step 574, loss = 0.00017946604930330068\n",
      "step 575, loss = 0.00016024101932998747\n",
      "step 576, loss = 0.00014403437671717256\n",
      "step 577, loss = 0.00019869834068231285\n",
      "step 578, loss = 0.00021668292174581438\n",
      "step 579, loss = 0.0001913152082124725\n",
      "step 580, loss = 0.00023223564494401217\n",
      "step 581, loss = 0.00019193560001440346\n",
      "step 582, loss = 0.00014494014612864703\n",
      "step 583, loss = 0.0001095674087991938\n",
      "step 584, loss = 0.00024719125940464437\n",
      "step 585, loss = 0.0001724468165775761\n",
      "step 586, loss = 0.00016133593453560024\n",
      "step 587, loss = 0.00014414983161259443\n",
      "step 588, loss = 0.0001470421557314694\n",
      "step 589, loss = 0.0001364188501611352\n",
      "step 590, loss = 0.00013428638339973986\n",
      "step 591, loss = 0.00017797661712393165\n",
      "step 592, loss = 0.00011638188880169764\n",
      "step 593, loss = 0.0001652738865232095\n",
      "step 594, loss = 0.00016893012798391283\n",
      "step 595, loss = 0.0002047144662356004\n",
      "step 596, loss = 0.0002087685134029016\n",
      "step 597, loss = 0.00014961145643610507\n",
      "step 598, loss = 0.0001335873967036605\n",
      "step 599, loss = 0.00014270597603172064\n",
      "step 600, loss = 0.00020250148372724652\n",
      "step 601, loss = 0.00015134135901462287\n",
      "step 602, loss = 0.00021151022519916296\n",
      "step 603, loss = 0.0002235107240267098\n",
      "step 604, loss = 0.0001579258096171543\n",
      "step 605, loss = 0.00014732187264598906\n",
      "step 606, loss = 0.00018439577252138406\n",
      "step 607, loss = 0.00017640963778831065\n",
      "step 608, loss = 0.000125550024677068\n",
      "step 609, loss = 0.00015497485583182424\n",
      "step 610, loss = 0.00012315958156250417\n",
      "step 611, loss = 0.00023634459648746997\n",
      "step 612, loss = 0.00014055184146855026\n",
      "step 613, loss = 0.00014522502897307277\n",
      "step 614, loss = 0.0002215960412286222\n",
      "step 615, loss = 0.00014292271225713193\n",
      "step 616, loss = 0.00015642073412891477\n",
      "step 617, loss = 0.00015173324209172279\n",
      "step 618, loss = 0.0001470107090426609\n",
      "step 619, loss = 0.0001906401157611981\n",
      "step 620, loss = 0.00020547131134662777\n",
      "step 621, loss = 0.00014445712440647185\n",
      "step 622, loss = 0.00019265545415692031\n",
      "step 623, loss = 0.00017526601732242852\n",
      "step 624, loss = 0.0002136640396201983\n",
      "step 625, loss = 0.0001870087580755353\n",
      "step 626, loss = 0.00019130563305225223\n",
      "step 627, loss = 0.00017984714941121638\n",
      "step 628, loss = 0.00017384253442287445\n",
      "step 629, loss = 0.0001477932237321511\n",
      "step 630, loss = 0.00015240507491398603\n",
      "step 631, loss = 0.00017333996947854757\n",
      "step 632, loss = 0.0001221525453729555\n",
      "step 633, loss = 0.0001441982458345592\n",
      "step 634, loss = 0.00020193967793602496\n",
      "step 635, loss = 0.00017550482880324125\n",
      "step 636, loss = 0.0001508051936980337\n",
      "step 637, loss = 0.00013201967522036284\n",
      "step 638, loss = 0.00018001842545345426\n",
      "step 639, loss = 0.00014915339124854654\n",
      "step 640, loss = 0.00013632129412144423\n",
      "step 641, loss = 0.00015801037079654634\n",
      "step 642, loss = 0.00016224445425905287\n",
      "step 643, loss = 0.00014581746654585004\n",
      "step 644, loss = 0.0001454709708923474\n",
      "step 645, loss = 0.0002178629656555131\n",
      "step 646, loss = 0.00015499931760132313\n",
      "step 647, loss = 0.00019450510444585234\n",
      "step 648, loss = 0.00015681009972468019\n",
      "step 649, loss = 0.00015731542953290045\n",
      "step 650, loss = 0.00014629014185629785\n",
      "step 651, loss = 0.00014160838327370584\n",
      "step 652, loss = 0.00014286319492384791\n",
      "step 653, loss = 0.00014333095168694854\n",
      "step 654, loss = 0.0001760353334248066\n",
      "step 655, loss = 0.00013766330084763467\n",
      "step 656, loss = 0.00017089258471969515\n",
      "step 657, loss = 0.00016360431618522853\n",
      "step 658, loss = 0.00013006923836655915\n",
      "step 659, loss = 0.00014891367754898965\n",
      "step 660, loss = 0.00016895477892830968\n",
      "step 661, loss = 0.00017472381296101958\n",
      "step 662, loss = 0.00015312038885895163\n",
      "step 663, loss = 0.00016509993292856961\n",
      "step 664, loss = 9.366516314912587e-05\n",
      "step 665, loss = 0.00012817280367016792\n",
      "step 666, loss = 0.0001445028610760346\n",
      "step 667, loss = 0.0001440708147129044\n",
      "step 668, loss = 0.00015980697935447097\n",
      "step 669, loss = 0.0001481518556829542\n",
      "step 670, loss = 0.00013700641284231097\n",
      "step 671, loss = 0.0001953695755219087\n",
      "step 672, loss = 0.00019485749362502247\n",
      "step 673, loss = 0.00013028373359702528\n",
      "step 674, loss = 0.00021531112724915147\n",
      "step 675, loss = 0.00014080155233386904\n",
      "step 676, loss = 0.000199839923880063\n",
      "step 677, loss = 0.00017768971156328917\n",
      "step 678, loss = 0.00018070463556796312\n",
      "step 679, loss = 0.00013208738528192043\n",
      "step 680, loss = 0.00015357272059191018\n",
      "step 681, loss = 0.0001664788433117792\n",
      "step 682, loss = 0.00019492729916237295\n",
      "step 683, loss = 0.00014132818614598364\n",
      "step 684, loss = 0.00017878979269880801\n",
      "step 685, loss = 0.00020127376774325967\n",
      "step 686, loss = 0.0001493736926931888\n",
      "step 687, loss = 0.00014843966346234083\n",
      "step 688, loss = 0.0001628988393349573\n",
      "step 689, loss = 0.0001491871807957068\n",
      "step 690, loss = 0.00014344509690999985\n",
      "step 691, loss = 0.00013195689825806767\n",
      "step 692, loss = 0.00016505051462445408\n",
      "step 693, loss = 0.00011635138071142137\n",
      "step 694, loss = 0.00017750932602211833\n",
      "step 695, loss = 0.00014663780166301876\n",
      "step 696, loss = 0.00015191170678008348\n",
      "step 697, loss = 0.000283308036159724\n",
      "step 698, loss = 0.00019940908532589674\n",
      "step 699, loss = 0.00015442416770383716\n",
      "step 700, loss = 0.00016180895909201354\n",
      "step 701, loss = 0.00018783406994771212\n",
      "step 702, loss = 0.00012608680117409676\n",
      "step 703, loss = 0.00015726228593848646\n",
      "step 704, loss = 0.00017490283062215894\n",
      "step 705, loss = 0.00017267234215978533\n",
      "step 706, loss = 0.00016780821897555143\n",
      "step 707, loss = 0.00013014941941946745\n",
      "step 708, loss = 0.0001764803600963205\n",
      "step 709, loss = 0.00014213938266038895\n",
      "step 710, loss = 0.00012014647654723376\n",
      "step 711, loss = 0.00016797086573205888\n",
      "step 712, loss = 0.00017709904932416975\n",
      "step 713, loss = 0.00016940393834374845\n",
      "step 714, loss = 0.0001496790791861713\n",
      "step 715, loss = 0.0001879515330074355\n",
      "step 716, loss = 0.00018677055777516216\n",
      "step 717, loss = 0.00015040667494758964\n",
      "step 718, loss = 0.00017057647346518934\n",
      "step 719, loss = 0.00015238844207488\n",
      "step 720, loss = 0.00019268490723334253\n",
      "step 721, loss = 0.00014382203517016023\n",
      "step 722, loss = 0.00016711390344426036\n",
      "step 723, loss = 0.00015753842308185995\n",
      "step 724, loss = 0.00014559438568539917\n",
      "step 725, loss = 0.00013149828009773046\n",
      "step 726, loss = 0.00017243472393602133\n",
      "step 727, loss = 0.00012147222878411412\n",
      "step 728, loss = 0.00010498549818294123\n",
      "step 729, loss = 0.00020681667956523597\n",
      "step 730, loss = 0.0001647181634325534\n",
      "step 731, loss = 0.0001735226105665788\n",
      "step 732, loss = 0.00015220479690469801\n",
      "step 733, loss = 0.00012597399472724646\n",
      "step 734, loss = 0.00015714691835455596\n",
      "step 735, loss = 0.00016472068091388792\n",
      "step 736, loss = 0.00014097517123445868\n",
      "step 737, loss = 0.00021781938266940415\n",
      "step 738, loss = 0.0001946160919032991\n",
      "step 739, loss = 0.00014989907504059374\n",
      "step 740, loss = 0.00016420533938799053\n",
      "step 741, loss = 0.00021149935491848737\n",
      "step 742, loss = 0.0002185862831538543\n",
      "step 743, loss = 7.58116802899167e-05\n",
      "step 744, loss = 0.00018662774527911097\n",
      "step 745, loss = 0.00012318379594944417\n",
      "step 746, loss = 0.00011867297871503979\n",
      "step 747, loss = 0.0001564456324558705\n",
      "step 748, loss = 0.00014105596346780658\n",
      "step 749, loss = 0.00015298563812393695\n",
      "step 750, loss = 0.00015765902935527265\n",
      "step 751, loss = 0.00019884435459971428\n",
      "step 752, loss = 0.0001856298913480714\n",
      "step 753, loss = 0.00016168497677426785\n",
      "step 754, loss = 0.00014625942276325077\n",
      "step 755, loss = 0.00013727147597819567\n",
      "step 756, loss = 0.00016480599879287183\n",
      "step 757, loss = 0.00017114482761826366\n",
      "step 758, loss = 0.00021628216200042516\n",
      "step 759, loss = 0.00019875881844200194\n",
      "step 760, loss = 0.00013993436004966497\n",
      "step 761, loss = 0.00013222434790804982\n",
      "step 762, loss = 0.00013712285726796836\n",
      "step 763, loss = 0.00023187245824374259\n",
      "step 764, loss = 0.00015785785217303783\n",
      "step 765, loss = 0.00010874785220948979\n",
      "step 766, loss = 0.0001968230353668332\n",
      "step 767, loss = 0.0001894797751447186\n",
      "step 768, loss = 0.00012785520812030882\n",
      "step 769, loss = 0.00011719000758603215\n",
      "step 770, loss = 0.00018312982865609229\n",
      "step 771, loss = 0.0001560493983561173\n",
      "step 772, loss = 0.00013675093941856176\n",
      "step 773, loss = 0.00014014261250849813\n",
      "step 774, loss = 0.00016438847524113953\n",
      "step 775, loss = 9.90772241493687e-05\n",
      "step 776, loss = 0.00014783337246626616\n",
      "step 777, loss = 0.00017270477837882936\n",
      "step 778, loss = 0.00022865564096719027\n",
      "step 779, loss = 0.0001611247134860605\n",
      "step 780, loss = 0.0001485702523496002\n",
      "step 781, loss = 0.0001394512364640832\n",
      "step 782, loss = 0.0001423074718331918\n",
      "step 783, loss = 0.00021304230904206634\n",
      "step 784, loss = 0.0001740723819239065\n",
      "step 785, loss = 0.0001146236463682726\n",
      "step 786, loss = 0.0001856680028140545\n",
      "step 787, loss = 0.0001804819912649691\n",
      "step 788, loss = 0.000166415236890316\n",
      "step 789, loss = 0.00014493742492049932\n",
      "step 790, loss = 0.0001037261463352479\n",
      "step 791, loss = 0.00014274388377089053\n",
      "step 792, loss = 0.00011623313912423328\n",
      "step 793, loss = 0.00013998169742990285\n",
      "step 794, loss = 0.00014645743067376316\n",
      "step 795, loss = 0.00018067171913571656\n",
      "step 796, loss = 0.0001608444144949317\n",
      "step 797, loss = 0.00013294717064127326\n",
      "step 798, loss = 0.00016140550724230707\n",
      "step 799, loss = 0.00015641935169696808\n",
      "step 800, loss = 0.0001664346782490611\n",
      "step 801, loss = 0.00013688125181943178\n",
      "step 802, loss = 0.00011190657824045047\n",
      "step 803, loss = 0.00015251444710884243\n",
      "step 804, loss = 0.00013776587729807943\n",
      "step 805, loss = 0.00012237587361596525\n",
      "step 806, loss = 0.00013432746345642954\n",
      "step 807, loss = 0.0001060960566974245\n",
      "step 808, loss = 0.00011687665391946211\n",
      "step 809, loss = 0.00010821608157129958\n",
      "step 810, loss = 8.960885315900669e-05\n",
      "step 811, loss = 0.00015354478091467172\n",
      "step 812, loss = 0.0001387379743391648\n",
      "step 813, loss = 0.00012171259004389867\n",
      "step 814, loss = 0.00013225791917648166\n",
      "step 815, loss = 0.00014980680134613067\n",
      "step 816, loss = 0.00016985480033326894\n",
      "step 817, loss = 0.00011724149953806773\n",
      "step 818, loss = 6.92214525770396e-05\n",
      "step 819, loss = 0.00016459343896713108\n",
      "step 820, loss = 0.00024015251256059855\n",
      "step 821, loss = 0.00012712548777926713\n",
      "step 822, loss = 0.00017071327602025121\n",
      "step 823, loss = 0.00011361087672412395\n",
      "step 824, loss = 0.00015045290638227016\n",
      "step 825, loss = 0.0001235124800587073\n",
      "step 826, loss = 0.00014565895253326744\n",
      "step 827, loss = 0.00014892050239723176\n",
      "step 828, loss = 0.00012474153481889516\n",
      "step 829, loss = 0.00013231186312623322\n",
      "step 830, loss = 0.0001704802125459537\n",
      "step 831, loss = 0.0001520474615972489\n",
      "step 832, loss = 0.00013482860231306404\n",
      "step 833, loss = 0.00017504555580671877\n",
      "step 834, loss = 0.00014056770305614918\n",
      "step 835, loss = 8.935909863794222e-05\n",
      "step 836, loss = 0.00015592128329444677\n",
      "step 837, loss = 0.00012393017823342234\n",
      "step 838, loss = 0.0001384498755214736\n",
      "step 839, loss = 0.0001422735513187945\n",
      "step 840, loss = 0.00016984628746286035\n",
      "step 841, loss = 0.0001668194745434448\n",
      "step 842, loss = 0.00013672272325493395\n",
      "step 843, loss = 0.0001551089808344841\n",
      "step 844, loss = 0.00012072091340087354\n",
      "step 845, loss = 0.0001328056096099317\n",
      "step 846, loss = 0.00018179701874032617\n",
      "step 847, loss = 0.0001662295253481716\n",
      "step 848, loss = 0.00016158590733539313\n",
      "step 849, loss = 0.00018557216390036047\n",
      "step 850, loss = 0.00011133655061712489\n",
      "step 851, loss = 0.0001559332595206797\n",
      "step 852, loss = 0.00012969136878382415\n",
      "step 853, loss = 0.00012437801342457533\n",
      "step 854, loss = 0.0002166099293390289\n",
      "step 855, loss = 0.0001550531160319224\n",
      "step 856, loss = 0.0002061331324512139\n",
      "step 857, loss = 0.00016305137251038104\n",
      "step 858, loss = 0.0001242434373125434\n",
      "step 859, loss = 0.0001234724186360836\n",
      "step 860, loss = 0.00015141995390877128\n",
      "step 861, loss = 0.00011791580618591979\n",
      "step 862, loss = 0.0001340240560239181\n",
      "step 863, loss = 0.00012711047020275146\n",
      "step 864, loss = 0.0001455349993193522\n",
      "step 865, loss = 8.61147855175659e-05\n",
      "step 866, loss = 0.00015403173165395856\n",
      "step 867, loss = 0.0001472991134505719\n",
      "step 868, loss = 0.0001409889227943495\n",
      "step 869, loss = 0.00019041144696529955\n",
      "step 870, loss = 0.00013392635446507484\n",
      "step 871, loss = 0.00015600472397636622\n",
      "step 872, loss = 0.00011796081525972113\n",
      "step 873, loss = 0.00014242416364140809\n",
      "step 874, loss = 0.00011599475692491978\n",
      "step 875, loss = 0.00015852003707550466\n",
      "step 876, loss = 8.81979358382523e-05\n",
      "step 877, loss = 0.00010677690443117172\n",
      "step 878, loss = 0.00011402183008613065\n",
      "step 879, loss = 0.00012601683556567878\n",
      "step 880, loss = 0.00013499506167136133\n",
      "step 881, loss = 0.0001224959851242602\n",
      "step 882, loss = 0.00013926047540735453\n",
      "step 883, loss = 0.00017369043780490756\n",
      "step 884, loss = 0.0001532488822704181\n",
      "step 885, loss = 0.00012490341032389551\n",
      "step 886, loss = 0.00015427180915139616\n",
      "step 887, loss = 0.00016223006241489202\n",
      "step 888, loss = 0.0001414704747730866\n",
      "step 889, loss = 0.0001691165380179882\n",
      "step 890, loss = 0.00016721243446227163\n",
      "step 891, loss = 0.00017993059009313583\n",
      "step 892, loss = 0.0001085189578589052\n",
      "step 893, loss = 0.00016195530770346522\n",
      "step 894, loss = 0.00013735087122768164\n",
      "step 895, loss = 0.00014173031377140433\n",
      "step 896, loss = 0.0001579810632392764\n",
      "step 897, loss = 0.00015750261081848294\n",
      "step 898, loss = 0.0001353803090751171\n",
      "step 899, loss = 0.00012077923020115122\n",
      "step 900, loss = 0.00015917085693217814\n",
      "step 901, loss = 0.00017407711129635572\n",
      "step 902, loss = 0.00015359009557869285\n",
      "step 903, loss = 0.00014572787040378898\n",
      "step 904, loss = 0.00015010735660325736\n",
      "step 905, loss = 0.0001475250319344923\n",
      "step 906, loss = 0.00012152684212196618\n",
      "step 907, loss = 0.00012838288967031986\n",
      "step 908, loss = 0.00019528184202499688\n",
      "step 909, loss = 0.00010320793080609292\n",
      "step 910, loss = 0.00013275996025186032\n",
      "step 911, loss = 9.323240374214947e-05\n",
      "step 912, loss = 0.00016650956240482628\n",
      "step 913, loss = 0.0001542606041766703\n",
      "step 914, loss = 0.0001510726724518463\n",
      "step 915, loss = 0.00010480058699613437\n",
      "step 916, loss = 0.00014626143092755228\n",
      "step 917, loss = 0.00013348800712265074\n",
      "step 918, loss = 0.0001720152940833941\n",
      "step 919, loss = 0.0001775436248863116\n",
      "step 920, loss = 0.00015045468171592802\n",
      "step 921, loss = 0.00012531891115941107\n",
      "step 922, loss = 0.00011811462900368497\n",
      "step 923, loss = 9.711874008644372e-05\n",
      "step 924, loss = 0.00014294982247520238\n",
      "step 925, loss = 0.0001257324474863708\n",
      "step 926, loss = 0.00010601121903164312\n",
      "step 927, loss = 0.00012625227100215852\n",
      "step 928, loss = 0.00012332870392128825\n",
      "step 929, loss = 0.00015165359945967793\n",
      "step 930, loss = 0.00011777100735343993\n",
      "step 931, loss = 0.00013919694174546748\n",
      "step 932, loss = 0.00017356419994030148\n",
      "step 933, loss = 9.856519318418577e-05\n",
      "step 934, loss = 0.00012862746370956302\n",
      "step 935, loss = 0.00018404758884571493\n",
      "step 936, loss = 0.00013331860827747732\n",
      "step 937, loss = 0.00013627539738081396\n",
      "step 938, loss = 0.0001483019586885348\n",
      "step 939, loss = 0.000155270317918621\n",
      "step 940, loss = 0.00014790939167141914\n",
      "step 941, loss = 0.0001771358074620366\n",
      "step 942, loss = 0.00013570667942985892\n",
      "step 943, loss = 9.952318941941485e-05\n",
      "step 944, loss = 0.00014365762763191015\n",
      "step 945, loss = 0.00012614281149581075\n",
      "step 946, loss = 0.0001643630093894899\n",
      "step 947, loss = 0.00017741965712048113\n",
      "step 948, loss = 0.00012852709915023297\n",
      "step 949, loss = 0.00017642730381339788\n",
      "step 950, loss = 0.00011039535456802696\n",
      "step 951, loss = 0.00016324911848641932\n",
      "step 952, loss = 0.00010709232446970418\n",
      "step 953, loss = 0.00013013402349315584\n",
      "step 954, loss = 0.00010236581147182733\n",
      "step 955, loss = 0.0001414701691828668\n",
      "step 956, loss = 0.00018114740669261664\n",
      "step 957, loss = 0.00013519356434699148\n",
      "step 958, loss = 0.0001522676757303998\n",
      "step 959, loss = 0.00011798852210631594\n",
      "step 960, loss = 0.00013515491446014494\n",
      "step 961, loss = 0.00012709734437521547\n",
      "step 962, loss = 0.0001284029131056741\n",
      "step 963, loss = 0.00017075460345949978\n",
      "step 964, loss = 0.00016355888510588557\n",
      "step 965, loss = 0.00015511926903855056\n",
      "step 966, loss = 0.00012189998233225197\n",
      "step 967, loss = 0.00015712597814854234\n",
      "step 968, loss = 0.00013303237210493535\n",
      "step 969, loss = 0.0001405107177561149\n",
      "step 970, loss = 0.0001241485442733392\n",
      "step 971, loss = 9.012781083583832e-05\n",
      "step 972, loss = 0.00013813651457894593\n",
      "step 973, loss = 0.00018405435548629612\n",
      "step 974, loss = 0.0001414119906257838\n",
      "step 975, loss = 0.00014852873573545367\n",
      "step 976, loss = 0.00011498723324621096\n",
      "step 977, loss = 0.00013127457350492477\n",
      "step 978, loss = 0.00016564228280913085\n",
      "step 979, loss = 0.000172293686773628\n",
      "step 980, loss = 0.00015438727859873325\n",
      "step 981, loss = 0.00011457530490588397\n",
      "step 982, loss = 0.0002020418323809281\n",
      "step 983, loss = 0.00012267185957171023\n",
      "step 984, loss = 0.00013394109555520117\n",
      "step 985, loss = 0.00011851388990180567\n",
      "step 986, loss = 0.00016049880650825799\n",
      "step 987, loss = 0.0001253748341696337\n",
      "step 988, loss = 0.00017383605882059783\n",
      "step 989, loss = 0.00012774211063515395\n",
      "step 990, loss = 0.0001459540508221835\n",
      "step 991, loss = 0.00014861348608974367\n",
      "step 992, loss = 0.0001486400724388659\n",
      "step 993, loss = 0.00016057537868618965\n",
      "step 994, loss = 0.00014145931345410645\n",
      "step 995, loss = 0.00016880572366062552\n",
      "step 996, loss = 0.00013353367103263736\n",
      "step 997, loss = 0.00015912191884126514\n",
      "step 998, loss = 0.00011786262621171772\n",
      "step 999, loss = 0.0001374225103063509\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "n = 1000\n",
    "\n",
    "model = torchModel()\n",
    "optim = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "loss = torch.nn.MSELoss()\n",
    "begin = time.time()\n",
    "\n",
    "for i,(x,y) in enumerate(get_torch_data(n)):\n",
    "    optim.zero_grad()\n",
    "    pred_y = model(x)\n",
    "    batch_loss = loss(y, pred_y)\n",
    "    batch_loss.backward()\n",
    "    optim.step()\n",
    "    print(f\"step {i}, loss = {batch_loss.item()}\")\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8444056510925293 s has passed.\n"
     ]
    }
   ],
   "source": [
    "print(end - begin, \"s has passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 1, 0.01).reshape(-1, 1)\n",
    "x = x.astype('float32')\n",
    "y = x ** 2\n",
    "pred_y = model(torch.from_numpy(x)).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, 'o', color='orange')\n",
    "plt.plot(x, pred_y, '*', color='violet')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
