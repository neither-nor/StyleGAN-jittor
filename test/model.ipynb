{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:36.421676 00 compiler.py:847] Jittor(1.2.2.13) src: /home/user3/.local/lib/python3.7/site-packages/jittor\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:36.424304 00 compiler.py:848] cache_path: /home/user3/.cache/jittor/default/g++\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:36.438858 00 __init__.py:250] Found /usr/local/cuda/bin/nvcc(10.2.89) at /usr/local/cuda/bin/nvcc.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:36.525457 00 __init__.py:250] Found gdb(8.1.0) at /usr/bin/gdb.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:36.542424 00 __init__.py:250] Found addr2line(2.30) at /usr/bin/addr2line.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:36.589207 00 compiler.py:897] pybind_include: -I/usr/include/python3.7m -I/usr/local/lib/python3.7/dist-packages/pybind11/include\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i 0104 22:44:36.410096 48 __init__.py:250] Found g++(7.5.0) at /usr/bin/g++.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:36.613054 00 compiler.py:899] extension_suffix: .cpython-37m-x86_64-linux-gnu.so\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:36.822905 00 __init__.py:169] Total mem: 62.79GB, using 16 procs for compiling.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:36.985427 00 jit_compiler.cc:21] Load cc_path: /usr/bin/g++\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:37.224330 00 init.cc:52] Found cuda archs: [75,]\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:37.393473 00 __init__.py:250] Found mpicc(2.1.1) at /usr/bin/mpicc.\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:37.448424 00 compiler.py:654] handle pyjt_include/home/user3/.local/lib/python3.7/site-packages/jittor/extern/mpi/inc/mpi_warper.h\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:37.504541 00 compile_extern.py:275] Downloading nccl...\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:37.586432 00 compile_extern.py:16] found /usr/local/cuda/include/cublas.h\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:37.589258 00 compile_extern.py:16] found /usr/lib/x86_64-linux-gnu/libcublas.so\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:38.612463 00 compile_extern.py:16] found /usr/local/cuda/include/cudnn.h\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:38.615407 00 compile_extern.py:16] found /usr/local/cuda/lib64/libcudnn.so\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:38.630338 00 compiler.py:654] handle pyjt_include/home/user3/.local/lib/python3.7/site-packages/jittor/extern/cuda/cudnn/inc/cudnn_warper.h\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:42.952192 00 compile_extern.py:16] found /usr/local/cuda/include/curand.h\u001b[m\n",
      "[SYNC]\u001b[38;5;2m[i 0104 22:44:42.955134 00 compile_extern.py:16] found /usr/local/cuda/lib64/libcurand.so\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import jittor as jt\n",
    "import numpy as np\n",
    "import random\n",
    "from math import sqrt\n",
    "#jt.flags.use_cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equal Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualLR:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def compute_weight(self, module):\n",
    "        weight = getattr(module, self.name + '_orig')\n",
    "        fan_in = weight[0].numel()\n",
    "\n",
    "        return weight * sqrt(2 / fan_in)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply(module, name):\n",
    "        fn = EqualLR(name)\n",
    "\n",
    "        weight = getattr(module, name)\n",
    "        delattr(module, name)\n",
    "        setattr(module, name + '_orig', weight)\n",
    "        module.register_pre_forward_hook(fn)\n",
    "\n",
    "        return fn\n",
    "\n",
    "    def __call__(self, module, input):\n",
    "        weight = self.compute_weight(module)\n",
    "        setattr(module, self.name, weight)\n",
    "\n",
    "\n",
    "def equal_lr(module, name='weight'):\n",
    "    EqualLR.apply(module, name)\n",
    "\n",
    "    return module\n",
    "\n",
    "class EqualConv2d(jt.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        conv = jt.nn.Conv2d(*args, **kwargs)\n",
    "        jt.init.gauss_(conv.weight, 0, 1)\n",
    "        jt.init.constant_(conv.bias,0)\n",
    "        \n",
    "        self.conv = equal_lr(conv)\n",
    "\n",
    "    def execute(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class EqualLinear(jt.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        linear = jt.nn.Linear(in_dim, out_dim)\n",
    "        jt.init.gauss_(linear.weight, 0, 1)\n",
    "        jt.init.constant_(linear.bias, 0)\n",
    "\n",
    "        self.linear = equal_lr(linear)\n",
    "\n",
    "    def execute(self, input):\n",
    "        return self.linear(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurFunctionBackward(jt.Function):\n",
    "    def execute(self, grad_output, kernel, kernel_flip):\n",
    "        self.saved_tensors = kernel, kernel_flip\n",
    "\n",
    "        grad_input = jt.nn.conv2d(\n",
    "            grad_output, kernel_flip, padding=1, groups=grad_output.shape[1]\n",
    "        )\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def grad(self, gradgrad_output):\n",
    "        kernel, kernel_flip = self.saved_tensors\n",
    "\n",
    "        grad_input = jt.nn.conv2d(\n",
    "            gradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1]\n",
    "        )\n",
    "\n",
    "        return grad_input, None, None\n",
    "\n",
    "class BlurFunction(jt.Function):\n",
    "    def execute(self, input, kernel, kernel_flip):\n",
    "        self.saved_tensors = kernel, kernel_flip\n",
    "\n",
    "        output = jt.nn.conv2d(input, kernel, padding=1, groups=input.shape[1])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def grad(self, grad_output):\n",
    "        kernel, kernel_flip = self.saved_tensors\n",
    "\n",
    "        grad_input = BlurFunctionBackward().execute(grad_output, kernel, kernel_flip)\n",
    "\n",
    "        return grad_input, None, None\n",
    "\n",
    "blur = BlurFunction().apply\n",
    "\n",
    "class Blur(jt.Module):\n",
    "    def __init__(self, channel):\n",
    "        weight = jt.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype='float32')\n",
    "        weight = weight.reshape(1, 1, 3, 3)\n",
    "        weight = weight / weight.sum()\n",
    "        weight_flip = jt.flip(weight, [2, 3])\n",
    "\n",
    "        self._weight = weight.repeat(channel, 1, 1, 1)\n",
    "        self._weight_flip = weight_flip.repeat(channel, 1, 1, 1)\n",
    "\n",
    "    def execute(self, input):\n",
    "        return blur(input, self._weight, self._weight_flip)\n",
    "        # return jt.nn.conv2d(input, self.weight, padding=1, groups=input.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedDownsample(jt.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, padding=0):\n",
    "        self.weight = jt.randn(out_channel, in_channel, kernel_size, kernel_size)\n",
    "        self.bias = jt.zeros(out_channel)\n",
    "\n",
    "        fan_in = in_channel * kernel_size * kernel_size\n",
    "        self.multiplier = sqrt(2 / fan_in)\n",
    "        self.pad = padding\n",
    "\n",
    "    def execute(self, input):\n",
    "        weight = jt.nn.pad(self.weight * self.multiplier, [1, 1, 1, 1])\n",
    "        weight = (\n",
    "            weight[:, :, 1:, 1:]  +\n",
    "            weight[:, :, :-1, 1:] +\n",
    "            weight[:, :, 1:, :-1] +\n",
    "            weight[:, :, :-1, :-1]\n",
    "        ) / 4\n",
    "\n",
    "        out = jt.nn.conv2d(input, weight, self.bias, stride=2, padding=self.pad)\n",
    "        return out\n",
    "\n",
    "class FusedUpsample(jt.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, padding=0):\n",
    "        self.weight = jt.randn(in_channel, out_channel, kernel_size, kernel_size)\n",
    "        self.bias = jt.zeros(out_channel)\n",
    "\n",
    "        fan_in = in_channel * kernel_size * kernel_size\n",
    "        self.multiplier = sqrt(2 / fan_in)\n",
    "        self.pad = padding\n",
    "\n",
    "    def execute(self, input):\n",
    "        weight = jt.nn.pad(self.weight * self.multiplier, [1, 1, 1, 1])\n",
    "        weight = (\n",
    "            weight[:, :, 1:, 1:] +\n",
    "            weight[:, :, :-1, 1:] +\n",
    "            weight[:, :, 1:, :-1] +\n",
    "            weight[:, :, :-1, :-1]\n",
    "        ) / 4\n",
    "\n",
    "        out = jt.nn.conv_transpose2d(input, weight, self.bias, stride=2, padding=self.pad)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(jt.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        kernel_size,\n",
    "        padding,\n",
    "        kernel_size2=None,\n",
    "        padding2=None,\n",
    "        downsample=False,\n",
    "        fused=False\n",
    "    ):\n",
    "        pad1 = padding\n",
    "        pad2 = padding\n",
    "        if padding2 is not None:\n",
    "            pad2 = padding2\n",
    "\n",
    "        kernel1 = kernel_size\n",
    "        kernel2 = kernel_size\n",
    "        if kernel_size2 is not None:\n",
    "            kernel2 = kernel_size2\n",
    "\n",
    "        self.conv1 = jt.nn.Sequential(\n",
    "            EqualConv2d(in_channel, out_channel, kernel1, padding=pad1),\n",
    "            jt.nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        if downsample:\n",
    "            if fused:\n",
    "                self.conv2 = jt.nn.Sequential(\n",
    "                    Blur(out_channel),\n",
    "                    FusedDownsample(out_channel, out_channel, kernel2, padding=pad2),\n",
    "                    jt.nn.LeakyReLU(0.2),\n",
    "                )\n",
    "            else:\n",
    "                self.conv2 = jt.nn.Sequential(\n",
    "                    Blur(out_channel),\n",
    "                    EqualConv2d(out_channel, out_channel, kernel2, padding=pad2),\n",
    "                    jt.nn.AvgPool2d(2),\n",
    "                    jt.nn.LeakyReLU(0.2),\n",
    "                )\n",
    "        else:\n",
    "            self.conv2 = jt.nn.Sequential(\n",
    "                EqualConv2d(out_channel, out_channel, kernel2, padding=pad2),\n",
    "                jt.nn.LeakyReLU(0.2),\n",
    "            )\n",
    "    def execute(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(jt.Module):\n",
    "    def __init__(self, fused=True, from_rgb_activate=False):\n",
    "        self.progression = jt.nn.ModuleList(\n",
    "            [\n",
    "                #ConvBlock( 16,  32, 3, 1, downsample=True, fused=fused),  # 512\n",
    "                #ConvBlock( 32,  64, 3, 1, downsample=True, fused=fused),  # 256\n",
    "                #ConvBlock( 64, 128, 3, 1, downsample=True, fused=fused),  # 128\n",
    "                ConvBlock(128, 256, 3, 1, downsample=True, fused=fused),  # 64\n",
    "                ConvBlock(256, 512, 3, 1, downsample=True),  # 32\n",
    "                ConvBlock(512, 512, 3, 1, downsample=True),  # 16\n",
    "                ConvBlock(512, 512, 3, 1, downsample=True),  # 8\n",
    "                ConvBlock(512, 512, 3, 1, downsample=True),  # 4\n",
    "                ConvBlock(513, 512, 3, 1, 4, 0),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        def make_from_rgb(out_channel):\n",
    "            if from_rgb_activate:\n",
    "                return jt.nn.Sequential(EqualConv2d(3, out_channel, 1), jt.nn.LeakyReLU(0.2))\n",
    "\n",
    "            else:\n",
    "                return EqualConv2d(3, out_channel, 1)\n",
    "\n",
    "        self.from_rgb = jt.nn.ModuleList(\n",
    "            [\n",
    "                #make_from_rgb(16),\n",
    "                #make_from_rgb(32),\n",
    "                #make_from_rgb(64),\n",
    "                make_from_rgb(128),\n",
    "                make_from_rgb(256),\n",
    "                make_from_rgb(512),\n",
    "                make_from_rgb(512),\n",
    "                make_from_rgb(512),\n",
    "                make_from_rgb(512),\n",
    "            ]\n",
    "        )\n",
    "        self.n_layer = len(self.progression)\n",
    "        self.linear = EqualLinear(512, 1)\n",
    "        \n",
    "    def execute(self, input, step=0, alpha=-1):\n",
    "        for i in range(step, -1, -1):\n",
    "            index = self.n_layer - i - 1\n",
    "\n",
    "            if i == step:\n",
    "                out = self.from_rgb[index](input)\n",
    "\n",
    "            if i == 0:\n",
    "                out_std = np.std(out.data, axis=0)\n",
    "                mean_std = jt.array(out_std.mean())\n",
    "                mean_std = mean_std.expand((out.size(0), 1, 4, 4))\n",
    "                out = jt.concat([out, mean_std], 1)\n",
    "\n",
    "            out = self.progression[index](out)\n",
    "\n",
    "            if i > 0:\n",
    "                if i == step and 0 <= alpha < 1:\n",
    "                    skip_rgb = jt.nn.avg_pool2d(input, 2)\n",
    "                    skip_rgb = self.from_rgb[index + 1](skip_rgb)\n",
    "\n",
    "                    out = (1 - alpha) * skip_rgb + alpha * out\n",
    "\n",
    "        out = out.squeeze(2).squeeze(2)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing image of [04 x 04]\n",
      "Testing image of [08 x 08]\n",
      "Testing image of [16 x 16]\n",
      "Testing image of [32 x 32]\n",
      "Testing image of [64 x 64]\n",
      "Testing image of [128 x 128]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling Operators(7/7) used: 2.37s eta:    0s \n"
     ]
    }
   ],
   "source": [
    "netD = Discriminator()\n",
    "nsteps = 6\n",
    "alpha = 0\n",
    "batch_size = 16\n",
    "for step in range(nsteps):\n",
    "    image_size = 2**(step+2)\n",
    "    print('Testing image of [%02d x %02d]' % (image_size, image_size))\n",
    "    image = jt.rand(batch_size, 3, image_size, image_size)\n",
    "    pred = netD(image, step, alpha)\n",
    "    assert(pred.shape == [batch_size, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StyleGAN Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StyledConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantInput(jt.Module):\n",
    "    def __init__(self, channel, size=4):\n",
    "        self.input = jt.randn(1, channel, size, size)\n",
    "\n",
    "    def execute(self, input):\n",
    "        batch = input.shape[0]\n",
    "        out = self.input.repeat(batch, 1, 1, 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "class NoiseInjection(jt.Module):\n",
    "    def __init__(self, channel):\n",
    "        self.weight = jt.zeros((1, channel, 1, 1))\n",
    "\n",
    "    def execute(self, image, noise):\n",
    "        return image + self.weight * noise\n",
    "\n",
    "class AdaptiveInstanceNorm(jt.nn.Module):\n",
    "    def __init__(self, in_channel, style_dim):\n",
    "        self.norm = jt.nn.InstanceNorm2d(in_channel, affine=False)\n",
    "        self.style = EqualLinear(style_dim, in_channel * 2)\n",
    "\n",
    "        self.style.linear.bias.data[:in_channel] = 1\n",
    "        self.style.linear.bias.data[in_channel:] = 0\n",
    "\n",
    "    def execute(self, input, style):\n",
    "        style = self.style(style).unsqueeze(2).unsqueeze(3)\n",
    "        gamma, beta = style.chunk(2, 1)\n",
    "\n",
    "        out = self.norm(input)\n",
    "        out = gamma * out + beta\n",
    "\n",
    "        return out \n",
    "    \n",
    "class StyledConvBlock(jt.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel,\n",
    "        out_channel,\n",
    "        kernel_size=3,\n",
    "        padding=1,\n",
    "        style_dim=512,\n",
    "        initial=False,\n",
    "        upsample=False,\n",
    "        fused=False,\n",
    "    ):\n",
    "        if initial:\n",
    "            self.conv1 = ConstantInput(in_channel)\n",
    "        else:\n",
    "            if upsample:\n",
    "                if fused:\n",
    "                    self.conv1 = jt.nn.Sequential(\n",
    "                        FusedUpsample(\n",
    "                            in_channel, out_channel, kernel_size, padding=padding\n",
    "                        ),\n",
    "                        Blur(out_channel),\n",
    "                    )\n",
    "                else:\n",
    "                    self.conv1 = jt.nn.Sequential(\n",
    "                        jt.nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                        EqualConv2d(\n",
    "                            in_channel, out_channel, kernel_size, padding=padding\n",
    "                        ),\n",
    "                        Blur(out_channel),\n",
    "                    )\n",
    "            else:\n",
    "                self.conv1 = EqualConv2d(\n",
    "                    in_channel, out_channel, kernel_size, padding=padding\n",
    "                )\n",
    "\n",
    "        self.noise1 = equal_lr(NoiseInjection(out_channel))\n",
    "        self.adain1 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
    "        self.lrelu1 = jt.nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.conv2  = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
    "        self.noise2 = equal_lr(NoiseInjection(out_channel))\n",
    "        self.adain2 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
    "        self.lrelu2 = jt.nn.LeakyReLU(0.2)\n",
    "    \n",
    "    def execute(self, input, style, noise):\n",
    "        out = self.conv1(input)\n",
    "        out = self.noise1(out, noise)\n",
    "        out = self.lrelu1(out)\n",
    "        out = self.adain1(out, style)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.noise2(out, noise)\n",
    "        out = self.lrelu2(out)\n",
    "        out = self.adain2(out, style)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntheses Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(jt.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def execute(self, input):\n",
    "        return input / jt.sqrt(jt.mean(input ** 2, dim=1, keepdims=True) + 1e-8)\n",
    "\n",
    "class Generator(jt.Module):\n",
    "    def __init__(self, code_dim, fused=True):\n",
    "        self.progression = jt.nn.ModuleList(\n",
    "            [\n",
    "                StyledConvBlock(512, 512, 3, 1, initial=True),   # 4\n",
    "                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 8\n",
    "                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 16\n",
    "                StyledConvBlock(512, 512, 3, 1, upsample=True),  # 32\n",
    "                StyledConvBlock(512, 256, 3, 1, upsample=True),  # 64\n",
    "                #StyledConvBlock(256, 128, 3, 1, upsample=True, fused=fused),  # 128\n",
    "                #StyledConvBlock(128,  64, 3, 1, upsample=True, fused=fused),  # 256\n",
    "                #StyledConvBlock( 64,  32, 3, 1, upsample=True, fused=fused),  # 512\n",
    "                #StyledConvBlock( 32,  16, 3, 1, upsample=True, fused=fused),  # 1024\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.to_rgb = jt.nn.ModuleList(\n",
    "            [\n",
    "                EqualConv2d(512, 3, 1),\n",
    "                EqualConv2d(512, 3, 1),\n",
    "                EqualConv2d(512, 3, 1),\n",
    "                EqualConv2d(512, 3, 1),\n",
    "                EqualConv2d(256, 3, 1),\n",
    "                #EqualConv2d(128, 3, 1),\n",
    "                #EqualConv2d(64, 3, 1),\n",
    "                #EqualConv2d(32, 3, 1),\n",
    "                #EqualConv2d(16, 3, 1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def execute(self, style, noise, step=0, alpha=-1, mixing_range=(-1, -1)):\n",
    "        out = noise[0]\n",
    "\n",
    "        if len(style) < 2:\n",
    "            inject_index = [len(self.progression) + 1]\n",
    "        else:\n",
    "            inject_index = sorted(random.sample(list(range(step)), len(style) - 1))\n",
    "\n",
    "        crossover = 0\n",
    "\n",
    "        for i, (conv, to_rgb) in enumerate(zip(self.progression, self.to_rgb)):\n",
    "            if mixing_range == (-1, -1):\n",
    "                if crossover < len(inject_index) and i > inject_index[crossover]:\n",
    "                    crossover = min(crossover + 1, len(style))\n",
    "                style_step = style[crossover]\n",
    "            else:\n",
    "                if mixing_range[0] <= i <= mixing_range[1]:\n",
    "                    style_step = style[1]\n",
    "                else:\n",
    "                    style_step = style[0]\n",
    "\n",
    "            if i > 0 and step > 0:\n",
    "                out_prev = out\n",
    "                \n",
    "            out = conv(out, style_step, noise[i])\n",
    "\n",
    "            if i == step:\n",
    "                out = to_rgb(out)\n",
    "                if i > 0 and 0 <= alpha < 1:\n",
    "                    skip_rgb = self.to_rgb[i - 1](out_prev)\n",
    "                    skip_rgb = jt.nn.interpolate(skip_rgb, scale_factor=2, mode='nearest')\n",
    "                    out = (1 - alpha) * skip_rgb + alpha * out\n",
    "                break\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Styled Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyledGenerator(jt.Module):\n",
    "    def __init__(self, code_dim=512, n_mlp=8):\n",
    "        self.generator = Generator(code_dim)\n",
    "\n",
    "        layers = [PixelNorm()]\n",
    "        for i in range(n_mlp):\n",
    "            layers.append(EqualLinear(code_dim, code_dim))\n",
    "            layers.append(jt.nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.style = jt.nn.Sequential(*layers)\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        input,\n",
    "        noise=None,\n",
    "        step=0,\n",
    "        alpha=-1,\n",
    "        mean_style=None,\n",
    "        style_weight=0,\n",
    "        mixing_range=(-1, -1),\n",
    "    ):\n",
    "        styles = []\n",
    "        if type(input) not in (list, tuple):\n",
    "            input = [input]\n",
    "\n",
    "        for i in input:\n",
    "            styles.append(self.style(i))\n",
    "\n",
    "        batch = input[0].shape[0]\n",
    "\n",
    "        if noise is None:\n",
    "            noise = []\n",
    "            for i in range(step + 1):\n",
    "                size = 4 * 2 ** i\n",
    "                noise.append(jt.randn(batch, 1, size, size))\n",
    "\n",
    "        if mean_style is not None:\n",
    "            styles_norm = []\n",
    "            \n",
    "            for style in styles:\n",
    "                styles_norm.append(mean_style + style_weight * (style - mean_style))\n",
    "\n",
    "            styles = styles_norm\n",
    "\n",
    "        return self.generator(styles, noise, step, alpha, mixing_range=mixing_range)\n",
    "\n",
    "    def mean_style(self, input):\n",
    "        style = self.style(input).mean(0, keepdims=True)\n",
    "\n",
    "        return style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Styled Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing image of [04 x 04]\n",
      "Testing image of [08 x 08]\n",
      "Testing image of [16 x 16]\n",
      "Testing image of [32 x 32]\n",
      "Testing image of [64 x 64]\n"
     ]
    }
   ],
   "source": [
    "netG = StyledGenerator()\n",
    "nsteps = 5\n",
    "alpha = 0\n",
    "batch_size = 16\n",
    "code_dim = 512\n",
    "z = jt.randn(batch_size, code_dim)\n",
    "\n",
    "for step in range(nsteps):\n",
    "    image_size = 2**(step+2)\n",
    "    print('Testing image of [%02d x %02d]' % (image_size, image_size))\n",
    "    fake_image = netG(z, step=step, alpha=alpha, style_weight=0.5)\n",
    "    assert(fake_image.shape == [batch_size, 3, image_size, image_size])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
